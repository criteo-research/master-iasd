{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark and the RDD API\n",
    "\n",
    "In this notebook, we will give an overview of Spark and introduce the RDD API.\n",
    "\n",
    "You can refer to the introduction to Spark and documentation for the Spark RDD API on the Spark website.\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "You can also checkout the academic paper that introduced Spark and RDDs. \n",
    "'Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing', Zaharia et al.\n",
    "https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf\n",
    "\n",
    "Below we assume you have read the introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark application  \n",
    "local: workers are threads on your machine  \n",
    "'*': one thread per core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder \\\n",
    "    .master(\"local[*]\")  \\\n",
    "    .appName('intro') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD API is available on the SparkContext sc  \n",
    "SparkSession ss will be used for the dataframe API in TP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ss.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A UI is available to understand/monitor the state of your application.   \n",
    "To access it, you http://localhost:4040 in a broswer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random array of 1M float and convert it to a RDD having 16 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "local_data = np.random.rand(2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 4 elements of local array: ', local_data[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the distributed version of this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(local_data, numSlices=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 4 elements of RDD: ', rdd.take(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice one job was created on the Spark UI with only one task.  \n",
    "Spark does not need to evaluate all the RDD to access only the first elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_size = rdd.count()\n",
    "print(f'We have {rdd_size} elements in our RDD, the same as in our local array: {len(local_data)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice another job was created in the Spark UI with 16 tasks. One task per partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we modify our RDD ?  \n",
    "We cannot modify RDD. They are immutable. However we can transform our existing RDD into a new RDD. This won't consume memory. The RDD are not materialized in memory. This is just a description of the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(lambda x: x - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this was instantaneous and no job was created in the Spark UI. Spark transformations (such as map above) are lazy. Only actions (such as count or take above) trigger the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A map does not change the partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.getNumPartitions() == rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep transforming our RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda x: 2*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An action will trigger the computation of the map function above. Here we sum all elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the Spark UI that only 16 tasks were executed although we applied a map function twice.  \n",
    "We could have expected to see 32 tasks: 16 tasks to apply the first map f1 and then 16 tasks to apply the second map f2.  \n",
    "But that would mean iterating through our RDD twice. Spark is smart and merged our two functions to apply f2 o f1 instead.  \n",
    "Everything happened as if we would have written the code below directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x: 2*(x-0.5)).reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we are summing elements unformly distributed between (-1, 1), we would expect a sum close to 0. Let's make sure our random generator is behaving properly by also checking the max and min values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.reduce(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.reduce(min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds good but to compute the sum, max, min we generated 3 jobs meaning going over our RDD 3 times. Can we do it all in one job ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = (0.0, -2.0, 2.0)\n",
    "# State is a tuple of size 3\n",
    "# At position 0: the sum of all elements whose initial value is 0.0\n",
    "# At position 1: the max of all elements whose initial value is -2.0 (since all elements are between -1 and 1)\n",
    "# At position 2: the min of all elements whose initial value is 2.0\n",
    "\n",
    "def add_one_element_to_state(state, elt):\n",
    "    state_sum, state_max, state_min = state\n",
    "    state_sum += elt\n",
    "    state_max = max(state_max, elt)\n",
    "    state_min = min(state_min, elt)\n",
    "    return (state_sum, state_max, state_min)\n",
    "\n",
    "def merge_two_states(state1, state2):\n",
    "    sum1, max1, min1 = state1\n",
    "    sum2, max2, min2 = state2\n",
    "    return ( sum1+sum2, max(max1, max2), min(min1, min2) )\n",
    "\n",
    "rdd3.aggregate(init_state, add_one_element_to_state, merge_two_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig deeper. We would like to make sure the distribution is uniform. Let's compute the average of the values on a partition of the [-1, 1] interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(-1000, 1001) / 1000\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket(x):\n",
    "    bin_idx = np.digitize(x, bins)\n",
    "    return (bins[bin_idx-1], bins[bin_idx])\n",
    "\n",
    "print(f'{local_data[0]} is in {bucket(local_data[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.keyBy(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the bucket function depends on the local variable bins. When sending the bucket function to the executors, spark will also send the bins variable. Spark will send the closure of the bucket function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_is_bad(elt):\n",
    "    (low, high), x = elt\n",
    "    return x < low or x >= high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_bucket_rdd = rdd4.filter(bucket_is_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_bucket_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = (0, 0.0)\n",
    "# At position 0, the number of values\n",
    "# At position 1, the sum of all values\n",
    "\n",
    "def accum_state(state, elt):\n",
    "    num_elts, sum_elts = state\n",
    "    num_elts += 1\n",
    "    sum_elts += elt\n",
    "    return (num_elts, sum_elts)\n",
    "\n",
    "def merge_states(state1, state2):\n",
    "    n1, sum1 = state1\n",
    "    n2, sum2 = state2\n",
    "    return (n1+n2, sum1+sum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_per_bucket = rdd4.aggregateByKey(init_state, accum_state, merge_states, numPartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing happened. aggregateByKey is a transformation not an action so it is evaluated lazily giving us the opportunity to do additional transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg(state):\n",
    "    num_elts, sum_elts = state\n",
    "    return sum_elts / num_elts\n",
    "\n",
    "avg_per_bucket = state_per_bucket.mapValues(compute_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_per_bucket_local = avg_per_bucket.collect()\n",
    "avg_per_bucket_local[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the Spark UI that the job we just executed as two stages. Spark cannot merge operations (like it did with the two map functions above) across a shuffle operation. The first stage corresponds to all map operations before the shuffle plus the preparation to the shuffle (the map phase), the second stage corresponds to the finalization of the shuffle (the reduce phase) plus all maps after the shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "avg_per_bucket_local.sort(key=lambda x: x[0][0])\n",
    "\n",
    "x = [(low+high)/2 for (low, high), _ in avg_per_bucket_local]\n",
    "y = [avg for _, avg in avg_per_bucket_local]\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally check that we have the same number of elements in each bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_bucket = state_per_bucket.mapValues(lambda x: x[0]).collect()\n",
    "\n",
    "count_per_bucket.sort(key=lambda x: x[0][0])\n",
    "\n",
    "x = [(low+high)/2 for (low, high), _ in count_per_bucket]\n",
    "y = [count for _, count in count_per_bucket]\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice Spark executed everything from the beginning again altough the first stage is identical (we only modified the map of the second stage). By default, Spark never caches the result of intermediate computation. We have to tell spark to save intermediate results when needed using the rdd.persist() method.  \n",
    "\n",
    "Replay the code above after persisting state_per_bucket. Notice the first stage of the job computing the count per bucket is skipped and that one RDD is persisted in the Storage tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally stop the spark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
