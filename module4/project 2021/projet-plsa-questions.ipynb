{"cells":[{"cell_type":"markdown","source":["## Problem Statement\nAim of this project is to implement a Probabilistic Latent Semantic Model. The content of this notebook is three fold:\n1. In the first part of the notebook, we will implement pyspark functions able to hande matrix operations. These functions will be useful to implement the algorithm.\n2. The second part of the notebook consists in implementing an expectation maximization algorithm and learn its parameters on the MovieLens dataset.\n3.  Once we have made sure that the algorithm works correctly, we will see how to use the model in order to recommend some movies to users based on their interests."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc298743-06b1-4b4f-8516-e60b8973ea98"}}},{"cell_type":"markdown","source":["## Setup environment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f466bf28-a3a4-4350-a138-20a79db55ba8"}}},{"cell_type":"markdown","source":["This section contains useful imports and boiler plate to launch your spark session."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e5b467e-7d0e-4c92-be1d-69d32e9b3d95"}}},{"cell_type":"code","source":["!pip install -q mlflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"563a95f6-c661-4ecc-8e76-325178201ccb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import os\nimport time\nimport zipfile\nimport urllib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport mlflow\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as sf\nfrom pyspark.sql import types as st\nfrom pyspark.sql.functions import udf\nfrom pyspark.storagelevel import StorageLevel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eaf33561-9b46-4617-90b1-82aac87fc541"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%matplotlib inline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f86f9264-d5f9-4d18-9f46-bb8fc8646ee4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# This section configures hdfs root and how to load a spark session.\n# Reading can be skipped during the first reading.\n\ndef load_spark_session():\n  return (\n    SparkSession\n    .builder\n    .appName(\"Dataset\")\n    .getOrCreate()\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e18a453-e1ec-4ca6-9d1c-11ea3fbe39a1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ss = load_spark_session()\nss"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f7dfb02-4d38-49a4-abf8-eb599b3a8acd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Preliminary exercices\nIn these exercices we will implement matrix operations that will be usefull to run the PLSA algorithm afterwards.\n\nWe define a matrix as a column of a spark DataFrame of type array(double), where the arrays of each rows have a fixed size."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf3bb64b-1809-4f02-876f-6e86036a0349"}}},{"cell_type":"markdown","source":["#### `matrix_sum_rows` \nTakes a matrix (a column containing arrays of fixed length) and returns the sum of each row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f92e3d39-6d8b-45c0-80fb-69f713062620"}}},{"cell_type":"code","source":["# Hint: https://stackoverflow.com/a/57448698/2015762\ndef matrix_sum_rows(col_name, length_of_array):\n    return # ...\n\ninput_array = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\nexpected_output = input_array.sum(axis=1)\nprint('Input array')\nprint(input_array)\nprint('Expected output')\nprint(expected_output)\nprint('Obtained output')\n(\n    ss.sparkContext.parallelize(input_array.tolist()).map(lambda x: st.Row(matrix=x)).toDF()\n    .withColumn('row_sum', matrix_sum_rows('matrix', 4))\n).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf9fb7b9-eaa5-4577-9073-5fd864f06e7f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### `matrix_sum_columns`\nTakes a matrix (a column containing arrays of fixed length) and returns the sum of each column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"741c536a-6bb5-4866-a778-2eb4ccd33798"}}},{"cell_type":"code","source":["# Hint: https://stackoverflow.com/a/54382990/2015762\ndef matrix_sum_columns(col_name, length_of_array):\n    pass\n\ninput_array = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\nexpected_output = input_array.sum(axis=0)\nprint('Input array')\nprint(input_array)\nprint('Expected output')\nprint(expected_output)\nprint('Obtained output')\n(\n    ss.sparkContext.parallelize(input_array.tolist()).map(lambda x: st.Row(matrix=x)).toDF()\n    .select(matrix_sum_columns('matrix', 4).alias('col_sum'))\n).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ccef2bc-8e51-4232-8988-7119852672d2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### `matrix_normalize_rows`\nTakes a matrix (a column containing arrays of fixed length) and returns the same matrix where the rows have been divded by their sum, such that each row sums to 1."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"304e7e34-d378-4ed1-b89d-72d9f60be693"}}},{"cell_type":"code","source":["def matrix_normalize_rows(col_name, length_of_array):\n    pass\n\ninput_array = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\nexpected_output = input_array / input_array.sum(axis=1).reshape(-1, 1)\nprint('Input array')\nprint(input_array)\nprint('Expected output')\nprint(expected_output)\nprint('Obtained output')\n(\n    ss.sparkContext.parallelize(input_array.tolist()).map(lambda x: st.Row(numbers=x)).toDF()\n    .withColumn('normalized_elements', matrix_normalize_rows('numbers', 4))\n).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78232183-f4ec-44bd-88bb-618b98d4a553"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### `matrix_elementwise_product`\nTakes two matrices and return their elementwise product (aka. Hadamard product)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8162f055-94a2-4c9f-94ae-abfbee2ad319"}}},{"cell_type":"code","source":["def matrix_elementwise_product(col_name_1, col_name_2, length_of_array):\n    pass\n\ninput_array_1 = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\ninput_array_2 = np.array([[1, 2, 1, 2], [10, 20, 10, 20]], dtype=float)\nexpected_output = input_array_1 * input_array_2\nprint('Input array')\nprint(input_array_1)\nprint(input_array_2)\nprint('Expected output')\nprint(expected_output)\nprint('Obtained output')\n(\n    ss.sparkContext.parallelize(zip(input_array.tolist(), input_array_2.tolist())).map(lambda x: st.Row(numbers_1=x[0], numbers_2=x[1])).toDF()\n    .withColumn('elementwise_products', matrix_elementwise_product('numbers_1', 'numbers_2', 4))\n).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6fb8e00-36a4-4a7d-b030-b8bea7553fc2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### `matrix_elementwise_divide`\nTakes two matrices and divide elementwise the first one by the second one."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a45a8c3-1bcd-4e70-9407-cf74e5b2f601"}}},{"cell_type":"code","source":["def matrix_elementwise_divide(col_name_1, col_name_2, length_of_array):\n    pass\n\ninput_array_1 = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\ninput_array_2 = np.array([[1, 2, 1, 2], [10, 20, 10, 20]], dtype=float)\nexpected_output = input_array_1 / input_array_2\nprint('Input array')\nprint(input_array_1)\nprint(input_array_2)\nprint('Expected output')\nprint(expected_output)\nprint('Obtained output')\n(\n    ss.sparkContext.parallelize(zip(input_array.tolist(), input_array_2.tolist())).map(lambda x: st.Row(numbers_1=x[0], numbers_2=x[1])).toDF()\n    .withColumn('elementwise_divided', matrix_elementwise_divide('numbers_1', 'numbers_2', 4))\n).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"272cb7f3-39c8-45f7-8510-58c07f2bbce5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Load Movie Lens dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be3d2e8f-5de3-4116-b074-1f33ec751c58"}}},{"cell_type":"code","source":["# Remove from cache all data from preliminary exercices\nss.catalog.clearCache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7f61ccb-1fb5-44d6-9f92-281a068a0c94"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# This section defines utility method to load our dataset.\n# Reading can be skipped during the first reading.\ndef download_dataset(dataset_path):\n    os.makedirs(dataset_path, exist_ok=True)\n    urllib.request.urlretrieve(\n        \"http://files.grouplens.org/datasets/movielens/ml-20m.zip\", \n        \"movie_lens_data/ml-20m.zip\"\n    )\n    with zipfile.ZipFile(\"movie_lens_data/ml-20m.zip\", \"r\") as zip_ref:\n        zip_ref.extractall(dataset_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f705a58-1c9d-4a7e-b61d-704c8ea2039a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["download_dataset(os.path.abspath('movie_lens_data'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd325836-e190-43e7-8a96-11401bd3070c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["hdfs_prefix = 'file:///databricks/driver/'\nmovies_path = f\"{hdfs_prefix}/movie_lens_data/ml-20m/movies.csv\"\nratings_path= f\"{hdfs_prefix}/movie_lens_data/ml-20m/ratings.csv\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04eea3cf-b99a-4198-ab6b-ecae2c02407f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["movies_df = ss.read.options(header=True).csv(movies_path)\nratings_df = ss.read.options(header=True).csv(ratings_path)\npositive_ratings_df = ratings_df.filter(\"rating>=3.5\").cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1574720e-1543-4268-b47e-4feb2cc0d819"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["movies_df.show(3)\nratings_df.show(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c9f2cfb-9ac4-49fe-92bb-296f417cd035"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Probabilistic Latent Semantic Analysis (PLSA)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23218e46-5de7-4835-bc4d-ec79c9ed90ff"}}},{"cell_type":"markdown","source":["### Preprocess dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b697aee-f841-43e1-99df-93beccbbbff3"}}},{"cell_type":"code","source":["# Control the dataset size to work with a small dataset.\n# You can decrease these parameters for a deeper analysis.\nmin_positive_ratings_per_movie = 10000\nselected_movies_df = (\n    positive_ratings_df\n    .groupBy('movieId')\n    .count()\n    .filter(sf.col('count') > min_positive_ratings_per_movie)\n    .select('movieId')\n)\n\nkeep_user_every = 100\nuser_movies_interactions = (\n    positive_ratings_df\n    .join(selected_movies_df, on='movieId')\n    .filter(sf.expr(f'PMOD(HASH(userId),{keep_user_every})')==0)\n    .select('userId', 'movieId')\n    .repartition('userId', 'movieId')\n).cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8a89037-87e6-4558-bafc-aedb828c0d3e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question\nWhat is the meaning of `.filter(sf.expr(f'PMOD(HASH(userId),{keep_user_every})')==0)` ? Why did we not use `.sample()` instead?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37f47270-c632-40c2-abed-8a2ba0b1f8b4"}}},{"cell_type":"code","source":["user_movies_interactions.select(\n    sf.count('*').alias('n_pairs'),\n    sf.countDistinct('userId').alias('n_users'),\n    sf.countDistinct('movieId').alias('n_movies'),\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30f6034b-1805-40ee-8d47-effa7ac39fc8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### PLSA Implementation\n\nRessources\n* Original paper : https://arxiv.org/pdf/1301.6705.pdf\n* Clear rephrasing : https://link.springer.com/article/10.1023/A:1007617005950 (Section 3)\n* Map reduce implementation by Google News : https://www2007.org/papers/paper570.pdf (We will use their notations from page 275).\n\nTry to recommend movies to users given:\n* N the number of users u\n* M the number of movies s\n* L the number of latent classes z\n* T number of user, movie interactions (each interaction (s_t, u_t) means user u_t liked movie s_t)\n\nWe suppose that the probability that a user will like a movie can be written in the form of a mixture model given by the equation:\n$$\np(s|u) = \\sum_{z=1}^L p(s|z) p(z|u)\n$$\nAnd we want to maximize the log-likelihood of the observed user interactions\n$$\nL = - \\frac{1}{T} \\sum_{t=1}^{T} \\log p(s_t|u_t) = - \\frac{1}{T} \\sum_{1}^{T} \\sum_{z=1}^L p(s_t|z) p(z|u_t)\n$$\nThat can be done using an EM algorithm working as follow:\n\n**E step**\n\nFor each interaction (u_t, s_t), compute for all z = 1, ..., L:\n$$\np(z|(u_t, s_t)) = \\frac{p(s_t|z) p(z|u_t)}{\\sum_z p(s_t|z) p(z|u_t)}\n$$\n\n**M step**\n\nFind each movie probability given a latent class\n$$\np(s|z) = \\frac{N(z, s)}{N(z)} \n\\quad \\text{where} \\quad N(z, s) = \\sum_s \\sum_u p(z|(u, s)) \n\\quad \\text{and} \\quad N(z) = \\sum_s N(z, s)\n$$\nFind each latent class probability given each user.\n$$\np(z|u) = \\frac{\\sum_s p(z|(u, s))}{\\sum_z \\sum_s p(z|(u, s))}\n$$\n\nWe will have the following dataframes in which the latent variables z are stored in columns of array\\<double\\>. Latent variable z_i is stored at index i of this array.\n\n* `count_z_s`: M rows, with 2 columns  `movieId` (int), `N(z,s)` (array\\<double\\>).\n* `count_z`: 1 row, with 1 column `N(z)`.\n* `p_s_knowing_z`: M rows, with 2 columns  `movieId` (int), `p(s|z)` (array\\<double\\>). For a given z, the sum of p(s|z) equals 1.\n* `p_z_knowing_u`: N rows, with 2 columns `userId` (int), `p(z|u)` (array\\<double\\>). For a given u, the sum of p(z|u) equals 1.\n* `p_z_knowing_u_and_s`: N x M rows, with 3 columns `userId` (int), `movieId` (int), `p(z|u,s)` (array\\<double\\>)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"506f981d-f3e7-48fc-9191-f0afb7cea16b"}}},{"cell_type":"markdown","source":["We will now implement methods that compute each of these equations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8521733-f255-4b35-894d-7ddffcf0fa74"}}},{"cell_type":"code","source":["def get_count_z(count_z_s, n_latent_classes):\n    \"\"\"Compute N(z) = sum_s N(z,s)\n    \"\"\"\n    pass\n\ncount_z_s = ss.sparkContext.parallelize([\n    st.Row(**{\"movieId\":1, \"N(z,s)\": [1., 3., 4.]}),\n    st.Row(**{\"movieId\":2, \"N(z,s)\": [4., 5., 0.]}),\n]).toDF()\nget_count_z(count_z_s, 3).show()\n# Expected [5., 8., 4.]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd31abe6-3fa7-47b2-abd6-fb2459ad1924"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_count_z_s(p_z_knowing_u_and_s, n_latent_classes):\n    \"\"\"Compute N(z,s) = sum_u p(z|u,s)\n    \"\"\"\n    pass"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d01f81f-68eb-4e43-b673-0c51f3a5d1a5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_p_s_knowing_z(count_z_s, count_z, n_latent_classes):\n    \"\"\"Compute p(s|z) = N(z,s) / N(z)\n    \n    Hint: crossJoin may help\n    \"\"\"\n    pass"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"781192e3-1ee8-44cc-97ee-4fb0b396ad7d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_p_z_knowing_u(p_z_knowing_u_and_s, n_latent_classes):\n    \"\"\"Compute p(z|u) = sum_s p(z|u,s) / sum_z sum_s p(z|u,s)\n    \"\"\"\n    pass"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"003d88a4-ea76-4cea-87a0-3d9211bab162"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_p_z_knowing_u_and_s(observed_pairs, count_z_s, count_z, p_z_knowing_u, n_latent_classes):\n    \"\"\"For all pairs of observed (u, s)\n    \n    Compute p(z|u,s) = [N(z, s) / N(z) * p(z|u)] / sum_z [N(z, s) / N(z) * p(z|u)]\n                     = [p(s|z) * p(z|u)] / sum_z [p(s|z) * p(z|u)]\n    \"\"\"\n    pass"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fb838c7-1901-4b43-8153-bd01cd174432"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def log_likelihood(observed_pairs, # ... # , n_latent_classes):\n    \"\"\"Compute the log likelihood of the observed pairs\n    \n    L = - 1 / T * sum_t log[ p(s|u) ]\n      = - 1 / T * sum_t log[ sum_z p(s|z) * p(z|u) ]\n    \"\"\"\n    return 0."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87efbe4d-0da9-4e16-9af9-c77ccdb13e3e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def initialize_statistics(observed_pairs, n_latent_classes):\n    \"\"\"Initialize either p(s|z) and p(z|u) or p(z|(u, s)) to be able to fuel the first iteration of the EM algorithm.\n    What would happen if you initialize these to a constant value ?\n    \"\"\"\n    # ...\n    pass"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55ad6326-4eb1-418d-84ba-8d6f6a78ab5f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def run_plsi(observed_pairs, n_iterations, n_latent_classes, checkpoint_every=1):\n    start_init_time = time.time()\n    ss.sparkContext.setJobDescription(\"Initialization\")\n\n    # ... = initialize_statistics(observed_pairs, n_latent_classes)\n    llh = log_likelihood(observed_pairs, # ... #, n_latent_classes)\n    mlflow.log_metric(key=\"llh\", value=llh, step=0)\n    print(f'LLH: {llh:.10f}')\n    \n    end_init_time = time.time()\n    print(f'Initialization: {end_init_time - start_init_time:.1f}s')\n    \n    for i in range(n_iterations):\n        start_e_step = time.time()\n        ss.sparkContext.setJobDescription(f\"Iteration {i+1}: E-step\")\n        # E step\n        # ...\n        \n        end_e_step = time.time()\n        print(f'Iteration {i+1}: E-step: {end_e_step - start_e_step:.1f}s')\n        \n        ss.sparkContext.setJobDescription(f\"Iteration {i+1}: M-step\")\n        # M step\n        # ...\n        \n        llh = log_likelihood(observed_pairs, # ... #, n_latent_classes)\n        mlflow.log_metric(key=\"llh\", value=llh, step=i+1)\n        \n        end_m_step = time.time()\n        print(f'Iteration {i+1}: M-step: {end_m_step - end_e_step:.1f}s')\n        print(f'LLH: {llh:.10f}')\n    \n    return get_p_s_knowing_z(count_z_s, count_z, n_latent_classes)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da86b933-689d-4899-9bd9-ecb9f75325fc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["n_iterations = 20\nn_latent_classes = 5\n\nwith mlflow.start_run():\n    mlflow.log_param(\"n_iterations\", n_iterations)\n    mlflow.log_param(\"n_latent_classes\", n_latent_classes)\n    run_plsi(user_movies_interactions, n_iterations=n_iterations, n_latent_classes=n_latent_classes)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5e739d9-ad54-4145-999a-6178c08d4b96"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Spark optimizations to make it work\n\nTry to improve the algorithm technical performance\n1. How does the EM algorithm is supposed to scale with the number of EM steps ? Do you observe such a scaling ?\n2. If each steap takes longer than the previous one: Try using .cache() wisely.\n3. Try to unpersist your dataframes when they become unneeded (look at the Storage tab in the Spark UI) (Optional + 2pts)\n4. If after few steps (typically 5), your algorithm starts being much slower and spend more and more time scheduling jobs (look in the Spark UI), try using [.localCheckpoint()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.localCheckpoint). How does it differ from caching ? What are the benefits and the drawbacks ?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14bf965b-be4e-40cf-8f2d-6d412c5d005f"}}},{"cell_type":"markdown","source":["## Extensions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d47c0ae2-db71-47d7-906a-2886d8e64328"}}},{"cell_type":"markdown","source":["Extensions:\nAll _non optional_ extensions are rated over 15pts / 20. Select some _optional_ extensions as well to get extra points. Yes, maximal grade is 30/20, so you do not need to do all _optional_ extensions.\n\n*Algorithm quality:* Make sure we are running things correctly\n1. Compute the top movies per latent classes : the one having the highest p(s|z) for a given z.\n2. Compute the log likelihood across iterations ? Try for several values of L. You can log these values to mlflow.\n3. Split randomly your ratings in two folds, train (80%) and test (20%). Train the algorithm on the train ratings and look for the likelihood on the test ratings ? How does it compare to the likelihood on the train ratings?\n4. If we suppose each movie can be represented by p(s|z) ∈ R^L, pick few movies and look at their nearest neighbors. (Optional + 2pts)\n5. Can you exhibit common movie characteristics (year or genre) for the top movies of a latent class? (Optional + 2pts)\n\n*Recommender System (Optional + 6pts)*\n1. How could this algorithm be used as a recommender system ? Formulate the prediction task: what would be the new movies you would you recommend to a user in your database ?\n2. Compute [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall) for both train and test ratings.\n3. How could one use the vectorial representation of p(s|z) ∈ R^L to recommend movies to a user that has several movies s but was not in the training set (we thus do not know the value of p(z|u) )?\n\n\n*LDA (Optional + 3pts)*\n\nCompare PLSI with LDA (implemented in [spark mllib](https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/mllib/clustering/LDA.html)) in terms of performance, quality and recommender system."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fdd0389-5493-4df6-a15d-1c41b0d8b3c7"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e40c2766-632d-444d-8dbf-14ff677b584d"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PLSA (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1412922235015320}},"nbformat":4,"nbformat_minor":0}
