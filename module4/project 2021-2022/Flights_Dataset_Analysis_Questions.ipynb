{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de Flights_Dataset_Analysis_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Session 3/4 : Flights Dataset Analysis\n",
        "In this session, we will conduct a few analyses on a simplified flight fares dataset.  \n",
        "In particular we will try to build cheapest routes from one point to another."
      ],
      "metadata": {
        "id": "HJ3XJCHQQ6hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading and Instructions\n",
        "You must return your notebook before **Wednesday March 2nd 23:59 Paris time** by email to David : d.diebold@criteo.com.  \n",
        "Grade will be composed of :\n",
        "1. Timely return\n",
        "2. Correctness (some questions may still leave you with some liberties)\n",
        "3. Report formatting : While we allow you to return your project in a notebook format, you should think your report as being a classic text and image pdf report in which the code is in appendix. That means your notebook should be fully readable while hiding all the code cells.\n",
        "4. Code Readability (factorized code, well-named variables, explain what you do when code becomes complicated, etc...)\n",
        "5. Performance (this is not a race but we want you to think about performance issues when designing your solution (shuffles, etc...). Don't hesitate to annotate your notebook with any remarks about your solution.\n"
      ],
      "metadata": {
        "id": "jBzndH833opa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Spark Environment\n",
        "Since we are not running on databricks, we will need to install Spark by ourselves, every time we run the session.  \n",
        "We need to install Spark, as well as a Java Runtime Environment.  \n",
        "Then we need to setup a few environment variables.  \n"
      ],
      "metadata": {
        "id": "W7mpca-zaxh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evGT05CJ9Bcw",
        "outputId": "48d9677b-f69a-4f22-8411-a39a7aec4f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  287M  100  287M    0     0   198M      0  0:00:01  0:00:01 --:--:--  198M\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -O https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "PoyY51Od9KrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "fgjP2e7r-p2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional step : Enable SparkUI through secure tunnel\n",
        "This step is useful if you want to look at Spark UI.\n",
        "First, you need to create a free ngrok account : https://dashboard.ngrok.com/login.  \n",
        "Then connect on the website and copy your AuthToken.\n"
      ],
      "metadata": {
        "id": "waeQxpkV_Tyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this step downloads ngrok, configures your AuthToken, then starts the tunnel\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!./ngrok authtoken my_ngrok_auth_token_retrieved_from_website # <-------------- change this line !\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA71x1we-wCU",
        "outputId": "abc9e204-2a9f-4cb1-85e5-7a3c0284c2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now get the Spark UI url on https://dashboard.ngrok.com/endpoints/status. We're done !"
      ],
      "metadata": {
        "id": "X9fdibw-AWgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful imports"
      ],
      "metadata": {
        "id": "EcQneiFya-p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pyspark.sql.functions as F\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Qw-LfxWabAt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "Aim of this notebook is to help you get comfortable with Spark Dataframe API while working on a flights dataset.  \n",
        "This dataset contains some domestic flight prices for US country.  \n",
        "We will call route a tuple identified by an origin airport and a destination airport.  \n",
        "We will try to find-out what are the best options for a traveler, to go from some place to another.  \n",
        "Here is a short description of the columns:\n",
        "- ItinID & MktID: vaguely demonstrates the order in which tickets were ordered (lower ID #'s being ordered first)\n",
        "- MktCoupons: the number of coupons in the market for that flight\n",
        "- Quarter: 1, 2, 3, or 4, all of which are in 2018\n",
        "- Origin: the city out of which the flight begins\n",
        "- OriginWac: USA State/Territory World Area Code\n",
        "- Dest: the city out of which the flight begins\n",
        "- DestWac: USA State/Territory World Area Code\n",
        "- Miles: the number of miles traveled\n",
        "- ContiguousUSA: binary column -- (2) meaning flight is in the contiguous (48) USA states, and (1) meaning it is not (ie: Hawaii, Alaska, off-shore territories)\n",
        "- NumTicketsOrdered: number of tickets that were purchased by the user\n",
        "- Airline Company: the two-letter airline company code that the user used from start to finish (key codes below)\n",
        "- PricePerTicket: ticket price"
      ],
      "metadata": {
        "id": "NwM5x_W3BR-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset described above\n",
        "from urllib import request\n",
        "import zipfile\n",
        "\n",
        "url = \"https://www.dropbox.com/s/kda4h5su4z6go05/flights.zip?dl=1\"\n",
        "filehandle, _ = request.urlretrieve(url)\n",
        "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
        "zip_file_object.extractall()"
      ],
      "metadata": {
        "id": "umxgKJhc-sPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This seond file contains a mapping with airports Code / Name / Latitude / Longitude\n",
        "# It can help to get a better understanding of the airports you are dealing with.\n",
        "# Source : https://www.partow.net/miscellaneous/airportdatabase/index.html#Downloads\n",
        "url2 = \"https://www.dropbox.com/s/xe2a3hgwlugos7a/GlobalAirportDatabase.txt?dl=1\"\n",
        "request.urlretrieve(url2, \"airport_latlon.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxur6Y50IfAZ",
        "outputId": "5995f6de-4a68-433a-9953-28e745edbf82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('airport_latlon.txt', <http.client.HTTPMessage at 0x7fbc6a291150>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt02q18wIz25",
        "outputId": "db66ee6f-42ca-42ff-e924-2ee19f446ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "airport_latlon.txt\t  spark-3.2.1-bin-hadoop3.2\n",
            "Cleaned_2018_Flights.csv  spark-3.2.1-bin-hadoop3.2.tgz\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (1 point)\n",
        "Display a few rows of the flights fare dataset, display it's schema, and count the amount of rows.  \n",
        "You are likely to read this dataset a lots of times ; rewrite the dataset on the file system in an optimized way, to optimize further readings.  "
      ],
      "metadata": {
        "id": "Dmz2xyHdkPtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLlBUjJZldwS",
        "outputId": "80723a9b-f88f-4272-8baa-e57999ad0603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of rows : 9534417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (4 points)  \n",
        "Find how many origin and destination airports are contained in the dataset.  \n",
        "Show them on a US map to get a better intuition of the dataset. You can use shapely and geopandas to perform this task.  \n",
        "Do we have all the lat/lon available ?"
      ],
      "metadata": {
        "id": "jBVPBi7ZCpWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3b4x87c-YbRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next two questions, we will want to get an understanding of ticket prices based on flight distance.  \n",
        "## Question 3 (2 points)\n",
        "To do that we first need to get and understanding of the flight distance distribution.  \n",
        "We want to display an histogram of flight distances. To do this :  \n",
        "- use numpy logspace function to create 10 distance buckets, base=1.05\n",
        "- then use numpy digitize function inside a spark UDF to create the buckets.\n",
        "- buckets should be displayed in the correct order, and displayed like this : [min;max]"
      ],
      "metadata": {
        "id": "SCKoL0qFbYtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BB7s1om3dqGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 (3 points)\n",
        "Display the average flight fares for each distance bucket.  \n",
        "Graph should also contain the confidence intervals.  \n",
        "Buckets should be displayed in the correct order, and displayed like this : $[min;max]$  \n",
        "Interpret the results.  "
      ],
      "metadata": {
        "id": "pckZKP_5woRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6VDGzYb4Wllj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5 (4 points)\n",
        "For the remainder of the notebook, we will only take care of the average price of each route.  \n",
        "Our goal is to find cheap combinations of flights to travel from one place to another.  \n",
        "First, we want to build a dataframe named 'cheapest_routes_df' containing the cheapest price to go from one place to another, with one or zero waypoint. Dataframe should look like this (Waypoints column can be empty) :  \n",
        "\n",
        "Origin  | Destination | Waypoints | TotalPrice\n",
        "-------------------|------------------|---|---\n",
        "ACY       | MOB | ATL | 323.0\n",
        "Row 2, Col 1       | Row 2, Col 2 | | 89.0\n",
        "  \n",
        "Is it interesting to consider waypoints to go from one place to another ?  "
      ],
      "metadata": {
        "id": "4682xNqsizwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G06_1Z14Wpsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6 (6 points)\n",
        "Now we want to create the dataframe with cheap combinations of flights from one place to another, but there is no longer any limitation on the amount of waypoints.  \n",
        "Let $Routes_{k}$ designate the dataframe that contains cheapest routes for at most $k$ waypoints.  \n",
        "This dataframe contains a column named 'Waypoints', containing an array of waypoints.  \n",
        "Then:  \n",
        "- Define a function that computes $Routes_{k+1}$ from $Routes_{k}$ and $Routes_{0}$.  \n",
        "- Test it on a simple dataset made of three rows, built with $spark.sparkContext.parallelize$.  \n",
        "- Use it iteratively to build what we want.  \n",
        "- At each step, measure the amount of routes with k waypoints.  \n",
        "- What is the stopping criterion ?  \n",
        "- Measure the execution time of each step.  \n",
        "- What if we want to execute the iterations up to $k=15$ ?  \n",
        "- Explain what happens, and find a solution to approximately have the same execution time at each iteration.  \n",
        "- Analyze the results obtained for $Routes_{maxK}$\n",
        "- Analyze the euros spared, putting them in front of the extra miles traveled."
      ],
      "metadata": {
        "id": "l6IwcBQB9Y1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OB7vFS1d9byw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}