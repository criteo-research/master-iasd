{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the Movielens dataset with the Spark RDD API"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9414d64e-5c0f-49ba-976b-daaf7c49daff"
        },
        "id": "C_GP8Bylovk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Spark Environment\n",
        "Since we are not running on databricks, we will need to install Spark by ourselves, every time we run the session.  \n",
        "We need to install Spark, as well as a Java Runtime Environment.  \n",
        "Then we need to setup a few environment variables.  "
      ],
      "metadata": {
        "id": "pN50BgfWo0tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -O https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "hE0-Ax0wp9Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "jsCGmN9GqA1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "0rOOIQccqDDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional step : Enable SparkUI through secure tunnel\n",
        "This step is useful if you want to look at Spark UI.\n",
        "First, you need to create a free ngrok account : https://dashboard.ngrok.com/login.  \n",
        "Then connect on the website and copy your AuthToken."
      ],
      "metadata": {
        "id": "9gb_bdz9qZPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this step downloads ngrok, configures your AuthToken, then starts the tunnel\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "#!./ngrok authtoken my_ngrok_auth_token_retrieved_from_website # <-------------- change this line !\n",
        "!./ngrok authtoken 25Pb4DqNqaoy5kCwimBO7dFMwvx_5BYL36GDSkQtRexvt9pRA\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "metadata": {
        "id": "qFlzHSUNqiIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Imports"
      ],
      "metadata": {
        "id": "h1N7rSiyqkVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import urllib\n",
        "import urllib.request as req\n",
        "import zipfile\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5e1bd4d2-d4ba-4ce4-841a-1996d75975e2"
        },
        "id": "f7pRHH29ovlQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the dataset"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "298fc35c-3d55-4ef3-b871-e93f7e28ff68"
        },
        "id": "_ygX9FKGovlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://files.grouplens.org/datasets/movielens/ml-20m.zip'\n",
        "filehandle, _ = urllib.request.urlretrieve(url)\n",
        "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
        "zip_file_object.namelist()\n",
        "zip_file_object.extractall()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "78cd57f5-3c06-4e38-be21-40fb49be98fd"
        },
        "id": "UlMv6YO6ovlb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "ZrRve531uWIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies_path = \"ml-20m/movies.csv\"\n",
        "ratings_path = \"ml-20m/ratings.csv\""
      ],
      "metadata": {
        "id": "MXJe1UFUucSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8a64edbd-9938-4f1b-8ffa-797577b35777"
        },
        "id": "dFpcflF_ovlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data with Spark Dataframe API.  \n",
        "Loading a csv with the RDD API is not supported out of the box and is painful to implement."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "77642c8b-87f0-4a60-a9bc-d32d494ca168"
        },
        "id": "cov1MXjcovln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df = spark.read.options(header=True, inferSchema=True).csv(ratings_path)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "15ab9a82-ad82-4801-99f6-2e03ec817660"
        },
        "id": "ZcljLfzLovlo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did you notice this created a job in the Spark UI? I thought Spark was lazy until we requested an action ?  \n",
        "Rerun the same command with inferSchema=False and compare the schema with the command df.printSchema(). Can you understand why Spark triggered a job and what it was for ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d7843f69-dd3a-459e-8ea4-c065585e5f89"
        },
        "id": "KqLInDF0ovlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two jobs are created when hen using inferSchema option. Spark needs to scan the whole dataset in order to infer the data type of each column. Yet, if you disable this option, you will realize that there is still one short job created. So much for the laziness ! To generate the dataframe, Spark needs to know how much columns we have inside each row. That's why a first job is created. Let's keep the inferSchema option set to True for now."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "81b715c5-898c-4150-9866-4f02569f3b55"
        },
        "id": "104UuWK_ovls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df.take(1)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0460f469-e466-42e3-a138-b106bdad3e0f"
        },
        "id": "jHyU15LUovlu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_rdd = ratings_df.rdd.map(lambda x: x.asDict())"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9465e1ea-1f6f-452a-9656-d3a04c0f4e30"
        },
        "id": "847xh8_iovlw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_rdd.take(3)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8d111954-6f6e-4eb3-8039-52b89ff04644"
        },
        "id": "zM9WUoBqovly"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Record type of a dataframe is the 'Row'. You can have any record type inside your RDD, we are using Python dictionaries there."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cb086b19-108c-439e-a1de-6cc488374168"
        },
        "id": "tAj1Sf5bovlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df = spark.read.options(header=True, inferSchema=True).csv(movies_path)\n",
        "movies_rdd = movies_df.rdd.map(lambda x: x.asDict())"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f49413b8-57bc-4c07-aceb-3364a2f5c157"
        },
        "id": "fkl5khoIovl0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "movies_rdd.take(1)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6207a91e-e1a7-49e3-952e-2a28364c7dec"
        },
        "id": "mh7kqiU_ovl1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ratings RDD is a bit large (about 2 min to run a request on it on a docker container with two cores). You can work on a smaller version of it to develop and debug your job and then run it on the full RDD to get the result.  \n",
        "Why do we persist the small RDD and not the regular one ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7d4a2e2a-fcc1-4b09-8477-39344a4afafd"
        },
        "id": "fqYCQueSovl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_small_rdd = ratings_rdd.filter(lambda x: x['userId'] < 20000).persist(pyspark.StorageLevel.DISK_ONLY)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7315fb92-e7d1-4584-8468-73c92c905153"
        },
        "id": "zJTj7fMhovl5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we persist the non-filtered RDD, we will lose all the benefits of the persist function and we will need to read the whole dataset every time. Some other remarks: when doing real analysis, filtering on userId may yield biased results, because you are likely to work with oldest users subscribed to MovieLens ; persist-to-disk function benefits are lost if you stop your Spark session. If your analysis is spanning over multiple sessions, you should save your dataset to distributed file system ; persist-to-memory may be adapted if you are running an iterative algorithm, but be vary, when using persist-to-memory, memory of executors may be shared with other users and you don't have a full guarantee that some partitions won't be recomputed from scratch at some point."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "68ea281e-3d3d-46f3-ab76-dcd276727241"
        },
        "id": "N6gkkQhxovl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will sample dataset and save it to DFS, then read-it again."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5d7b5648-2866-4a58-9fa3-2a600fd9d65e"
        },
        "id": "L-VNdwVHovl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_path = \"ml-20m/sampled_ratings.csv\"\n",
        "ratings_df.sample(fraction=0.1).write.format(\"csv\").save(sampled_path, mode=\"overwrite\", header=True)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ab9176e5-9d2c-41ee-9994-c39c6f1d82fb"
        },
        "id": "5M9AtKh0ovl7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_small_df = spark.read.options(header=True, inferSchema=True).csv(sampled_path)\n",
        "ratings_small_rdd = ratings_small_df.rdd.map(lambda x: x.asDict())"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "27ef56bf-c7aa-4a6e-aa5c-e10982250646"
        },
        "id": "vL5oQbccovl7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the sampled rdd when tinkering with your RDD. When you are sure about what you are doing, you can try to use the entire RDD."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5768a9ee-d8c3-4aa3-8964-c6ab17507565"
        },
        "id": "zV6_pBDeovl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. How many ratings ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f2bbaa94-00a4-4918-9a50-0e36a2624599"
        },
        "id": "FpzYeUk_ovl9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6e4c40d3-a322-4a38-95c6-d988d11dc14c"
        },
        "id": "qVr6u71xovl-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. How many users ?\n",
        "\n",
        "Read the documentation for the distinct function in the RDD API and find a solution with this method.\n",
        "There is another solution relying on a more generic function ? Can you solve the problem without using distinct function ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "95d32f14-b440-4887-b7b7-b1812a30c83b"
        },
        "id": "cRyI-MwtovmA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d43cdf33-30e9-4173-8cc0-b6f737c7930a"
        },
        "id": "B_hprYtDovmA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. How many ratings per grade ?\n",
        "\n",
        "How many users rated a movie with grade r for r in [0,5]?    \n",
        "Plot it. Do you notice something unusual ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "bcafcfa1-8fb3-40bc-8924-407fdca54fd8"
        },
        "id": "NZLObi3eovmB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "04289a83-cda5-48b0-8ecb-7c0652ac71f3"
        },
        "id": "ZOtkk9nAovmC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. Histogram of number of ratings per user\n",
        "\n",
        "Plot the distribution of the number of movies rated per user. In other words, what is the fraction of users that rated between bins[i] and bins[i+1] movies for the following bins.  \n",
        "What is the average and median number of ratings per user?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "32d301ab-9023-40cb-a481-774c72e0cd75"
        },
        "id": "yFr4a61WovmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bins = np.unique(np.logspace(0, 160, base=1.05, num=50, dtype='int32'))\n",
        "bins"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0544493a-3987-4623-a0b1-34fe95905722"
        },
        "id": "2reXpkrYovmG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1280b702-8faa-4938-a848-b4b1e8a3a37f"
        },
        "id": "LVyoDcsMovmG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. Most popular movies\n",
        "\n",
        "What are the 20 movies with the most ratings ?  \n",
        "We would like the answer with the movie title and not the movie id.  \n",
        "Look at the documentation of the join and top functions."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3fde2c15-5bbd-4643-83cf-f664cfed3d05"
        },
        "id": "5dJO9u8kovmK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d96df17e-ff67-43ef-8699-f3b2dd798a48"
        },
        "id": "IYxsAF59ovmK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. Writing partioned datasets\n",
        "\n",
        "The ratings dataset is available as one big csv file. It is not very convenient since we have to go through the entire file to look for ratings for a specific userId. Moreover, we cannot open only a small part of the dataset.  \n",
        "Could you write the ratings dataset into 16 files located in /tmp/ratings/part=X/ratings.csv for X in [0, 16[ where userId in part=X are such that userId % 16 == X ?  Your function should return the list of written files with the number of ratings for each file.\n",
        "Look at the documentation of partitionBy and mapPartitionsWithIndex."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c2640dd5-bc4c-4fbe-9049-cbcac05b8e2e"
        },
        "id": "8XiTvAzKovmV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "32f3748b-9925-4997-9940-860392f7b51e"
        },
        "id": "FFMmgDy-ovmX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. Most popular genre per year\n",
        "\n",
        "For every year since 1980, determine what is the most popular genre.  \n",
        "Look at the documentation of the flatMap function."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "989bc3f3-9fdf-4867-b3bc-0161e71c2861"
        },
        "id": "vUz09o9povma"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b6035a2e-c6c1-4257-a9c7-bfb12751c8e4"
        },
        "id": "bEMIiJitovmb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8.  Best movies\n",
        "\n",
        "Amongst the movies with at least 1000 ratings, what are the top 20 movies per median rating ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "13200f13-1641-4774-bb22-00b75c9b377c"
        },
        "id": "xctn2Rn6ovmj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "15b1dc71-19e5-4e6e-a6ad-9998d9a1b418"
        },
        "id": "TVMQsaraovmk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # When you're done with a session you've created, stop it\n",
        "spark.stop()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "00572075-6e9d-49a5-9c54-9f40e2999e6a"
        },
        "id": "vyr3hrSGovmm"
      },
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.6.15",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "name": "rdd",
    "notebookId": 869522255783514,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "rdd solution",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 78139586907239
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}