{"cells":[{"cell_type":"markdown","source":["# Exploring the Movielens dataset with the Spark RDD API"],"metadata":{}},{"cell_type":"code","source":["%matplotlib inline\n\nimport urllib\nimport urllib.request as req\nimport zipfile\nimport glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport pyspark\nfrom pyspark.sql import SparkSession"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Downloading the dataset"],"metadata":{}},{"cell_type":"code","source":["url = 'http://files.grouplens.org/datasets/movielens/ml-20m.zip'\nfilehandle, _ = urllib.request.urlretrieve(url)\nzip_file_object = zipfile.ZipFile(filehandle, 'r')\nzip_file_object.namelist()\n\nzip_file_object.extractall()\n\nmovies_path = \"file:///databricks/driver/ml-20m/movies.csv\"\nratings_path = \"file:///databricks/driver/ml-20m/ratings.csv\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Loading the data"],"metadata":{}},{"cell_type":"code","source":["ss = SparkSession.builder \\\n    .master(\"local[*]\")  \\\n    .appName('movielens-rdd') \\\n    .getOrCreate()\n\nsc = ss.sparkContext\nsc"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Loading data with Spark Dataframe API.  \nLoading a csv with the RDD API is not supported out of the box and is painful to implement."],"metadata":{}},{"cell_type":"code","source":["ratings_df = spark.read.options(header=True, inferSchema=True).csv(ratings_path)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Did you notice this created a job in the Spark UI? I thought Spark was lazy until we requested an action ?  \nRerun the same command with inferSchema=False and compare the schema with the command df.printSchema(). Can you understand why Spark triggered a job and what it was for ?"],"metadata":{}},{"cell_type":"code","source":["ratings_rdd = ratings_df.rdd.map(lambda x: x.asDict())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["ratings_rdd.take(10)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["movies_df = ss.read.csv(f'/tmp/{movielens_dir}/movies.csv', header=True, inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["movies_rdd = movies_df.rdd.map(lambda x: x.asDict())"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["movies_rdd.take(10)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["The ratings RDD is a bit large (about 2 min to run a request on it on a docker container with two cores). You can work on a smaller version of it to develop and debug your job and then run it on the full RDD to get the result.  \nWhy do we persist the small RDD and not the regular one ?"],"metadata":{}},{"cell_type":"code","source":["ratings_small_rdd = ratings_rdd.filter(lambda x: x['userId'] < 20000).persist(pyspark.StorageLevel.DISK_ONLY)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Q1. How many ratings ?"],"metadata":{}},{"cell_type":"markdown","source":["### Q2. How many users ?\n\nRead the documentation for the distinct function in the RDD API.  \nCan you compute it without using distinct ?"],"metadata":{}},{"cell_type":"markdown","source":["### Q3. How many ratings per grade ?\n\nHow many users rated a movie with grade r for r in [0,5]?    \nPlot it. Do you notice something unusual ?"],"metadata":{}},{"cell_type":"markdown","source":["### Q4. Histogram of number of ratings per user\n\nPlot the distribution of the number of movies rated per user. In other words, what is the fraction of users that rated between bins[i] and bins[i+1] movies for the following bins.  \nWhat is the average and median number of ratings per user?"],"metadata":{}},{"cell_type":"code","source":["bins = np.unique(np.logspace(0, 160, base=1.05, num=50, dtype='int32'))\nbins"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Q5. Most popular movies\n\nWhat are the 20 movies with the most ratings ?  \nWe would like the answer with the movie title and not the movie id.  \nLook at the documentation of the join and top functions."],"metadata":{}},{"cell_type":"markdown","source":["### Q6. Writing partioned datasets\n\nThe ratings dataset is available as one big csv file. It is not very convenient since we have to go through the entire file to look for ratings for a specific userId. Moreover, we cannot open only a small part of the dataset.  \nCould you write the ratings dataset into 16 files located in /tmp/ratings/part=X/ratings.csv for X in [0, 16[ where userId in part=X are such that userId % 16 == X ?  Your function should return the list of written files with the number of ratings for each file.\nLook at the documentation of partitionBy and mapPartitionsWithIndex."],"metadata":{}},{"cell_type":"markdown","source":["### Q7. Most popular genre per year\n\nFor every year since 1980, determine what is the most popular genre.  \nLook at the documentation of the flatMap function."],"metadata":{}},{"cell_type":"markdown","source":["### Q8.  Best movies\n\nAmongst the movies with at least 1000 ratings, what are the top 20 movies per median rating ?"],"metadata":{}},{"cell_type":"code","source":["ss.stop()"],"metadata":{},"outputs":[],"execution_count":26}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.3","nbconvert_exporter":"python","file_extension":".py"},"name":"rdd","notebookId":3975475116327523},"nbformat":4,"nbformat_minor":0}
