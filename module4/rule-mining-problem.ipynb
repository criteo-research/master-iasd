{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Session 3: Association rule mining\n",
    "\n",
    "In this session, we will build a first set of algorithms to infer rules from a dataset.  This is known as association rule mining (e.g. people who buy potates and bread are likely to be building a burger and therefore interested in salad and steaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and unzipping the data (MovieLens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "url = 'http://files.grouplens.org/datasets/movielens/ml-20m.zip'\n",
    "filehandle, _ = urllib.request.urlretrieve(url, '/tmp/data.zip')\n",
    "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
    "zip_file_object.namelist()\n",
    "zip_file_object.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Dataset\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_path = \"file:///databricks/driver/ml-20m/movies.csv\"\n",
    "ratings_path = \"file:///databricks/driver/ml-20m/ratings.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the csv files using [`spark.read`](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read.options(header=True).csv(movies_path)\n",
    "# TASK 1: explain what the filter below does.  Why did we not use sample instead?\n",
    "ratings_df = spark.read.options(header=True).csv(ratings_path).filter(sf.expr('PMOD(HASH(userId),10)')==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cache the read dataframes to avoid reloading them in subsequent computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.cache()\n",
    "ratings_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print a few rows from each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.select('movieId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.select('userId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as sf\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: filter ratings to keep only the latest 100 ratings per user\n",
    "# Hint: create a new column called tm_rank that sorts the ratings per timestamp and per hash of movie id.\n",
    "#       use sf.rank(), sf.struct(), sf.hash() to create this column, then filter() to filter the data\n",
    "#       do not forget to drop this new column once you are done.\n",
    "lim_ratings_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: this should return a min of 20 and a max of 100\n",
    "lim_ratings_df\\\n",
    "    .groupby('userId')\\\n",
    "    .agg(sf.count('*').alias('num_ratings'))\\\n",
    "    .agg(sf.min('num_ratings'), sf.max('num_ratings'))\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive approach: Find recurring pairs & triplets.\n",
    "This approach is simple and not efficient but gives you a baseline and intuition for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: find recurring pairs with a naive approach, then show the top 25 results\n",
    "# Remember to use the title field to make the results interpretable\n",
    "# Also, make sure to work with lim_ratings_df, not ratings_df!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movie_pairs_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4: find recurring triplets and show the top 25 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Second approach: A priori  \n",
    "Implement your own version of A priori.  You may use resources from the web.\n",
    "https://fr.wikipedia.org/wiki/Algorithme_APriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5: implement the a priori approach to find recurring pairs and triplets more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the execution time of the different approaches in your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FP-growth\n",
    "https://www.softwaretestinghelp.com/fp-growth-algorithm-data-mining/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 6: explain how FP-growth works in a few lines (in English or French!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 7: Use FP Growth from the Spark MLlib to generate rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Validation\n",
    "Build a validation dataset to debug your code (naive, a priori, fp-growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 8: implement your validation dataset here and run your naive, a priori and FP growth code.  Report running times.\n",
    "lim_ratings_df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading & instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must return your notebook before ** Sunday Feb 9th midnight Paris time ** by email to Amine, Marc and Olivier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grade will be composed of :\n",
    "1. Timely return\n",
    "2. Correctness (how you built and used your validation dataset)\n",
    "3. Readability\n",
    "4. Performance (this is not a race but we want to see that you compared the running time of your three algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "rule-mining",
  "notebookId": 258734209120091
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
