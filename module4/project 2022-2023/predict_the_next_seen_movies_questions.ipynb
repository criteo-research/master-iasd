{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoNpnIQOBcfx"
   },
   "source": [
    "# Predict the next seen movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nMdnOrdauzb"
   },
   "source": [
    "> The goal of this project is to validate your skills learned during the previous tutorials with the PySpark distributed computing software. You will build and test several implementations of recommendation systems. Here are the parts of the project:\n",
    "> \n",
    "> * Part A: Load and preprocess the dataset. \n",
    "> * Part B: Build a first set of recommendation algorithms: naive with recurring pairs, a priori, and fp-growth to infer rules from a dataset.\n",
    "> * Part C: Implement the PLSI algorithm (\"Probabilistic Latent Semantic Indexing\"), which is one way to create embeddings from a dataset. These embeddings can be used to fuel a recommendation engine.\n",
    "> * Part D: Predict the next movies seen by a user in function of the last movies that he/she has seen with the previous implemented algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69sUuh3EEEBw"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxRuqODPWgYM"
   },
   "source": [
    "## Install Spark Environment\n",
    "\n",
    "> Since you are running on Google Colab, you will need to install Spark by ourselves, every time we run a new session. You need to install Spark, as well as a Java Runtime Environment.  Then you need to setup a few environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjh2g1iwrxAQ"
   },
   "source": [
    "Execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u89WlZmhWPgW"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!curl -O https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.3-bin-hadoop3.2.tgz\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsVv0JfGC1me"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IInRgu9Hrnse"
   },
   "source": [
    "or directly with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFR3wKqDrrsZ"
   },
   "outputs": [],
   "source": [
    "#!pip install pyspark==3.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBTHZp82G_Ea"
   },
   "source": [
    "Create and launch a Spark session with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "RKP62k6BW5Xz",
    "outputId": "3717708e-b09c-4b2d-82e6-508091a9f422",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().set('spark.ui.port', '4050')\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "  .master('local[*]')\\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "yA7rmX4p8BBo",
    "outputId": "032fdcab-c4d1-46f1-fa52-81c9c6aeef0c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWQFahH3HfJ9"
   },
   "source": [
    "/!\\ The Spark UI link is not accessible. Use the optional next session if you want to access it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bDPyOrBGzkC"
   },
   "source": [
    "Uncomment and execute the following line, if you want to close and stop the created Spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXvFSpAkGyMm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5yHDm0vXkIQ"
   },
   "source": [
    "## Optional step: Enable Spark UI access through a secure tunnel\n",
    "\n",
    "> This step is useful if you want to look at Spark UI.\n",
    "First, you need to create a free ngrok account : https://dashboard.ngrok.com/login.  \n",
    "Then connect on the website and copy your AuthToken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2bV9Ydi8sLs",
    "outputId": "c05116b9-54c9-489e-e61c-3545e13d01d6"
   },
   "outputs": [],
   "source": [
    "!uname -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCerOlcLXmgj",
    "outputId": "1ebd17be-82b4-4b5f-e7fc-36e8d1ada469"
   },
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHgGqKOeAYVH",
    "outputId": "09fe2b9d-700d-4c9f-d911-3e4fd11c866f"
   },
   "outputs": [],
   "source": [
    "!./ngrok authtoken '2HYfdysedjgB5lcoeE8lqqShgIe_sZUm49MquX5okpssVMYQ' # <-------------- change this line !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1G-XfsyHAT84"
   },
   "outputs": [],
   "source": [
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jWA3ILoOBAw",
    "outputId": "1b9b2d20-9fb8-4679-bf8b-e3648f16b31b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "\n",
    "ngrok_tunnels = urllib.request.urlopen('http://localhost:4040/api/tunnels').read().decode('utf8')\n",
    "spark_ui_url = json.loads(ngrok_tunnels)['tunnels'][0]['public_url']\n",
    "print(\"Spark UI:\", spark_ui_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSX0QbniXqdR"
   },
   "source": [
    "## Other Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb4tXmFUWMVr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request as req\n",
    "import zipfile\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from fractions import Fraction\n",
    "from decimal import Decimal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFUvUg2ufEkJ"
   },
   "source": [
    "# Part A - Dataset (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGC6YNJm8wh2"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_d9HFUnfPYY"
   },
   "outputs": [],
   "source": [
    "url = 'http://files.grouplens.org/datasets/movielens/ml-20m.zip'\n",
    "filehandle, _ = req.urlretrieve(url)\n",
    "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
    "zip_file_object.namelist()\n",
    "zip_file_object.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3lK0389fXkK",
    "outputId": "6799d146-b413-49bd-bc75-07ec85377e8b"
   },
   "outputs": [],
   "source": [
    "!find ml-20m -type f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uwruo0oIfaop",
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_path = \"ml-20m/movies.csv\"\n",
    "ratings_path = \"ml-20m/ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWQSjOtYWMVx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_df_csv = spark.read.options(header=True, inferSchema=True).csv(ratings_path)\n",
    "movies_df_csv = spark.read.options(header=True, inferSchema=True).csv(movies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1M21UsEBv5Up",
    "outputId": "3ed86c02-39e1-4d19-fea9-211c80f6d4c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y39-SiqZv9hu",
    "outputId": "3fc2191b-389b-4695-dddd-dc1c85282188",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratings_df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtR77Ttq8zT-"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "> In the following sections, we will work with algorithms that may scale more or less well with the size of the dataset. In this section we will develop several functions that will allow you to sample the dataset. Then in the next part of the notebook, it will be up to you to choose which tools you want to apply to your dataset in order to train your models, knowing that we want to have reasonable processing times (in the order of a minute), while having models that 'work' well.\n",
    ">\n",
    "> **Test all the functions that process the dataframe on a toy example, like what is done in first data processing question.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1zW1MHjDZnu"
   },
   "source": [
    "### Question A1\n",
    "\n",
    ">Write the *ratings_df_csv* and *movies_df_csv* DataFrame in a compressed parquet format.\n",
    ">\n",
    ">Reload them from parquet, to make next computations faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stqYh6SJGTCa"
   },
   "source": [
    "### Question A2\n",
    "\n",
    "> Compute an estimation of the whole *ratings_df* dataset size in memory.\n",
    ">\n",
    ">Find the amount of partitions used by *ratings_df*.\n",
    ">\n",
    ">We try to have partitions not too big (in order not to crash our executors), and not to small (dealing with too many small partitions can lead to issues, on driver side for example). For this reason, a rule of thumb to have partitions of 128MB is okay. Given this, what do you think of the dataframe? Is the amount of partitions okay? Or should we repartition it? What would be the function to use if we need to repartition the dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IESUqOGp_rDf"
   },
   "source": [
    "### Question A3\n",
    "\n",
    ">Create a function named *remove_bad_ratings* that takes a rating dataframe as an argument, and returns a dataframe whose ratings are greater or equals to a rating_threshold, with default value of *3.5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMKLPBXaI0TA"
   },
   "source": [
    "### Question A4\n",
    "\n",
    ">Create a function named *sample_users* that takes a rating dataframe as an argument, and returns a dataframe with only a *ratio* of users (we want to keep all ratings from users that we keep) ; default value of *ratio* parameter is *0.1*. Function should be deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJV6UbAiLGiv"
   },
   "source": [
    "### Question A5\n",
    "\n",
    "> Create a function named *remove_exotic_movies*, taking a rating dataframe as argument, and that removes all movies which have less than *nb_min_ratings* ; *nb_min_ratings* parameter has a default value of *1000*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTKUFM95pAwP"
   },
   "source": [
    "### Question A6\n",
    "\n",
    "> Compute the following stats on the dataset:\n",
    "> - Amount of distinct users\n",
    "> - Amount of distinct movies\n",
    "> - Total amount of ratings\n",
    "> - Let r_u be the amount of ratings made by user u. Study the distribution of r_u over all users (quantiles, histogram...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFjZb6tFmEPt"
   },
   "source": [
    "### Question A7\n",
    "\n",
    "> Create a function named *remove_old_movies_in_timelines*, that takes a ratings dataframe as parameter, and only keeps the *nb_max_movies* most recent movies seen by each user ; *nb_max_movies* parameter is defaulted at 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTveqlrl_T8u"
   },
   "source": [
    "# Part B - Association Rules (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fukr5HVii2BG"
   },
   "source": [
    "## Naive: Recurring pairs\n",
    "\n",
    "> This approach is simple and not efficient but gives you a baseline and intuition for the next steps.\n",
    ">\n",
    "> Morally, what we want to do is:\n",
    "> - for each user, regroup all the movies they have liked inside a single row. We will call this the 'user timeline'\n",
    "> - for each user, generate all pairs of movies across their list of movies.\n",
    "> - for each pair of movies, count the amount of distinct users with this pair.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKvQ69TpwF_N"
   },
   "source": [
    "### Question B1\n",
    "\n",
    "> Create a function named *compute_timeline*, that takes a ratings dataframe as parameter, and returns the 'user timeline', a dataframe following this schema:\n",
    "> - userId : integer\n",
    "> - movies : list[integer] (list of movieId seen by user)\n",
    ">\n",
    "> Test it on a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGO98KnLxKpe"
   },
   "source": [
    "### Question B2\n",
    "\n",
    "> Let's imagine that all of our executors have 4GB of memory. If we consider the 'user timeline' dataset where movie ratings are greater or equal than 3.5, is it okay to store list of movie ids inside rows, as far as memory is concerned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rSlT9Rd3w23"
   },
   "source": [
    "### Question B3\n",
    "\n",
    "> Create a function named *compute_pairs*, that takes a user timeline dataframe as parameter, and returns a dataframe of movie pairs (generated across all movies of their timeline) following this schema:\n",
    "> - userId : integer\n",
    "> - movieId1 : integer\n",
    "> - movieId2 : integer\n",
    ">\n",
    "> You can rely on an udf to generate list of pair of movies from a list of movies.\n",
    "> \n",
    "> Test it on a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD-P_FNL8yEk"
   },
   "source": [
    "### Question B4\n",
    "\n",
    "> Let's imagine that all of our executors have 4GB of memory. \n",
    "> \n",
    "> If we consider If we consider the 'user timeline where movie ratings are greater or equal than 3.5, what will happen when we generate pairs dataframe for this dataset ?\n",
    "> \n",
    "> You need to consider:\n",
    "> - amount of bytes retained by lists of pairs\n",
    "> - amount of partitions we have in user timeline\n",
    ">\n",
    "> Also, consider what may happen because of skew. We may have all big user timelines inside same partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YhKxZd6BdWx"
   },
   "source": [
    "### Question B5\n",
    "\n",
    "> Create a function named *compute_pair_frequencies*, that takes a movie pair dataframe as parameter, and returns a dataframe of movie pairs and their user count, following this schema:\n",
    "> - movieId1 : integer\n",
    "> - movieId2 : integer\n",
    "> - count : integer\n",
    "> \n",
    "> Dataframe should be **ordered**, with most frequent pairs first.\n",
    ">\n",
    "> Test it on a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d81d3Thf6DPD"
   },
   "source": [
    "### Question B6\n",
    "\n",
    "> Quickly test the whole algorithm on *ratings_df* or a subset of it.\n",
    "> \n",
    "> How many shuffles for the whole algorithm ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiZfGfshztLZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuqp0Y6ky8ed"
   },
   "source": [
    "## A priori\n",
    "\n",
    "> You can find a good description of Apriori algorithm here:  \n",
    "> https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    ">\n",
    "> Some other resources:  \n",
    "> [Apriori — Association Rule Mining In-depth Explanation and Python Implementation](https://towardsdatascience.com/apriori-association-rule-mining-explanation-and-python-implementation-290b42afdfc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0E8hbjPFLgd"
   },
   "source": [
    "### Question B7\n",
    "\n",
    "> Implement your own version of A priori to compute most frequent pairs and quickly test it on *ratings_df* or a subset of it.\n",
    "> \n",
    "> You may want to rely on *F.explode* as an alternative to udf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYjT4dDVFSGv"
   },
   "source": [
    "### Question B8\n",
    "\n",
    "> Implement your own version of A priori to compute most frequent triplets.\n",
    "> \n",
    "> A this stage of the 'A priori' section, you are probably doing the same thing multiple times. \n",
    "> \n",
    "> Maybe it's time to factorize your code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRUB_yQA7I-m"
   },
   "source": [
    "## 3. FP-Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can find a good description of FP-Growth algorithm with Spark here:  \n",
    "> https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html\n",
    ">\n",
    "> Some other resources:  \n",
    "[FP Growth — Frequent Pattern Generation in Data Mining with Python Implementation](https://towardsdatascience.com/fp-growth-frequent-pattern-generation-in-data-mining-with-python-implementation-244e561ab1c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5CJmMyFW9Q"
   },
   "source": [
    "### Question B9\n",
    "\n",
    "> Use the Spark version of FP-Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj1HcZkRGRRs"
   },
   "source": [
    "# Part C - Probabilistic Latent Semantic Model (5 points)\n",
    "\n",
    "> Aim of this section is to implement a Probabilistic Latent Semantic Model.\n",
    "> \n",
    "> We will use an expectation maximization algorithm to learn its parameters.\n",
    ">\n",
    "> In the first set of questions you will implement some utility functions to deal with matrix manipulations.\n",
    ">\n",
    "> In the second set of questions, you will implement the algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "papRtCuWJxho"
   },
   "source": [
    "## Matrix manipulation functions\n",
    "\n",
    "> We will implement matrix operations that will be usefull to run the PLSI algorithm afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtRkYH9NC-RO"
   },
   "source": [
    "### Question CMatrix1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzVkNoL4KQ0L"
   },
   "source": [
    "#### `matrix_sum_rows` \n",
    "\n",
    "> Takes a matrix (a column containing arrays of fixed length) and returns the sum of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1VyUs79KUeg"
   },
   "outputs": [],
   "source": [
    "# Hint: https://stackoverflow.com/a/57448698/2015762\n",
    "def matrix_sum_rows(col_name, length_of_array):\n",
    "    # ...\n",
    "    pass\n",
    "\n",
    "input_array = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\n",
    "expected_output = input_array.sum(axis=1)\n",
    "print('Input array')\n",
    "print(input_array)\n",
    "print('Expected output')\n",
    "print(expected_output)\n",
    "print('Obtained output')\n",
    "(\n",
    "    spark.sparkContext.parallelize(input_array.tolist()).map(lambda x: Row(matrix=x)).toDF()\n",
    "    .withColumn('row_sum', matrix_sum_rows('matrix', 4))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRczikuRIiwH"
   },
   "source": [
    "### Question CMatrix2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFJwS3HAKYDj",
    "tags": []
   },
   "source": [
    "#### `matrix_sum_columns`\n",
    "\n",
    "> Takes a matrix (a column containing arrays of fixed length) and returns the sum of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df36LGTSKclE"
   },
   "outputs": [],
   "source": [
    "# Hint: https://stackoverflow.com/a/54382990/2015762\n",
    "def matrix_sum_columns(col_name, length_of_array):\n",
    "    # ...\n",
    "    pass\n",
    "\n",
    "input_array = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\n",
    "expected_output = input_array.sum(axis=0)\n",
    "print('Input array')\n",
    "print(input_array)\n",
    "print('Expected output')\n",
    "print(expected_output)\n",
    "print('Obtained output')\n",
    "(\n",
    "    spark.sparkContext.parallelize(input_array.tolist()).map(lambda x: Row(matrix=x)).toDF()\n",
    "    .select(matrix_sum_columns('matrix', 4).alias('col_sum'))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47EAeV3_IqpH"
   },
   "source": [
    "### Question CMatrix3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCWm2vK4KhES"
   },
   "source": [
    "#### `matrix_normalize_rows`\n",
    "\n",
    "> Takes a matrix (a column containing arrays of fixed length) and returns the same matrix where the rows have been divded by their sum, such that each row sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsNhEtSuKZBI"
   },
   "outputs": [],
   "source": [
    "def matrix_normalize_rows(col_name, length_of_array):\n",
    "    # ...\n",
    "    pass\n",
    "\n",
    "input_array = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\n",
    "expected_output = input_array / input_array.sum(axis=1).reshape(-1, 1)\n",
    "print('Input array')\n",
    "print(input_array)\n",
    "print('Expected output')\n",
    "print(expected_output)\n",
    "print('Obtained output')\n",
    "(\n",
    "    spark.sparkContext.parallelize(input_array.tolist()).map(lambda x: Row(numbers=x)).toDF()\n",
    "    .withColumn('normalized_elements', matrix_normalize_rows('numbers', 4))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdOIy3UaIu9P"
   },
   "source": [
    "### Question CMatrix4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPWSuJVkKkoS"
   },
   "source": [
    "#### `matrix_elementwise_product`\n",
    "\n",
    "> Takes two matrices and return their elementwise product (aka. Hadamard product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EHBDsfzKovu"
   },
   "outputs": [],
   "source": [
    "def matrix_elementwise_product(col_name_1, col_name_2, length_of_array):\n",
    "    # ...\n",
    "    pass\n",
    "\n",
    "input_array_1 = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\n",
    "input_array_2 = np.array([[1, 2, 1, 2], [10, 20, 10, 20]], dtype=float)\n",
    "expected_output = input_array_1 * input_array_2\n",
    "print('Input array')\n",
    "print(input_array_1)\n",
    "print(input_array_2)\n",
    "print('Expected output')\n",
    "print(expected_output)\n",
    "print('Obtained output')\n",
    "(\n",
    "    spark.sparkContext.parallelize(zip(input_array.tolist(), input_array_2.tolist())).map(lambda x: Row(numbers_1=x[0], numbers_2=x[1])).toDF()\n",
    "    .withColumn('elementwise_products', matrix_elementwise_product('numbers_1', 'numbers_2', 4))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G7umlGqIx0F"
   },
   "source": [
    "### Question CMatrix5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLayzlbbKvS2"
   },
   "source": [
    "#### `matrix_elementwise_divide`\n",
    "\n",
    "> Takes two matrices and divide elementwise the first one by the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZ2Mhzj9KwT6"
   },
   "outputs": [],
   "source": [
    "def matrix_elementwise_divide(col_name_1, col_name_2, length_of_array):\n",
    "    # ...\n",
    "    pass\n",
    "\n",
    "input_array_1 = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\n",
    "input_array_2 = np.array([[1, 2, 1, 2], [10, 20, 10, 20]], dtype=float)\n",
    "expected_output = input_array_1 / input_array_2\n",
    "print('Input array')\n",
    "print(input_array_1)\n",
    "print(input_array_2)\n",
    "print('Expected output')\n",
    "print(expected_output)\n",
    "print('Obtained output')\n",
    "(\n",
    "    spark.sparkContext.parallelize(zip(input_array.tolist(), input_array_2.tolist())).map(lambda x: Row(numbers_1=x[0], numbers_2=x[1])).toDF()\n",
    "    .withColumn('elementwise_divided', matrix_elementwise_divide('numbers_1', 'numbers_2', 4))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwbYk4FxJ4oF"
   },
   "source": [
    "## PLSI\n",
    "\n",
    "> With\n",
    "> * N the number of users u\n",
    "> * M the number of movies s\n",
    "> * L the number of latent classes z\n",
    "> * T number of (user, movie) interactions (each interaction (s_t, u_t) means user u_t liked movie s_t)\n",
    ">\n",
    ">We suppose that the probability that a user will like a movie can be written in the form of a mixture model given by the equation:\n",
    "$$\n",
    "p(s|u) = \\sum_{z=1}^L p(s|z) p(z|u)\n",
    "$$\n",
    "And we want to optimize the likelihood of the observed user interactions\n",
    "$$\n",
    "L = - \\frac{1}{T} \\sum_{1}^{T} \\log p(s_t|u_t) = - \\frac{1}{T} \\sum_{1}^{T} \\sum_{z=1}^L p(s_t|z) p(z|u_t)\n",
    "$$\n",
    "That can be done using an EM algorithm working as follow:\n",
    ">\n",
    ">**E step**\n",
    ">\n",
    ">For each interaction (u_t, s_t), compute for all z = 1, ..., L:\n",
    "$$\n",
    "p(z|(u_t, s_t)) = \\frac{p(s_t|z) p(z|u_t)}{\\sum_z p(s_t|z) p(z|u_t)}\n",
    "$$\n",
    ">\n",
    ">**M step**\n",
    ">\n",
    ">Find each movie probability given a latent class\n",
    "$$\n",
    "p(s|z) = \\frac{N(z, s)}{N(z)} \n",
    "\\quad \\text{where} \\quad N(z, s) = \\sum_s \\sum_u p(z|(u, s)) \n",
    "\\quad \\text{and} \\quad N(z) = \\sum_s N(z, s)\n",
    "$$\n",
    "Find each latent class probability given each user.\n",
    "$$\n",
    "p(z|u) = \\frac{\\sum_s p(z|(u, s))}{\\sum_z \\sum_s p(z|(u, s))}\n",
    "$$\n",
    ">\n",
    ">We will have the following dataframes\n",
    ">\n",
    ">* `count_z_s`: M rows, with columns  `movieId`, `N(z,s)`.\n",
    ">* `count_z`: 1 row, with column `N(z)`.\n",
    ">* `p_s_knowing_z`: M rows, with columns  `movieId`, `p(s|z)`. For a given z, the sum of p(s|z) equals 1.\n",
    ">* `p_z_knowing_u`: N rows, with columns `userId`, `p(z|u)`. For a given u, the sum of p(z|u) equals 1.\n",
    ">* `p_z_knowing_u_and_s`: N x M rows, with columns `userId`, `movieId`, `p(z|u,s)`.\n",
    ">\n",
    "> \n",
    "> Implement the PLSI algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question CPLSI1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDGcbe41LwPV"
   },
   "outputs": [],
   "source": [
    "def get_count_z(count_z_s, n_latent_classes):\n",
    "    \"\"\"Compute N(z) = sum_s N(z,s)\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass\n",
    "\n",
    "count_z_s = ss.sparkContext.parallelize([\n",
    "    st.Row(**{\"movieId\":1, \"N(z,s)\": [1., 3., 4.]}),\n",
    "    st.Row(**{\"movieId\":2, \"N(z,s)\": [4., 5., 0.]}),\n",
    "]).toDF()\n",
    "get_count_z(count_z_s, 3).show()\n",
    "# Expected [5., 8., 4.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question CPLSI1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyftz13QLyez"
   },
   "outputs": [],
   "source": [
    "def get_count_z_s(p_z_knowing_u_and_s, n_latent_classes):\n",
    "    \"\"\"Compute N(z,s) = sum_u p(z|u,s)\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question CPLSI1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-hhH_jgLzaN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_p_s_knowing_z(count_z_s, count_z, n_latent_classes):\n",
    "    \"\"\"Compute p(s|z) = N(z,s) / N(z)\n",
    "    \n",
    "    Hint: crossJoin may help\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question CPLSI1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8qlxgcWL2Pc"
   },
   "outputs": [],
   "source": [
    "def get_p_z_knowing_u(p_z_knowing_u_and_s, n_latent_classes):\n",
    "    \"\"\"Compute p(z|u) = sum_s p(z|u,s) / sum_z sum_s p(z|u,s)\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question CPLSI1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvjuAPagL49D"
   },
   "outputs": [],
   "source": [
    "def get_p_z_knowing_u_and_s(observed_pairs, count_z_s, count_z, p_z_knowing_u, n_latent_classes):\n",
    "    \"\"\"For all pairs of observed (u, s)\n",
    "    \n",
    "    Compute p(z|u,s) = [N(z, s) / N(z) * p(z|u)] / sum_z [N(z, s) / N(z) * p(z|u)]\n",
    "                     = [p(s|z) * p(z|u)] / sum_z [p(s|z) * p(z|u)]\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question CPLSI1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Lb6J5x_L7sj"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(observed_pairs, count_z_s, count_z, p_z_knowing_u, n_latent_classes):\n",
    "    \"\"\"Compute the log likelihood of the observed pairs\n",
    "    \n",
    "    L = - 1 / T * sum_t log[ p(s|u) ]\n",
    "      = - 1 / T * sum_t log[ sum_z p(s|z) * p(z|u) ]\n",
    "    \"\"\"\n",
    "   # ...\n",
    "   pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question CPLSI1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-x-AT8iL-TD"
   },
   "outputs": [],
   "source": [
    "def initialize_statistics(observed_pairs, n_latent_classes):\n",
    "    \"\"\"Initialize either p(s|z) and p(z|u) or p(z|(u, s)) to be able to fuel the first iteration of the EM algorithm.\n",
    "    What would happen if you initialize these to a constant value ?\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question CPLSI1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQiwyBZVMAu-"
   },
   "outputs": [],
   "source": [
    "def run_plsi(observed_pairs, n_iterations, n_latent_classes, checkpoint_every=1):\n",
    "    start_init_time = time.time()\n",
    "    spark.sparkContext.setJobDescription(\"Initialization\")\n",
    "\n",
    "    # ... = initialize_statistics(observed_pairs, n_latent_classes)\n",
    "    llh = log_likelihood(observed_pairs, # ... #, n_latent_class\n",
    "    mlflow.log_metric(key=\"llh\", value=llh, step=0)\n",
    "    print(f'LLH: {llh:.10f}')\n",
    "    \n",
    "    end_init_time = time.time()\n",
    "    print(f'Initialization: {end_init_time - start_init_time:.1f}s')\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        start_e_step = time.time()\n",
    "        spark.sparkContext.setJobDescription(f\"Iteration {i+1}: E-step\")\n",
    "        # E step\n",
    "        # ...\n",
    "        \n",
    "        end_e_step = time.time()\n",
    "        print(f'Iteration {i+1}: E-step: {end_e_step - start_e_step:.1f}s')\n",
    "        \n",
    "        spark.sparkContext.setJobDescription(f\"Iteration {i+1}: M-step\")\n",
    "        # M step\n",
    "        # ...\n",
    "        \n",
    "        llh = log_likelihood(observed_pairs, count_z_s, count_z, p_z_knowing_u, n_latent_classes)\n",
    "        mlflow.log_metric(key=\"llh\", value=llh, step=i+1)\n",
    "        \n",
    "        end_m_step = time.time()\n",
    "        print(f'Iteration {i+1}: M-step: {end_m_step - end_e_step:.1f}s')\n",
    "        print(f'LLH: {llh:.10f}')\n",
    "    \n",
    "    return get_p_s_knowing_z(count_z_s, count_z, n_latent_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er6q4Xm5Jj7W"
   },
   "outputs": [],
   "source": [
    "!pip install -q mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dV0zwPSlJkqk"
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpBo-j0dMGIs"
   },
   "outputs": [],
   "source": [
    "n_iterations = 20\n",
    "n_latent_classes = 5\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"n_iterations\", n_iterations)\n",
    "    mlflow.log_param(\"n_latent_classes\", n_latent_classes)\n",
    "    run_plsi(ratings_df.sample(0.1), n_iterations=n_iterations, n_latent_classes=n_latent_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK3XqiypKREd"
   },
   "source": [
    "### Question CPLSI2.1\n",
    "\n",
    "> How does the EM algorithm is supposed to scale with the number of EM steps ? Do you observe such a scaling ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK3XqiypKREd"
   },
   "source": [
    "### Question CPLSI2.2\n",
    "> If each steap takes longer than the previous one: Try using .cache() wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK3XqiypKREd"
   },
   "source": [
    "### Question CPLSI2.3\n",
    "\n",
    "> Try to unpersist your dataframes when they become unneeded (look at the Storage tab in the Spark UI) (Optional + 2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK3XqiypKREd"
   },
   "source": [
    "### Question CPLSI2.4\n",
    "\n",
    "> If after few steps (typically 5), your algorithm starts being much slower and spend more and more time scheduling jobs (look in the Spark UI), try using [.localCheckpoint()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.localCheckpoint). How does it differ from caching ? What are the benefits and the drawbacks ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbstFMhYnpSk"
   },
   "source": [
    "# Part D - Test them all (5 points)\n",
    "\n",
    "> In this section, we create training and test datasets, and test all the different prediction algorithms described above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "engmZXePsEfv"
   },
   "source": [
    "## Training and Testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIxB2g3QsKGC"
   },
   "source": [
    "### Question D1\n",
    "\n",
    "> Create the training dataset named *training_df*. It is made of raw ratings dataframe, where:\n",
    "> - *hash(userId) % 2 == 0*\n",
    "> - and *rating >= 3.5*\n",
    "> \n",
    "> You should rely on functions written in Part A.\n",
    "> \n",
    "> Persist it on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK43OefMszyg"
   },
   "source": [
    "### Question D2\n",
    "\n",
    "> Create a function named *create_test_df*, that creates a test dataset from a ratings dataframe ; it only retain the following records:\n",
    "> - hash(userId) % 2 == 1\n",
    "> - rating >= 3.5\n",
    "> \n",
    "> Also, the function returns a dataframe structured like this:\n",
    "> - userid : the user id\n",
    "> - movies : list[integer] (all the movies in the user timeline minus the *K* most recent ones)\n",
    "> - label : list[integer] (all the *K* most recent movies in user timeline)\n",
    ">\n",
    "> *K* is parameter whose default value is 5.\n",
    "> \n",
    "> Test the test dataset creation on a toy example.\n",
    "> \n",
    "> Create the real test dataset from the whole movieLense dataset. Name it *test_df*. Persist it on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CIzNEs5zgVW"
   },
   "source": [
    "### Question D3\n",
    "\n",
    "> Use/adapt each of the algorithms defined in previous sections (naïve, a-priori, FP-growth, PLSI) to predict the 5 next movies that will be seen by the user based on previously seen movies.\n",
    "> \n",
    "> Each algorithm can be 'trained' on *training_df* or a subset of it ; choose and justify.\n",
    "> \n",
    "> For each algorithm, make a quick qualitative analysis, to see how relevant recommended movies are. You should rely on *movies_df* for this question.\n",
    ">\n",
    "> Then, compare the algorithms with the *test_df*, with metrics like *recall* and *precision at k* (define some methods that compute recall and precision at k from test dataframe and predictions dataframe parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAyI7OsuWMWe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
