{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip -q\n",
        "!pip install progressbar -q\n",
        "!pip install memory_profiler -q\n",
        "!pip install --upgrade pandas>=1.2 -q"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cb667d2a-3b5f-410e-aaf4-dd439732c51e"
        },
        "id": "p52JIoW8WR9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6eacb1-766b-4a96-92ee-4754850f17ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/2.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.1 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext memory_profiler"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "28d273c8-e128-4b73-9b5e-aa4eab0656df"
        },
        "id": "c-713ffTWR9U"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import tarfile\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import warnings\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import progressbar\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import roc_auc_score, log_loss"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b638cb6c-d45d-4a28-8839-bb1c5dad06f1"
        },
        "id": "-Wj6PP_KWR9W"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw Dataset"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c53bbab7-6051-4983-96e3-793ee117588b"
        },
        "id": "8MMzLqYoWR9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Criteo Advertising Challenge dataset"
      ],
      "metadata": {
        "id": "oxvAZ3vtbZMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ProgressBar borrowed from https://stackoverflow.com/a/53643011/2015762\n",
        "class ProgressBar():\n",
        "    def __init__(self):\n",
        "        self.pbar = None\n",
        "\n",
        "    def __call__(self, block_num, block_size, total_size):\n",
        "        if not self.pbar:\n",
        "            self.pbar=progressbar.ProgressBar(maxval=total_size)\n",
        "            self.pbar.start()\n",
        "\n",
        "        downloaded = block_num * block_size\n",
        "        if downloaded < total_size:\n",
        "            self.pbar.update(downloaded)\n",
        "        else:\n",
        "            self.pbar.finish()\n",
        "\n",
        "def download_dataset(dataset_url, dataset_folder_path, compressed_dataset_path):\n",
        "    # Download dataset\n",
        "    os.makedirs(dataset_folder_path, exist_ok=True)\n",
        "    urllib.request.urlretrieve(dataset_url, compressed_dataset_path, ProgressBar())\n",
        "\n",
        "def extract_dataset(compressed_dataset_path, dataset_folder_path, dataset_path):\n",
        "    # Extract train.txt (dataset with labels) and readme\n",
        "    with tarfile.open(compressed_dataset_path, \"r\") as input_file:\n",
        "        input_file.extract('readme.txt', dataset_folder_path)\n",
        "        input_file.extract('train.txt', dataset_folder_path)\n",
        "        os.rename(os.path.join(dataset_folder_path, 'train.txt'), dataset_path)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "af3c5721-fde1-4c56-8437-57d8e8bc5311"
        },
        "id": "2qD-hUMuWR9b"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://criteostorage.blob.core.windows.net/criteo-research-datasets/kaggle-display-advertising-challenge-dataset.tar.gz\"\n",
        "dataset_folder_path = os.path.abspath('sync/data/criteo_dataset')\n",
        "compressed_dataset_path = os.path.join(dataset_folder_path, \"criteo_dataset.tar.gz\")\n",
        "dataset_path = os.path.join(dataset_folder_path, \"criteo_dataset.txt\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1478b2f8-d38d-47c0-83da-60b9bc391b2b"
        },
        "id": "UpkN4v1OWR9f"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(compressed_dataset_path):\n",
        "    download_dataset(dataset_url, dataset_folder_path, compressed_dataset_path)\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    extract_dataset(compressed_dataset_path, dataset_folder_path, dataset_path)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "63ada22d-76b5-4e2c-932a-69c89e541765"
        },
        "id": "Q-CbCuJvWR9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbea4064-379d-47a6-d4cc-d15df4a951bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick look at the files we have downloaded.\n",
        "\n",
        "Within iPython notebook, we can execute bash command by prepending the cell with `!` and insert python variable into it with `{}`"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "55660968-0843-4d77-a996-c0970110f7bc"
        },
        "id": "oqfkakZPWR9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh {dataset_folder_path}"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "99c7acfc-a685-4e49-a609-a409d6199f62"
        },
        "id": "Xtn2Wh3SWR9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295abffb-ac76-433f-e106-d072c986316f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 15G\n",
            "drwxr-xr-x 2 root      root  4.0K Mar 16 08:10 .\n",
            "drwxr-xr-x 3 root      root  4.0K Mar 16 08:03 ..\n",
            "-rw-r--r-- 1 root      root  4.3G Mar 16 08:06 criteo_dataset.tar.gz\n",
            "-rw-r--r-- 1 293604138 staff  11G May 12  2014 criteo_dataset.txt\n",
            "-rw-r--r-- 1 293604138 staff 1.9K Aug 22  2014 readme.txt\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "!cat {dataset_folder_path}/readme.txt"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "99f480f9-5224-419b-904a-fc38429d97e6"
        },
        "id": "3LEk8louWR9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "864ddb1b-00be-4cb3-cf22-88367952540b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        ------ Display Advertising Challenge ------\n",
            "\n",
            "Dataset: dac-v1\n",
            "\n",
            "This dataset contains feature values and click feedback for millions of display \n",
            "ads. Its purpose is to benchmark algorithms for clickthrough rate (CTR) prediction.\n",
            "It has been used for the Display Advertising Challenge hosted by Kaggle:\n",
            "https://www.kaggle.com/c/criteo-display-ad-challenge/\n",
            "\n",
            "===================================================\n",
            "\n",
            "Full description:\n",
            "\n",
            "This dataset contains 2 files:\n",
            "  train.txt\n",
            "  test.txt\n",
            "corresponding to the training and test parts of the data. \n",
            "\n",
            "====================================================\n",
            "\n",
            "Dataset construction:\n",
            "\n",
            "The training dataset consists of a portion of Criteo's traffic over a period\n",
            "of 7 days. Each row corresponds to a display ad served by Criteo and the first\n",
            "column is indicates whether this ad has been clicked or not.\n",
            "The positive (clicked) and negatives (non-clicked) examples have both been\n",
            "subsampled (but at different rates) in order to reduce the dataset size.\n",
            "\n",
            "There are 13 features taking integer values (mostly count features) and 26\n",
            "categorical features. The values of the categorical features have been hashed\n",
            "onto 32 bits for anonymization purposes. \n",
            "The semantic of these features is undisclosed. Some features may have missing values.\n",
            "\n",
            "The rows are chronologically ordered.\n",
            "\n",
            "The test set is computed in the same way as the training set but it \n",
            "corresponds to events on the day following the training period. \n",
            "The first column (label) has been removed.\n",
            "\n",
            "====================================================\n",
            "\n",
            "Format:\n",
            "\n",
            "The columns are tab separeted with the following schema:\n",
            "<label> <integer feature 1> ... <integer feature 13> <categorical feature 1> ... <categorical feature 26>\n",
            "\n",
            "When a value is missing, the field is just empty.\n",
            "There is no label field in the test set.\n",
            "\n",
            "====================================================\n",
            "\n",
            "Dataset assembled by Olivier Chapelle (o.chapelle@criteo.com)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "label_columns = ['label']\n",
        "integer_features = [f'int_feat_{i}' for i in range(1, 14)]\n",
        "categorical_features = [f'cat_feat_{i}' for i in range(1, 27)]\n",
        "columns = label_columns + integer_features + categorical_features"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "83209dbe-6666-43d0-8ab2-c9a7139ae4e3"
        },
        "id": "Ya9QCYx5WR9p"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(dataset_path, nrows=10, header=None, sep='\\t', names=columns)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "97987163-55dc-4807-a4e8-0d5db8781629"
        },
        "id": "bifPYKNBWR9q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "aaee20f6-4228-4803-d6bf-4412b5df2354"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label  int_feat_1  int_feat_2  int_feat_3  int_feat_4  int_feat_5  \\\n",
              "0      0         1.0           1         5.0         0.0        1382   \n",
              "1      0         2.0           0        44.0         1.0         102   \n",
              "2      0         2.0           0         1.0        14.0         767   \n",
              "3      0         NaN         893         NaN         NaN        4392   \n",
              "4      0         3.0          -1         NaN         0.0           2   \n",
              "5      0         NaN          -1         NaN         NaN       12824   \n",
              "6      0         NaN           1         2.0         NaN        3168   \n",
              "7      1         1.0           4         2.0         0.0           0   \n",
              "8      0         NaN          44         4.0         8.0       19010   \n",
              "9      0         NaN          35         NaN         1.0       33737   \n",
              "\n",
              "   int_feat_6  int_feat_7  int_feat_8  int_feat_9  ...  cat_feat_17  \\\n",
              "0         4.0          15           2         181  ...     e5ba7672   \n",
              "1         8.0           2           2           4  ...     07c540c4   \n",
              "2        89.0           4           2         245  ...     8efede7f   \n",
              "3         NaN           0           0           0  ...     1e88c74f   \n",
              "4         0.0           3           0           0  ...     1e88c74f   \n",
              "5         NaN           0           0           6  ...     776ce399   \n",
              "6         NaN           0           1           2  ...     776ce399   \n",
              "7         0.0           1           0           0  ...     e5ba7672   \n",
              "8       249.0          28          31         141  ...     e5ba7672   \n",
              "9        21.0           1           2           3  ...     d4bb7bd8   \n",
              "\n",
              "   cat_feat_18  cat_feat_19  cat_feat_20 cat_feat_21 cat_feat_22 cat_feat_23  \\\n",
              "0     f54016b9     21ddcdc9     b1252a9d    07b5194c         NaN    3a171ecb   \n",
              "1     b04e4670     21ddcdc9     5840adea    60f6221e         NaN    3a171ecb   \n",
              "2     3412118d          NaN          NaN    e587c466    ad3062eb    3a171ecb   \n",
              "3     74ef3502          NaN          NaN    6b3a5ca6         NaN    3a171ecb   \n",
              "4     26b3c7a7          NaN          NaN    21c9516a         NaN    32c7478e   \n",
              "5     92555263          NaN          NaN    242bb710    8ec974f4    be7c41b4   \n",
              "6     cdfa8259          NaN          NaN    20062612         NaN    93bad2c0   \n",
              "7     74ef3502          NaN          NaN    5316a17f         NaN    32c7478e   \n",
              "8     42a2edb9          NaN          NaN    0014c32a         NaN    32c7478e   \n",
              "9     70d0f5f9          NaN          NaN    0e63fca0         NaN    32c7478e   \n",
              "\n",
              "  cat_feat_24 cat_feat_25 cat_feat_26  \n",
              "0    c5c50484    e8b83407    9727dd16  \n",
              "1    43f13e8b    e8b83407    731c3655  \n",
              "2    3b183c5c         NaN         NaN  \n",
              "3    9117a34a         NaN         NaN  \n",
              "4    b34f3128         NaN         NaN  \n",
              "5    72c78f11         NaN         NaN  \n",
              "6    1b256e61         NaN         NaN  \n",
              "7    9117a34a         NaN         NaN  \n",
              "8    3b183c5c         NaN         NaN  \n",
              "9    0e8fe315         NaN         NaN  \n",
              "\n",
              "[10 rows x 40 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5dfa8b89-2567-412a-829c-d2d1b4785518\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>int_feat_1</th>\n",
              "      <th>int_feat_2</th>\n",
              "      <th>int_feat_3</th>\n",
              "      <th>int_feat_4</th>\n",
              "      <th>int_feat_5</th>\n",
              "      <th>int_feat_6</th>\n",
              "      <th>int_feat_7</th>\n",
              "      <th>int_feat_8</th>\n",
              "      <th>int_feat_9</th>\n",
              "      <th>...</th>\n",
              "      <th>cat_feat_17</th>\n",
              "      <th>cat_feat_18</th>\n",
              "      <th>cat_feat_19</th>\n",
              "      <th>cat_feat_20</th>\n",
              "      <th>cat_feat_21</th>\n",
              "      <th>cat_feat_22</th>\n",
              "      <th>cat_feat_23</th>\n",
              "      <th>cat_feat_24</th>\n",
              "      <th>cat_feat_25</th>\n",
              "      <th>cat_feat_26</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1382</td>\n",
              "      <td>4.0</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>181</td>\n",
              "      <td>...</td>\n",
              "      <td>e5ba7672</td>\n",
              "      <td>f54016b9</td>\n",
              "      <td>21ddcdc9</td>\n",
              "      <td>b1252a9d</td>\n",
              "      <td>07b5194c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3a171ecb</td>\n",
              "      <td>c5c50484</td>\n",
              "      <td>e8b83407</td>\n",
              "      <td>9727dd16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>102</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>07c540c4</td>\n",
              "      <td>b04e4670</td>\n",
              "      <td>21ddcdc9</td>\n",
              "      <td>5840adea</td>\n",
              "      <td>60f6221e</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3a171ecb</td>\n",
              "      <td>43f13e8b</td>\n",
              "      <td>e8b83407</td>\n",
              "      <td>731c3655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>767</td>\n",
              "      <td>89.0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>245</td>\n",
              "      <td>...</td>\n",
              "      <td>8efede7f</td>\n",
              "      <td>3412118d</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>e587c466</td>\n",
              "      <td>ad3062eb</td>\n",
              "      <td>3a171ecb</td>\n",
              "      <td>3b183c5c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>893</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4392</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1e88c74f</td>\n",
              "      <td>74ef3502</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6b3a5ca6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3a171ecb</td>\n",
              "      <td>9117a34a</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1e88c74f</td>\n",
              "      <td>26b3c7a7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21c9516a</td>\n",
              "      <td>NaN</td>\n",
              "      <td>32c7478e</td>\n",
              "      <td>b34f3128</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12824</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>776ce399</td>\n",
              "      <td>92555263</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>242bb710</td>\n",
              "      <td>8ec974f4</td>\n",
              "      <td>be7c41b4</td>\n",
              "      <td>72c78f11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3168</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>776ce399</td>\n",
              "      <td>cdfa8259</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20062612</td>\n",
              "      <td>NaN</td>\n",
              "      <td>93bad2c0</td>\n",
              "      <td>1b256e61</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>e5ba7672</td>\n",
              "      <td>74ef3502</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5316a17f</td>\n",
              "      <td>NaN</td>\n",
              "      <td>32c7478e</td>\n",
              "      <td>9117a34a</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>44</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>19010</td>\n",
              "      <td>249.0</td>\n",
              "      <td>28</td>\n",
              "      <td>31</td>\n",
              "      <td>141</td>\n",
              "      <td>...</td>\n",
              "      <td>e5ba7672</td>\n",
              "      <td>42a2edb9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0014c32a</td>\n",
              "      <td>NaN</td>\n",
              "      <td>32c7478e</td>\n",
              "      <td>3b183c5c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>33737</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>d4bb7bd8</td>\n",
              "      <td>70d0f5f9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0e63fca0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>32c7478e</td>\n",
              "      <td>0e8fe315</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 40 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dfa8b89-2567-412a-829c-d2d1b4785518')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5dfa8b89-2567-412a-829c-d2d1b4785518 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5dfa8b89-2567-412a-829c-d2d1b4785518');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shrink it to a toy dataset"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "48033b2b-08be-4490-b9c8-4d6c8d61b38d"
        },
        "id": "wZbGsE__WR9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first create a toy dataset with \"only\" 1 million rows (out of 45 millions)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "23587706-411e-487a-89b6-2423b7e389dc"
        },
        "id": "zHEyd_JNWR9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_dataset_path = os.path.join(dataset_folder_path, \"criteo_toy_dataset.txt\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c1015efa-7924-4f3a-8bac-0c0fc691d425"
        },
        "id": "8x6n3G2zWR9t"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 1000000 {dataset_path} > {toy_dataset_path}"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "13ad4dd2-c2d8-4c3f-bde2-c5cdd82933d7"
        },
        "id": "NBnfeSnrWR9v"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shortcut to get the toy dataset"
      ],
      "metadata": {
        "id": "QQZT2xEVa_YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If it takes too much time, download this toy dataset there instead."
      ],
      "metadata": {
        "id": "JfCQVjtYaxUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "too_long = False\n",
        "if too_long:\n",
        "  from urllib import request\n",
        "  os.makedirs(dataset_folder_path, exist_ok=True)\n",
        "  toy_dataset_path = os.path.join(dataset_folder_path, \"criteo_toy_dataset.txt\")\n",
        "  toy_dataset_url = 'https://www.dropbox.com/s/305lnmwphmu4cir/criteo_toy_dataset.txt?dl=1'\n",
        "  request.urlretrieve(toy_dataset_url, toy_dataset_path, ProgressBar())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNvzPky8bGSw",
        "outputId": "b984982a-bf85-4fba-b14b-63eaff1f98a9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimate Ratio of Positive samples"
      ],
      "metadata": {
        "id": "bUSIitZFcJwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we want to perform a basic operation: estimate the number of positive samples within the data"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "543acccb-6454-4ffe-960d-0dcbc33894b7"
        },
        "id": "YW7BM72LWR9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic approach"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e85b4203-6efc-46f0-ba5b-52ebe297b95a"
        },
        "id": "3iVxOPERWR9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_positive_label_proportion(dataset_path, columns):\n",
        "    df = pd.read_csv(dataset_path, sep=\"\\t\", header=None, names=columns, usecols=['label'])\n",
        "    return df['label'].mean()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3d57d10b-15ee-4e3d-82fc-495d40a6b376"
        },
        "id": "kl2X7feEWR9w"
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's measure its memory footprint with the `%%memit` magic function"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fceb5b2b-9553-44a7-8d83-030409be41d9"
        },
        "id": "qT5U0AHOWR9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion(toy_dataset_path, columns)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dafb5614-7640-4d36-a0f1-eac939d24f65"
        },
        "id": "vAzIJL4TWR9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac28c31-a620-4c0a-a08f-5c605b94d95e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive_label_proportion 0.254949\n",
            "peak memory: 407.25 MiB, increment: 1.85 MiB\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "What would happen if you run the same function on a 45 times bigger dataset ?\n",
        "\n",
        "You can give a try with `compute_positive_label_proportion(dataset_path, columns)`... at your own risks."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "83936c1c-2f35-47df-9daf-0ea7b4036e92"
        },
        "id": "2nTAyYW3WR9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specifying schema\n",
        "We can help pandas by specifying the column types to be used such that it does not need to infer it. Do so with the parameter dtype of pd.read_csv: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7ba93320-7ce7-4a5c-b36f-f1e9f86a1528"
        },
        "id": "Vlcy4MeeWR90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_types = OrderedDict()\n",
        "for col_name in columns:\n",
        "    if col_name in label_columns: col_type = 'bool'\n",
        "    if col_name in integer_features: col_type = 'float32'\n",
        "    if col_name in categorical_features: col_type = 'str'\n",
        "    col_types[col_name] = col_type\n",
        "\n",
        "def compute_positive_label_proportion_with_dtype(dataset_path, columns, col_types):\n",
        "    # Read csv with dtype and return positive_label_proportion\n",
        "    df = pd.read_csv(dataset_path, sep=\"\\t\", header=None, names=columns, dtype=col_types)\n",
        "    return df['label'].mean()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "66f49dc1-bf77-4b33-b4f9-85a8c4f2cbe6"
        },
        "id": "UWKJHqfPWR90"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion_with_dtype(toy_dataset_path, columns, col_types)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7e275dee-57b0-4e2e-9d14-775b3c07daba"
        },
        "id": "A4KfxPStWR92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd3d5ec-48c9-41a7-d1d1-49bc7d7695e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive_label_proportion 0.254949\n",
            "peak memory: 1128.77 MiB, increment: 721.52 MiB\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading data by chunks\n",
        "We can control the amount of memory we need by loading only a small chunk of the data and processing it before moving to the next chunk.\n",
        "\n",
        "See documentation at https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#iterating-through-files-chunk-by-chunk\n",
        "\n",
        "```\n",
        "reader = pd.read_csv(..., chunksize=10, nrows=100):\n",
        "for chunk in reader:\n",
        "    print(chunk)\n",
        "```"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "56287730-4405-4ae2-90a2-cedf29035f80"
        },
        "id": "kePDZ3HdWR93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_positive_label_proportion_with_dtype_and_chunksize(dataset_path, columns, col_types, chunksize):\n",
        "    # Read csv with dtype and chunksize and return positive_label_proportion\n",
        "    reader = pd.read_csv(\n",
        "        dataset_path, sep=\"\\t\", header=None, names=columns, dtype=col_types, chunksize=chunksize, \n",
        "    )\n",
        "    sum_labels = 0\n",
        "    sum_rows = 0\n",
        "    for chunk in reader:\n",
        "        sum_labels += chunk['label'].sum()\n",
        "        sum_rows += len(chunk)\n",
        "    return sum_labels / sum_rows"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d3c192e9-b4aa-4b81-9abc-9258896a506d"
        },
        "id": "xlWBbAKPWR93"
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion_with_dtype_and_chunksize(toy_dataset_path, columns, col_types, 100_000)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "68a665f7-489e-4484-8cf3-4d376d1ea657"
        },
        "id": "KBQpWprpWR94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5504b3c-375a-4572-b051-cf9fc913370a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive_label_proportion 0.254949\n",
            "peak memory: 492.44 MiB, increment: 0.00 MiB\n"
          ]
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can now be applied to the full dataset with no memory issue."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a5625703-6f57-414d-a2f6-25d86f4ae4fb"
        },
        "id": "8uJUIPVQWR95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion_with_dtype_and_chunksize(dataset_path, columns, col_types, 100_000)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9f925158-ac73-4723-be68-249d05c75f5c"
        },
        "id": "qzFz2N0PWR95"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evaluation"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4722389c-73ce-482b-b494-a363950f2276"
        },
        "id": "sD2iNmnfWR97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split train and test datasets\n",
        "Since the datasets contain one line per example, we can split them into train and test by simply iterating over the lines. For each line in the original dataset: write it to the test data set with a probability p and write it to the train dataset with a probability 1 - p."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f7b3b941-be9a-4dff-ac77-fb980120f091"
        },
        "id": "85ojC-KgWR98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test(full_dataset_path, train_dataset_path, test_dataset_path, test_ratio, seed=302984, print_every=None):\n",
        "    random.seed(seed)\n",
        "    with open(full_dataset_path, 'r') as input_file, open(train_dataset_path, 'w') as train_file, open(test_dataset_path, 'w') as test_file:\n",
        "        for i, line in enumerate(input_file):\n",
        "            if random.uniform(0, 1) <= test_ratio:\n",
        "                test_file.write(line)\n",
        "            else:\n",
        "                train_file.write(line)\n",
        "            \n",
        "            if print_every is not None and (i + 1) % print_every == 0:\n",
        "                print(f\"Processed {i + 1} lines\")\n",
        "        print(f\"Processed {i + 1} lines\")\n",
        "        \n",
        "train_dataset_path = os.path.join(dataset_folder_path, \"criteo_train_dataset.txt\")\n",
        "test_dataset_path = os.path.join(dataset_folder_path, \"criteo_test_dataset.txt\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6ddfeb48-4ac1-43ef-9c14-ff395c0fcb18"
        },
        "id": "Qn-_jfDZWR99"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(train_dataset_path) or not os.path.exists(test_dataset_path):\n",
        "    split_train_test(dataset_path, train_dataset_path, test_dataset_path, test_ratio=0.1, print_every=10_000_000)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b433a302-de7a-4045-b67c-f3ebb3aab794"
        },
        "id": "n-MFoeBFWR99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7df74e-168d-4671-e254-71da3d886c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 10000000 lines\n",
            "Processed 20000000 lines\n",
            "Processed 30000000 lines\n",
            "Processed 40000000 lines\n",
            "Processed 45840617 lines\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l {test_dataset_path}"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2faffa05-0de3-401f-b7d4-b87714a98a17"
        },
        "id": "FovKEtV0WR9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df89f39f-1aba-4888-9475-41e68c817d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4585250 /content/sync/data/criteo_dataset/criteo_test_dataset.txt\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffling\n",
        "The convergence guarantees of SGD rely on the fact that the observations come at random. Hence, shuffling between epochs is important.\n",
        "\n",
        "First result of \"How to shuffle a file that is too big for memory\" on Google: https://stackoverflow.com/a/40814865/2015762\n",
        "\n",
        "Note that quicker pseudo-shuffling strategies exists, but this fits our \"Big data on your laptop\" problematic."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b524a5dd-b21f-4572-bf9b-6de10e539122"
        },
        "id": "4eJCDJVCWR9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_shuffled_path = os.path.join(dataset_folder_path, \"criteo_test_dataset_shuffled.txt\")\n",
        "train_dataset_shuffled_path = os.path.join(dataset_folder_path, \"criteo_train_dataset_shuffled.txt\")"
      ],
      "metadata": {
        "id": "4BCrAenzh-nd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!awk 'BEGIN{srand();} {printf \"%06d %s\\n\", rand()*1000000, $0;}' /content/sync/data/criteo_dataset/criteo_test_dataset.txt | sort -n | cut -c8- > /content/sync/data/criteo_dataset/criteo_test_dataset_shuffled.txt\n",
        "# We can run it on the train dataset too but let's skip it since it is quite long\n",
        "# !awk 'BEGIN{srand();} {printf \"%06d %s\\n\", rand()*1000000, $0;}' /content/sync/data/criteo_dataset/criteo_train_dataset.txt | sort -n | cut -c8- > /content/sync/data/criteo_dataset/criteo_train_dataset_shuffled.txt"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e8546d50-68d4-4f24-abb1-e0c9aa11df96"
        },
        "id": "shqNz_yEWR9_"
      },
      "outputs": [],
      "execution_count": 43
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "In order to train a logistic model on chunks of data, we will use scikit-learn `SGDClassifier` (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) and train for its `log` loss with its `partial_fit` method.\n",
        "We can now apply the previous data processing pipeline and add the training to obtain a trained classifier."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d7444e81-90d6-43c5-acf8-68699e77a3dd"
        },
        "id": "pLjCE-MLWR-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  To begin with, let's not do any preprocessing and deal with \"ready to use\" continuous features only\n",
        "def preprocess_simple(chunk, integer_features, categorical_features):\n",
        "    return chunk[integer_features].fillna(-1)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5f300552-af45-4ab0-a495-db4ba9a8a827"
        },
        "id": "b2zWAybwWR-B"
      },
      "outputs": [],
      "execution_count": 124
    },
    {
      "cell_type": "code",
      "source": [
        "def do_train(path, preprocess, max_training_steps=1000, chunk_size=1000, print_every=50):\n",
        "  classifier = SGDClassifier(loss=\"log_loss\")#, alpha=0.001)\n",
        "  # 1. Read train data by chunks\n",
        "  reader = pd.read_csv(\n",
        "      path, sep=\"\\t\", header=None, names=columns, dtype=col_types, chunksize=chunk_size, \n",
        "  )\n",
        "  losses = []\n",
        "  for i, chunk in enumerate(reader):\n",
        "      # 2. Apply preprocess_data to return the continous features\n",
        "      features = preprocess(chunk, integer_features, categorical_features)\n",
        "      # 3. Train classifier on this chunk  with fit.\n",
        "      classifier.partial_fit(features, chunk[\"label\"], classes=[0, 1])\n",
        "      # 4. Stop after `max_training_steps`\n",
        "      if i > max_training_steps:\n",
        "          break\n",
        "\n",
        "      label_predictions = classifier.predict_proba(features)[:, 1]\n",
        "      losses.append(log_loss(chunk[\"label\"], label_predictions))\n",
        "          \n",
        "      if print_every is not None and (i + 1) % print_every == 0:\n",
        "        print(f'{i+1} : {np.array(losses).mean()}')\n",
        "  return classifier"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c2fff3fb-3a17-42b0-b5aa-d05ff9632ae7"
        },
        "id": "UfP7PsajWR-C"
      },
      "outputs": [],
      "execution_count": 211
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = do_train(train_dataset_path, preprocess_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWWnjLANpoDY",
        "outputId": "d2f111ea-f8d5-4929-c98e-52e7c0bda07c"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "Let's evaluate the performances of the trained classifier. We should iterate over the test dataset and evaluate the labels predicted by the classifier with `roc_auc_score` and `log_loss`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3ccf2390-7b6e-4ec1-b29f-5e3f767e3fbe"
        },
        "id": "ytACb7xiWR-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_test(classifier, path, preprocess, max_testing_steps = 100, chunk_size = 1000, print_every = 10):\n",
        "  reader = pd.read_csv(\n",
        "      path, sep=\"\\t\", header=None, names=columns, dtype=col_types, chunksize=chunk_size, \n",
        "  )\n",
        "  roc_auc_scores = []\n",
        "  log_losses = []\n",
        "  # 1. Read test data by chunks\n",
        "  for i, chunk in enumerate(reader):\n",
        "      # 2. Apply preprocess_data to return the continous features\n",
        "      features = preprocess(chunk, integer_features, categorical_features)\n",
        "      # 3. Predict labels with classifiers\n",
        "      label_predictions = classifier.predict_proba(features)[:, 1]\n",
        "      # 4. Compute AUC score and Log loss for this chunk\n",
        "      roc_auc_scores += [roc_auc_score(chunk[\"label\"], label_predictions)]\n",
        "      log_losses += [log_loss(chunk[\"label\"], label_predictions)]\n",
        "      \n",
        "      if i > max_testing_steps:\n",
        "          return (np.mean(roc_auc_scores), np.mean(log_losses))\n",
        "          \n",
        "      if print_every is not None and (i + 1) % print_every == 0:\n",
        "          print(i+1)\n",
        "  # 6. Return metrics\n",
        "  return (np.mean(roc_auc_scores), np.mean(log_losses))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9a3f8de1-b8aa-41c8-9a46-9c62122da7f9"
        },
        "id": "FVMZYX5UWR-D"
      },
      "outputs": [],
      "execution_count": 127
    },
    {
      "cell_type": "code",
      "source": [
        "(roc_auc_scores, log_losses) = do_test(classifier, test_dataset_path, preprocess_simple)\n",
        "print(f\"AUC = {roc_auc_scores}\")\n",
        "print(f\"LogLoss = {log_losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljckg0E1wr9A",
        "outputId": "e2ffdb23-7550-4b7a-baad-be8501500896"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "AUC = 0.5567602629608853\n",
            "LogLoss = 10.021558459885304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f2c9ec3e-554e-4caf-b9bb-bcec65ed3dba"
        },
        "id": "axiFIbG6WR-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous features\n",
        "A smart way to deal with continuous features (counting integer features are part of them), consists in transforming them into categorical features through a quantile transformation. To do so we will use scikit-learn KBinsDiscretizer : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html.\n",
        "\n",
        "It can be used as following\n",
        "```\n",
        "df = pd.DataFrame({'col_1': np.random.normal(size=1000), 'col_2': np.random.poisson(lam=1, size=1000)})\n",
        "bucketizer = KBinsDiscretizer(n_bins=20, encode='ordinal')\n",
        "bucketizer.fit(df)\n",
        "df_bucketized = pd.DataFrame(bucketizer.transform(df), columns=[f'{col}_bucketized' for col in df.columns], index=df.index)\n",
        "sns.jointplot(data=pd.concat((df, df_bucketized), axis=1), x=\"col_1\", y=\"col_1_bucketized\")\n",
        "```"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cacde538-c462-4da5-a5cb-78bbf2e8b624"
        },
        "id": "scI_arubWR-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create a `KBinsDiscretizer` and train it on the first chunk of the dataset. Try the `fit` method of `KBinsDiscretizer` on the chunk and see the type of what is returned. This is not a dataframe any more but your classifier will accept this type for its feature matrix.\n",
        "1. Update `preprocess_data` to add a bucketize step to the training pipeline.\n",
        "1. Do not forget to deal with missing values, you do not want to carry on NaNs. You can for example replace them with -1."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e87b6181-a50c-4a25-9269-825927d14df4"
        },
        "id": "x9HZ2jc5WR-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader = pd.read_csv(\n",
        "    train_dataset_path, sep=\"\\t\", header=None, names=columns, dtype=col_types, chunksize=10_000\n",
        ")\n",
        "chunk = reader.get_chunk(1000)"
      ],
      "metadata": {
        "id": "X3pAygbUs-0K"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucketizer = KBinsDiscretizer(n_bins=20)\n",
        "with warnings.catch_warnings(record=True):\n",
        "    bucketizer.fit(chunk[integer_features].fillna(-1))"
      ],
      "metadata": {
        "id": "Dc_WkJlnuRR1"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bin_edges contains one bin definition per column:\n",
        "print(len(bucketizer.bin_edges_))\n",
        "# Displaying first column bins:\n",
        "print(bucketizer.bin_edges_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZgspnLLuTav",
        "outputId": "3ef3a06f-ecaf-40fa-9e18-258a232e23d1"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "[-1.  0.  1.  2.  4.  9. 88.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bucketize(df, bucketizer):\n",
        "    return bucketizer.transform(df.fillna(-1))\n",
        "\n",
        "def preprocess_bucket(chunk, integer_features, categorical_features):\n",
        "    return bucketize(chunk[integer_features], bucketizer)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6603e270-56e3-4ed9-a6e3-b6b8b70b1e0b"
        },
        "id": "3E-2LHNcWR-F"
      },
      "outputs": [],
      "execution_count": 134
    },
    {
      "cell_type": "code",
      "source": [
        "reader = pd.read_csv(\n",
        "    train_dataset_path, sep=\"\\t\", header=None, names=columns, dtype=col_types, chunksize=10_000\n",
        ")\n",
        "small_chunk = reader.get_chunk(5)\n",
        "mat = preprocess_bucket(small_chunk, integer_features, categorical_features)\n",
        "mat.todense()"
      ],
      "metadata": {
        "id": "lu3hxt0v03at"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_with_buckets = do_train(train_dataset_path, preprocess_bucket)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Fa9uOCv-fu",
        "outputId": "94c5beda-d6c4-4b75-dc38-1f95df37b10b"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(roc_auc_scores, log_losses) = do_test(classifier_with_buckets, test_dataset_path, preprocess_bucket)\n",
        "print(f\"AUC = {roc_auc_scores}\")\n",
        "print(f\"LogLoss = {log_losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5r6QsjRwvtz",
        "outputId": "4c84f80b-fdce-4132-eeb8-1f15983d4f0b"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "AUC = 0.7140477968883315\n",
            "LogLoss = 0.5124704331207389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat {test_dataset_path} | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Govy6FgAq2nL",
        "outputId": "5ef38f88-3ab4-4735-8070-a4542a5aa2cc"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4585250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical features"
      ],
      "metadata": {
        "id": "RFREUs7mG-3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Hashing Trick\n",
        "For categorical features we will implement the hashing trick by ourselves. As a quick reminder, for each row\n",
        "\n",
        "1. Select the categorical features \n",
        "1. Create for each feature the string concatenating the feature name and the feature value\n",
        "1. Apply a hash function to each of these string and use this value to choose the feature's column index\n",
        "1. Store the transformed features in a sparse matrix"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ada99dde-fc5f-41e5-bd37-b7a018e66736"
        },
        "id": "tSRJkWyXWR-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a function that hashes strings the deterministic way\n",
        "from sklearn.utils.murmurhash import murmurhash3_bytes_s32\n",
        "def hash_string(string, seed=0):\n",
        "    return murmurhash3_bytes_s32(string.encode(), seed)\n",
        "hash_string('my_feature=my_feature_value')\n",
        "# Note, if we were using builtin function hash('my_feature=my_feature_value'), we would have had a different hash value at each run"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ef9557fb-a1d4-4d70-b70c-158be1bc30b4"
        },
        "id": "JrGXn5qcWR-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7d6729-18e1-4a12-9937-d3098b18ca4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1480568101"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ],
      "execution_count": 137
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function that:\n",
        "# - takes as arguments:\n",
        "#    - the dataframe to transform\n",
        "#    - size of hash_space\n",
        "# - returns a numpy array with hashes of each categorical columns\n",
        "# hashes should belong to [0;hash_space[\n",
        "def get_features_hashes(row, hash_space):\n",
        "    # return the list of the hashes values for each categorical feature in the row\n",
        "    features_as_string = [f\"{label}={value}\" for label, value in zip(row.index, row.values)]\n",
        "    return [abs(hash_string(string)) % hash_space for string in features_as_string]"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "31886db4-d1db-4a02-a9bd-f752ce68ae0a"
        },
        "id": "STKgtwo2WR-I"
      },
      "outputs": [],
      "execution_count": 138
    },
    {
      "cell_type": "code",
      "source": [
        "# try out the function one one row of your chunk with a hash_space of 2^16\n",
        "hash_space = 2 ** 16\n",
        "row = chunk[categorical_features].iloc[0]\n",
        "np.array(get_features_hashes(row, hash_space))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHla2E_l1fQ0",
        "outputId": "9c227888-0eb3-4915-9df8-c394d2ca7886"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6307, 50007, 16938, 64016, 52877, 55356, 40417, 44725, 34895,\n",
              "       31363, 35777, 46842, 15202, 57383, 34431, 18421, 47320, 18458,\n",
              "       25889, 53956,  9729, 12896, 56167, 63487, 61374, 63046])"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function that transforms a dataframe into a sparse matrix m that can be passed to the learning\n",
        "# Matrix m contains `hash_space` columns\n",
        "# m[i,j] = 1 if the hashed value of at least one categorical column is `j` for line `i`\n",
        "\n",
        "# use csr_matrix to create the sparse matrix\n",
        "# Fill the csr_matrix, using csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)]) constructor\n",
        "\n",
        "def transform_with_hashing_trick(df, hash_space):\n",
        "    series = df.apply(lambda row: get_features_hashes(row, hash_space), axis=1)\n",
        "\n",
        "    rows_and_cols_by_line = [([row_index] * len(col_indexes), col_indexes) for (row_index, col_indexes) in enumerate(series)]\n",
        "    all_row_indices = sum([row_indices for (row_indices, _) in rows_and_cols_by_line], [])\n",
        "    all_col_indices = sum([col_indices for (_, col_indices) in rows_and_cols_by_line], [])\n",
        "\n",
        "    data = np.ones_like(all_col_indices)\n",
        "    return csr_matrix((data, (all_row_indices, all_col_indices)), shape=(len(df), hash_space), dtype=float)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2c9c09dd-41fe-4fa5-93bb-b7c0afaabb0c"
        },
        "id": "R8RcbJ6WWR-I"
      },
      "outputs": [],
      "execution_count": 188
    },
    {
      "cell_type": "code",
      "source": [
        "m = transform_with_hashing_trick(small_chunk[categorical_features], 2 ** 8)\n",
        "print(m.sum(axis=1))\n",
        "print(len(categorical_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbmyG8BI2Ufj",
        "outputId": "9a81c2b8-6cdd-4700-b047-6b6992c2ebe3"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[26.]\n",
            " [26.]\n",
            " [26.]\n",
            " [26.]\n",
            " [26.]]\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the `preprocess_data` method we had defined before.\n",
        "# New implementation should:\n",
        "# - apply bucketization on integer columns\n",
        "# - apply hashing trick on categorical columns\n",
        "# - return a concatenation of matrix with both features\n",
        "# you can rely on scipy.sparse hstack to concatenate matrix\n",
        "from scipy.sparse import hstack\n",
        "hash_space = 2 ** 16\n",
        "def preprocess_hash_v1(df, integer_features, categorical_features):\n",
        "  bucketized_integer_features = bucketize(df[integer_features], bucketizer)\n",
        "  hashed_categorical_features = transform_with_hashing_trick(df[categorical_features], hash_space)\n",
        "  #return hashed_categorical_features\n",
        "  return hstack((bucketized_integer_features, hashed_categorical_features))"
      ],
      "metadata": {
        "id": "UZo9D0ov8l-x"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the training\n",
        "classifier_with_hashing_v1 = do_train(train_dataset_path, preprocess_hash_v1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_O7-nWl9dlB",
        "outputId": "6e607d64-1698-479b-b128-d6e37c28e03b"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 : 1.2501235214970996\n",
            "Label Mean : 0.234 ; Pred Mean : 0.15926569913433536\n",
            "100 : 0.8032901182249903\n",
            "Label Mean : 0.259 ; Pred Mean : 0.3315099972608472\n",
            "150 : 0.651252213299453\n",
            "Label Mean : 0.267 ; Pred Mean : 0.4128187651365036\n",
            "200 : 0.582421515901622\n",
            "Label Mean : 0.259 ; Pred Mean : 0.2708464522489461\n",
            "250 : 0.5446114589989384\n",
            "Label Mean : 0.243 ; Pred Mean : 0.22123147945730232\n",
            "300 : 0.5196043347908924\n",
            "Label Mean : 0.268 ; Pred Mean : 0.26893085771222003\n",
            "350 : 0.5036891876780105\n",
            "Label Mean : 0.274 ; Pred Mean : 0.28610705073869125\n",
            "400 : 0.4923385710767311\n",
            "Label Mean : 0.254 ; Pred Mean : 0.24017421163851563\n",
            "450 : 0.4840735374838529\n",
            "Label Mean : 0.27 ; Pred Mean : 0.2527085231081293\n",
            "500 : 0.4774353433300452\n",
            "Label Mean : 0.267 ; Pred Mean : 0.34577484178490003\n",
            "550 : 0.471866471398522\n",
            "Label Mean : 0.27 ; Pred Mean : 0.33103182446296925\n",
            "600 : 0.46771402691661534\n",
            "Label Mean : 0.255 ; Pred Mean : 0.19998025264745456\n",
            "650 : 0.4638227632199241\n",
            "Label Mean : 0.248 ; Pred Mean : 0.2645292371968121\n",
            "700 : 0.46085575485756375\n",
            "Label Mean : 0.268 ; Pred Mean : 0.29593955583603543\n",
            "750 : 0.4584959389351305\n",
            "Label Mean : 0.239 ; Pred Mean : 0.2528246442337242\n",
            "800 : 0.45642425428212946\n",
            "Label Mean : 0.222 ; Pred Mean : 0.22147940245889894\n",
            "850 : 0.4548140823473235\n",
            "Label Mean : 0.262 ; Pred Mean : 0.32298924260539214\n",
            "900 : 0.45320642032421854\n",
            "Label Mean : 0.235 ; Pred Mean : 0.23322788977635495\n",
            "950 : 0.45205853646008176\n",
            "Label Mean : 0.251 ; Pred Mean : 0.27041025547248\n",
            "1000 : 0.45108118410049797\n",
            "Label Mean : 0.271 ; Pred Mean : 0.3098772249782854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# look at metrics\n",
        "(roc_auc_scores, log_losses) = do_test(classifier_with_hashing_v1, test_dataset_path, preprocess_hash_v1)\n",
        "print(f\"AUC = {roc_auc_scores}\")\n",
        "print(f\"LogLoss = {log_losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPFwLKuB-Gn3",
        "outputId": "f31a7446-e68b-433a-ff94-59897211699b"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "AUC = 0.768672242857132\n",
            "LogLoss = 0.48363301599994474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use scikit-learn Feature Hasher"
      ],
      "metadata": {
        "id": "wNAVzhZbH_rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually, the hashing trick is well known and already implemented in scikit-learn FeatureHasher:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0c520c13-7534-4fad-9bf2-2f88c15cd8f6"
        },
        "id": "YT0FwBAgWR-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rewrite a new version of `transform_with_hashing_trick` that relies on FeatureHasher\n",
        "# create the hasher only once\n",
        "# you may need this to convert the dataframe: \n",
        "# https://stackoverflow.com/questions/29815129/pandas-dataframe-to-list-of-dictionaries\n",
        "# Test it on a chunk\n",
        "hasher = FeatureHasher(n_features=2 ** 20)\n",
        "def transform_with_hashing_trick_v2(df, hasher):\n",
        "  normalized_df = (chunk.fillna(\"nan\"))\n",
        "  return hasher.transform(normalized_df.to_dict('records'))\n",
        "\n",
        "transform_with_hashing_trick_v2(chunk[categorical_features], hasher)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ba5b0a8e-722b-47e2-a46f-b9a9dce7c4d3"
        },
        "id": "ug1rD9iaWR-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc6e942-8696-4338-ecf7-22b506312244"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1000x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 37489 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ],
      "execution_count": 169
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare speed of two methods on chunk of 1000 rows"
      ],
      "metadata": {
        "id": "OEXjWom4yXYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit transform_with_hashing_trick(chunk, 2 ** 20)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6aee4d29-3d84-4f2a-b387-ec233412213b"
        },
        "id": "7yar9NoRWR-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99dac89c-8c00-497f-d85f-632fbb44caa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "361 ms ± 153 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "execution_count": 172
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit transform_with_hashing_trick_v2(chunk, hasher)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "22633b92-95dd-4fda-8580-f11bf7a2f102"
        },
        "id": "fA7epmDCWR-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6fafaca-6c07-42c8-c703-ac2263360299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52.8 ms ± 1.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "execution_count": 173
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test it on the pipeline."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4598f259-ae84-457e-b565-1fbccd4359e9"
        },
        "id": "J7lfZfbpWR-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# redefine preprocess_data\n",
        "hasher = FeatureHasher(n_features=2 ** 20)\n",
        "def preprocess_hash_v2(df, integer_features, categorical_features):\n",
        "  bucketized_integer_features = bucketize(df[integer_features], bucketizer)\n",
        "  hashed_categorical_features = transform_with_hashing_trick_v2(df, hasher, categorical_features)\n",
        "  return hstack((bucketized_integer_features, hashed_categorical_features))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "51d49529-7ca5-4f67-b16e-b8c347ac29b3"
        },
        "id": "xRD-tekGWR-O"
      },
      "outputs": [],
      "execution_count": 175
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_hash_v2(chunk, integer_features, categorical_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "luBX31eE27uP",
        "outputId": "2117b688-c8b7-4cc7-c782-3cc6d868bd36"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-4edfbf232a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocess_hash_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteger_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-175-7068d6b05bf0>\u001b[0m in \u001b[0;36mpreprocess_hash_v2\u001b[0;34m(df, integer_features, categorical_features)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_hash_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteger_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mbucketized_integer_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucketize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucketizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mhashed_categorical_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_with_hashing_trick_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucketized_integer_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashed_categorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: transform_with_hashing_trick_v2() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_with_hashing_v2 = do_train(train_dataset_path, max_training_steps=2000, chunk_size=1000, print_every=200)"
      ],
      "metadata": {
        "id": "0u4Yl0wq2HE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(roc_auc_scores, log_losses) = do_test(classifier_with_hashing_v2, test_dataset_path)\n",
        "print(f\"AUC = {roc_auc_scores}\")\n",
        "print(f\"LogLoss = {log_losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3e2noCG8Dnf",
        "outputId": "1a253ac6-7fea-4848-ff6a-5de22ddec898"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "AUC = 0.6992177807345303\n",
            "LogLoss = 0.5196194999645235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Cross Features with hashing"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "157a858a-544e-4219-86b6-4b3423310e16"
        },
        "id": "9S-4m87YWR-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features_hashes(row, hash_space):\n",
        "    # return the list of the hashes values for each categorical feature in the row\n",
        "    features_as_string = (\n",
        "        [f\"{l1}={v1};{l2}={v2}\"\n",
        "         for i,(l1,v1) in enumerate(zip(row.index, row.values))\n",
        "         for j,(l2, v2) in enumerate(zip(row.index, row.values))\n",
        "         if i <= j\n",
        "         ])\n",
        "    return [abs(hash_string(string)) % hash_space for string in features_as_string]"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "da65498b-985a-42ce-863d-acea33fa3f09"
        },
        "id": "X9kcSx1gWR-P"
      },
      "outputs": [],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "source": [
        "hash_space = 2 ** 16\n",
        "classifier_with_cross = do_train(train_dataset_path, max_training_steps=10_000, chunk_size=1000, print_every=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "2MGC6UYcLz8n",
        "outputId": "18e7ed72-efaf-4bbb-a58b-793136bda81e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a004aac086e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhash_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier_with_cross\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_training_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-980c7634935d>\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(path, max_training_steps, chunk_size, print_every)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;31m# 2. Apply preprocess_data to return the continous features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteger_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0;31m# 3. Train classifier on this chunk  with fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-fea1640e8b92>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(df, integer_features, categorical_features)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteger_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mbucketized_integer_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucketize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucketizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mhashed_categorical_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_with_hashing_trick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucketized_integer_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashed_categorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-515e0aab8510>\u001b[0m in \u001b[0;36mtransform_with_hashing_trick\u001b[0;34m(df, hash_space, categorical_features)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrows_and_cols_by_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_indexes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_indexes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mall_row_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_indices\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows_and_cols_by_chunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mall_col_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_indices\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows_and_cols_by_chunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_col_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(roc_auc_scores, log_losses) = do_test(classifier_with_cross, test_dataset_path)\n",
        "print(f\"AUC = {roc_auc_scores}\")\n",
        "print(f\"LogLoss = {log_losses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG2geVMLMBXf",
        "outputId": "22f69818-a210-4620-b223-719efb1ee3a2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "AUC = 0.7110682780699549\n",
            "LogLoss = 0.5106296209457268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZ0BCgYVNV0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Local online learning - after 1st session",
      "dashboards": [],
      "language": "python",
      "widgets": {},
      "notebookOrigID": 1919988149413991
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}