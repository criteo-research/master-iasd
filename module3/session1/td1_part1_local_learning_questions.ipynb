{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip -q\n",
        "!pip install progressbar -q\n",
        "!pip install memory_profiler -q\n",
        "!pip install --upgrade pandas>=1.2 -q"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cb667d2a-3b5f-410e-aaf4-dd439732c51e"
        },
        "id": "p52JIoW8WR9R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext memory_profiler"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "28d273c8-e128-4b73-9b5e-aa4eab0656df"
        },
        "id": "c-713ffTWR9U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import tarfile\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import warnings\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import progressbar\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import roc_auc_score, log_loss"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b638cb6c-d45d-4a28-8839-bb1c5dad06f1"
        },
        "id": "-Wj6PP_KWR9W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw Dataset"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c53bbab7-6051-4983-96e3-793ee117588b"
        },
        "id": "8MMzLqYoWR9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Criteo Advertising Challenge dataset"
      ],
      "metadata": {
        "id": "oxvAZ3vtbZMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ProgressBar borrowed from https://stackoverflow.com/a/53643011/2015762\n",
        "class ProgressBar():\n",
        "    def __init__(self):\n",
        "        self.pbar = None\n",
        "\n",
        "    def __call__(self, block_num, block_size, total_size):\n",
        "        if not self.pbar:\n",
        "            self.pbar=progressbar.ProgressBar(maxval=total_size)\n",
        "            self.pbar.start()\n",
        "\n",
        "        downloaded = block_num * block_size\n",
        "        if downloaded < total_size:\n",
        "            self.pbar.update(downloaded)\n",
        "        else:\n",
        "            self.pbar.finish()\n",
        "\n",
        "def download_dataset(dataset_url, dataset_folder_path, compressed_dataset_path):\n",
        "    # Download dataset\n",
        "    os.makedirs(dataset_folder_path, exist_ok=True)\n",
        "    urllib.request.urlretrieve(dataset_url, compressed_dataset_path, ProgressBar())\n",
        "\n",
        "def extract_dataset(compressed_dataset_path, dataset_folder_path, dataset_path):\n",
        "    # Extract train.txt (dataset with labels) and readme\n",
        "    with tarfile.open(compressed_dataset_path, \"r\") as input_file:\n",
        "        input_file.extract('readme.txt', dataset_folder_path)\n",
        "        input_file.extract('train.txt', dataset_folder_path)\n",
        "        os.rename(os.path.join(dataset_folder_path, 'train.txt'), dataset_path)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "af3c5721-fde1-4c56-8437-57d8e8bc5311"
        },
        "id": "2qD-hUMuWR9b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://criteostorage.blob.core.windows.net/criteo-research-datasets/kaggle-display-advertising-challenge-dataset.tar.gz\"\n",
        "dataset_folder_path = os.path.abspath('sync/data/criteo_dataset')\n",
        "compressed_dataset_path = os.path.join(dataset_folder_path, \"criteo_dataset.tar.gz\")\n",
        "dataset_path = os.path.join(dataset_folder_path, \"criteo_dataset.txt\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1478b2f8-d38d-47c0-83da-60b9bc391b2b"
        },
        "id": "UpkN4v1OWR9f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(compressed_dataset_path):\n",
        "    download_dataset(dataset_url, dataset_folder_path, compressed_dataset_path)\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    extract_dataset(compressed_dataset_path, dataset_folder_path, dataset_path)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "63ada22d-76b5-4e2c-932a-69c89e541765"
        },
        "id": "Q-CbCuJvWR9g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick look at the files we have downloaded.\n",
        "\n",
        "Within iPython notebook, we can execute bash command by prepending the cell with `!` and insert python variable into it with `{}`"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "55660968-0843-4d77-a996-c0970110f7bc"
        },
        "id": "oqfkakZPWR9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh {dataset_folder_path}"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "99c7acfc-a685-4e49-a609-a409d6199f62"
        },
        "id": "Xtn2Wh3SWR9m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!cat {dataset_folder_path}/readme.txt"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "99f480f9-5224-419b-904a-fc38429d97e6"
        },
        "id": "3LEk8louWR9o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "label_columns = ['label']\n",
        "integer_features = [f'int_feat_{i}' for i in range(1, 14)]\n",
        "categorical_features = [f'cat_feat_{i}' for i in range(1, 27)]\n",
        "columns = label_columns + integer_features + categorical_features"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "83209dbe-6666-43d0-8ab2-c9a7139ae4e3"
        },
        "id": "Ya9QCYx5WR9p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(dataset_path, nrows=10, header=None, sep='\\t', names=columns)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "97987163-55dc-4807-a4e8-0d5db8781629"
        },
        "id": "bifPYKNBWR9q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shrink it to a toy dataset"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "48033b2b-08be-4490-b9c8-4d6c8d61b38d"
        },
        "id": "wZbGsE__WR9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first create a toy dataset with \"only\" 1 million rows (out of 45 millions)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "23587706-411e-487a-89b6-2423b7e389dc"
        },
        "id": "zHEyd_JNWR9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_dataset_path = os.path.join(dataset_folder_path, \"criteo_toy_dataset.txt\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c1015efa-7924-4f3a-8bac-0c0fc691d425"
        },
        "id": "8x6n3G2zWR9t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 1000000 {dataset_path} > {toy_dataset_path}"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "13ad4dd2-c2d8-4c3f-bde2-c5cdd82933d7"
        },
        "id": "NBnfeSnrWR9v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shortcut to get the toy dataset"
      ],
      "metadata": {
        "id": "QQZT2xEVa_YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If it takes too much time, download this toy dataset there instead."
      ],
      "metadata": {
        "id": "JfCQVjtYaxUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "too_long = False\n",
        "if too_long:\n",
        "  from urllib import request\n",
        "  os.makedirs(dataset_folder_path, exist_ok=True)\n",
        "  toy_dataset_path = os.path.join(dataset_folder_path, \"criteo_toy_dataset.txt\")\n",
        "  toy_dataset_url = 'https://www.dropbox.com/s/305lnmwphmu4cir/criteo_toy_dataset.txt?dl=1'\n",
        "  request.urlretrieve(toy_dataset_url, toy_dataset_path, ProgressBar())"
      ],
      "metadata": {
        "id": "lNvzPky8bGSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimate Ratio of Positive samples"
      ],
      "metadata": {
        "id": "bUSIitZFcJwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we want to perform a basic operation: estimate the number of positive samples within the data"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "543acccb-6454-4ffe-960d-0dcbc33894b7"
        },
        "id": "YW7BM72LWR9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic approach"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e85b4203-6efc-46f0-ba5b-52ebe297b95a"
        },
        "id": "3iVxOPERWR9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_positive_label_proportion(dataset_path, columns):\n",
        "  # fill me !\n",
        "  pass"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3d57d10b-15ee-4e3d-82fc-495d40a6b376"
        },
        "id": "kl2X7feEWR9w"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's measure its memory footprint with the `%%memit` magic function"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fceb5b2b-9553-44a7-8d83-030409be41d9"
        },
        "id": "qT5U0AHOWR9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion(toy_dataset_path, columns)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dafb5614-7640-4d36-a0f1-eac939d24f65"
        },
        "id": "vAzIJL4TWR9y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What would happen if you run the same function on a 45 times bigger dataset ?\n",
        "\n",
        "You can give a try with `compute_positive_label_proportion(dataset_path, columns)`... at your own risks."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "83936c1c-2f35-47df-9daf-0ea7b4036e92"
        },
        "id": "2nTAyYW3WR9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specifying schema\n",
        "We can help pandas by specifying the column types to be used such that it does not need to infer it. Do so with the parameter dtype of pd.read_csv: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7ba93320-7ce7-4a5c-b36f-f1e9f86a1528"
        },
        "id": "Vlcy4MeeWR90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_types = OrderedDict()\n",
        "# fill col types here\n",
        "\n",
        "def compute_positive_label_proportion_with_dtype(dataset_path, columns, col_types):\n",
        "    # Read csv with dtype and return positive_label_proportion\n",
        "    # fill me !\n",
        "    pass"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "66f49dc1-bf77-4b33-b4f9-85a8c4f2cbe6"
        },
        "id": "UWKJHqfPWR90"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion_with_dtype(toy_dataset_path, columns, col_types)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7e275dee-57b0-4e2e-9d14-775b3c07daba"
        },
        "id": "A4KfxPStWR92"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading data by chunks\n",
        "We can control the amount of memory we need by loading only a small chunk of the data and processing it before moving to the next chunk.\n",
        "\n",
        "See documentation at https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#iterating-through-files-chunk-by-chunk\n",
        "\n",
        "```\n",
        "reader = pd.read_csv(..., chunksize=10, nrows=100):\n",
        "for chunk in reader:\n",
        "    print(chunk)\n",
        "```"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "56287730-4405-4ae2-90a2-cedf29035f80"
        },
        "id": "kePDZ3HdWR93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_positive_label_proportion_with_dtype_and_chunksize(dataset_path, columns, col_types, chunksize):\n",
        "    # Read csv with dtype and chunksize and return positive_label_proportion\n",
        "    # fill me !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d3c192e9-b4aa-4b81-9abc-9258896a506d"
        },
        "id": "xlWBbAKPWR93"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion_with_dtype_and_chunksize(toy_dataset_path, columns, col_types, 100_000)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "68a665f7-489e-4484-8cf3-4d376d1ea657"
        },
        "id": "KBQpWprpWR94"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can now be applied to the full dataset with no memory issue."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a5625703-6f57-414d-a2f6-25d86f4ae4fb"
        },
        "id": "8uJUIPVQWR95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%memit\n",
        "positive_label_proportion = compute_positive_label_proportion_with_dtype_and_chunksize(dataset_path, columns, col_types, 100_000)\n",
        "print('positive_label_proportion', positive_label_proportion)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9f925158-ac73-4723-be68-249d05c75f5c"
        },
        "id": "qzFz2N0PWR95"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evaluation"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4722389c-73ce-482b-b494-a363950f2276"
        },
        "id": "sD2iNmnfWR97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split train and test datasets\n",
        "Since the datasets contain one line per example, we can split them into train and test by simply iterating over the lines. For each line in the original dataset: write it to the test data set with a probability p and write it to the train dataset with a probability 1 - p."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f7b3b941-be9a-4dff-ac77-fb980120f091"
        },
        "id": "85ojC-KgWR98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test(full_dataset_path, train_dataset_path, test_dataset_path, test_ratio, seed=302984, print_every=None):\n",
        "  # fill me !\n",
        "  pass"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6ddfeb48-4ac1-43ef-9c14-ff395c0fcb18"
        },
        "id": "Qn-_jfDZWR99"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = os.path.join(dataset_folder_path, \"criteo_train_dataset.txt\")\n",
        "test_dataset_path = os.path.join(dataset_folder_path, \"criteo_test_dataset.txt\")\n",
        "if not os.path.exists(train_dataset_path) or not os.path.exists(test_dataset_path):\n",
        "    split_train_test(dataset_path, train_dataset_path, test_dataset_path, test_ratio=0.1, print_every=10_000_000)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b433a302-de7a-4045-b67c-f3ebb3aab794"
        },
        "id": "n-MFoeBFWR99"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l {test_dataset_path}"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2faffa05-0de3-401f-b7d4-b87714a98a17"
        },
        "id": "FovKEtV0WR9-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffling\n",
        "The convergence guarantees of SGD rely on the fact that the observations come at random. Hence, shuffling between epochs is important.\n",
        "\n",
        "First result of \"How to shuffle a file that is too big for memory\" on Google: https://stackoverflow.com/a/40814865/2015762\n",
        "\n",
        "Note that quicker pseudo-shuffling strategies exists, but this fits our \"Big data on your laptop\" problematic."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b524a5dd-b21f-4572-bf9b-6de10e539122"
        },
        "id": "4eJCDJVCWR9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_shuffled_path = os.path.join(dataset_folder_path, \"criteo_test_dataset_shuffled.txt\")\n",
        "train_dataset_shuffled_path = os.path.join(dataset_folder_path, \"criteo_train_dataset_shuffled.txt\")"
      ],
      "metadata": {
        "id": "4BCrAenzh-nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!awk 'BEGIN{srand();} {printf \"%06d %s\\n\", rand()*1000000, $0;}' /content/sync/data/criteo_dataset/criteo_test_dataset.txt | sort -n | cut -c8- > /content/sync/data/criteo_dataset/criteo_test_dataset_shuffled.txt\n",
        "# We can run it on the train dataset too but let's skip it since it is quite long\n",
        "# !awk 'BEGIN{srand();} {printf \"%06d %s\\n\", rand()*1000000, $0;}' /content/sync/data/criteo_dataset/criteo_train_dataset.txt | sort -n | cut -c8- > /content/sync/data/criteo_dataset/criteo_train_dataset_shuffled.txt"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e8546d50-68d4-4f24-abb1-e0c9aa11df96"
        },
        "id": "shqNz_yEWR9_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "In order to train a logistic model on chunks of data, we will use scikit-learn `SGDClassifier` (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) and train for its `log` loss with its `partial_fit` method.\n",
        "We can now apply the previous data processing pipeline and add the training to obtain a trained classifier."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d7444e81-90d6-43c5-acf8-68699e77a3dd"
        },
        "id": "pLjCE-MLWR-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  To begin with, let's not do any preprocessing and deal with \"ready to use\" continuous features only\n",
        "def preprocess_simple(chunk, integer_features, categorical_features):\n",
        "    return chunk[integer_features].fillna(-1)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5f300552-af45-4ab0-a495-db4ba9a8a827"
        },
        "id": "b2zWAybwWR-B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def do_train(path, preprocess, max_training_steps=1000, chunk_size=1000, print_every=50):\n",
        "  # fit some SGDClassifier, chunk by chunk, with partial_fit method, then return it"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c2fff3fb-3a17-42b0-b5aa-d05ff9632ae7"
        },
        "id": "UfP7PsajWR-C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = do_train(train_dataset_path, preprocess_simple)"
      ],
      "metadata": {
        "id": "dWWnjLANpoDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "Let's evaluate the performances of the trained classifier. We should iterate over the test dataset and evaluate the labels predicted by the classifier with `roc_auc_score` and `log_loss`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3ccf2390-7b6e-4ec1-b29f-5e3f767e3fbe"
        },
        "id": "ytACb7xiWR-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_test(classifier, path, preprocess, max_testing_steps = 100, chunk_size = 1000, print_every = 10):\n",
        "  # return AUC and log loss averaged over all chunks\n",
        "  # print average auc and log_loss at each `print_every`\n",
        "  # functions to use:\n",
        "  # - classifier.predict_proba\n",
        "  # - log_loss\n",
        "  # - roc_auc_score"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9a3f8de1-b8aa-41c8-9a46-9c62122da7f9"
        },
        "id": "FVMZYX5UWR-D"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute your AUC and log_loss with simple model"
      ],
      "metadata": {
        "id": "ljckg0E1wr9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f2c9ec3e-554e-4caf-b9bb-bcec65ed3dba"
        },
        "id": "axiFIbG6WR-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous features\n",
        "A smart way to deal with continuous features (counting integer features are part of them), consists in transforming them into categorical features through a quantile transformation. To do so we will use scikit-learn KBinsDiscretizer : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html.\n",
        "\n",
        "It can be used as following\n",
        "```\n",
        "df = pd.DataFrame({'col_1': np.random.normal(size=1000), 'col_2': np.random.poisson(lam=1, size=1000)})\n",
        "bucketizer = KBinsDiscretizer(n_bins=20, encode='ordinal')\n",
        "bucketizer.fit(df)\n",
        "df_bucketized = pd.DataFrame(bucketizer.transform(df), columns=[f'{col}_bucketized' for col in df.columns], index=df.index)\n",
        "sns.jointplot(data=pd.concat((df, df_bucketized), axis=1), x=\"col_1\", y=\"col_1_bucketized\")\n",
        "```"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cacde538-c462-4da5-a5cb-78bbf2e8b624"
        },
        "id": "scI_arubWR-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create a `KBinsDiscretizer` and train it on the first chunk of the dataset. Try the `fit` method of `KBinsDiscretizer` on the chunk and see the type of what is returned. This is not a dataframe any more but your classifier will accept this type for its feature matrix.\n",
        "1. Update `preprocess_data` to add a bucketize step to the training pipeline.\n",
        "1. Do not forget to deal with missing values, you do not want to carry on NaNs. You can for example replace them with -1."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e87b6181-a50c-4a25-9269-825927d14df4"
        },
        "id": "x9HZ2jc5WR-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get a chunk of 1000 lines of your dataset from `train_dataset_path`, you will use it to fit the bucketizer\n",
        "# also get a chunk of 10 lines of your dataset, you will use it to make sure things look correct\n"
      ],
      "metadata": {
        "id": "X3pAygbUs-0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a KBinsDiscretizer of 20 bins, fit it on big chunk, where NaN values are replaced with -1\n"
      ],
      "metadata": {
        "id": "Dc_WkJlnuRR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the bins, test the bucketizer on small chunk. You can use `.todense()` function to look at a small sparse matrix\n"
      ],
      "metadata": {
        "id": "nZgspnLLuTav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# return a matrix of bucketized integer features\n",
        "def preprocess_bucket(chunk, integer_features, categorical_features):\n",
        "  pass"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6603e270-56e3-4ed9-a6e3-b6b8b70b1e0b"
        },
        "id": "3E-2LHNcWR-F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# test the process function on a small chunk\n"
      ],
      "metadata": {
        "id": "lu3hxt0v03at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train your model with bucketized columns\n"
      ],
      "metadata": {
        "id": "H6Fa9uOCv-fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at AUC, log_loss\n"
      ],
      "metadata": {
        "id": "j5r6QsjRwvtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical features"
      ],
      "metadata": {
        "id": "RFREUs7mG-3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Hashing Trick\n",
        "For categorical features we will implement the hashing trick by ourselves. As a quick reminder, for each row\n",
        "\n",
        "1. Select the categorical features \n",
        "1. Create for each feature the string concatenating the feature name and the feature value\n",
        "1. Apply a hash function to each of these string and use this value to choose the feature's column index\n",
        "1. Store the transformed features in a sparse matrix"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ada99dde-fc5f-41e5-bd37-b7a018e66736"
        },
        "id": "tSRJkWyXWR-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a function that hashes strings the deterministic way\n",
        "from sklearn.utils.murmurhash import murmurhash3_bytes_s32\n",
        "def hash_string(string, seed=0):\n",
        "    return murmurhash3_bytes_s32(string.encode(), seed)\n",
        "hash_string('my_feature=my_feature_value')\n",
        "# Note, if we were using builtin function hash('my_feature=my_feature_value'), we would have had a different hash value at each run"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ef9557fb-a1d4-4d70-b70c-158be1bc30b4"
        },
        "id": "JrGXn5qcWR-H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function that:\n",
        "# - takes as arguments:\n",
        "#    - the dataframe to transform\n",
        "#    - size of hash_space\n",
        "# - returns an array with hashes of each categorical columns\n",
        "# hashes should belong to [0;hash_space[\n",
        "def get_features_hashes(row, hash_space):\n",
        "  pass"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "31886db4-d1db-4a02-a9bd-f752ce68ae0a"
        },
        "id": "STKgtwo2WR-I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# try out the function one one row of your small chunk with a hash_space of 2^16\n"
      ],
      "metadata": {
        "id": "dHla2E_l1fQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function that transforms a dataframe into a sparse matrix m that can be passed to the learning\n",
        "# Matrix m contains `hash_space` columns\n",
        "# m[i,j] = 1 if the hashed value of at least one categorical column is `j` for line `i`\n",
        "\n",
        "# use csr_matrix to create the sparse matrix\n",
        "# Fill the csr_matrix, using csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)]) constructor\n",
        "\n",
        "def transform_with_hashing_trick(df, hash_space):\n",
        "  pass"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2c9c09dd-41fe-4fa5-93bb-b7c0afaabb0c"
        },
        "id": "R8RcbJ6WWR-I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function on a small chunk.\n",
        "# Make sure that sum of each row is equal to the number of categorical features\n"
      ],
      "metadata": {
        "id": "mbmyG8BI2Ufj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a `preprocess_hash_v1` method.\n",
        "# New implementation should:\n",
        "# - apply bucketization on integer columns\n",
        "# - apply hashing trick on categorical columns\n",
        "# - return a concatenation of matrix with both features\n",
        "# you can rely on scipy.sparse hstack to concatenate matrix\n",
        "from scipy.sparse import hstack\n",
        "hash_space = 2 ** 16\n",
        "def preprocess_hash_v1(df, integer_features, categorical_features):\n",
        "  pass"
      ],
      "metadata": {
        "id": "UZo9D0ov8l-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the training\n"
      ],
      "metadata": {
        "id": "X_O7-nWl9dlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at metrics\n"
      ],
      "metadata": {
        "id": "nPFwLKuB-Gn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use scikit-learn Feature Hasher"
      ],
      "metadata": {
        "id": "wNAVzhZbH_rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually, the hashing trick is well known and already implemented in scikit-learn FeatureHasher:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0c520c13-7534-4fad-9bf2-2f88c15cd8f6"
        },
        "id": "YT0FwBAgWR-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rewrite a new version of `transform_with_hashing_trick` that relies on FeatureHasher\n",
        "# create the hasher only once\n",
        "# you may need this to convert the dataframe: \n",
        "# https://stackoverflow.com/questions/29815129/pandas-dataframe-to-list-of-dictionaries\n",
        "def transform_with_hashing_trick_v2(df, hash_space):\n",
        "  pass"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ba5b0a8e-722b-47e2-a46f-b9a9dce7c4d3"
        },
        "id": "ug1rD9iaWR-K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it on a small chunk\n"
      ],
      "metadata": {
        "id": "-zqfvhdIDrrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test speed of transform_with_hashing_trick and transform_with_hashing_trick_v2 with timeit function."
      ],
      "metadata": {
        "id": "Yd9kd5_LDwhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6aee4d29-3d84-4f2a-b387-ec233412213b"
        },
        "id": "7yar9NoRWR-L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "22633b92-95dd-4fda-8580-f11bf7a2f102"
        },
        "id": "fA7epmDCWR-L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a preprocess_hash_v2 method that relies on transform_with_hashing_trick_v2\n",
        "def preprocess_hash_v2(df, integer_features, categorical_features):\n",
        "  pass"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "51d49529-7ca5-4f67-b16e-b8c347ac29b3"
        },
        "id": "xRD-tekGWR-O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it on a chunk\n"
      ],
      "metadata": {
        "id": "luBX31eE27uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n"
      ],
      "metadata": {
        "id": "0u4Yl0wq2HE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at metrics\n"
      ],
      "metadata": {
        "id": "X3e2noCG8Dnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Cross Features with hashing"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "157a858a-544e-4219-86b6-4b3423310e16"
        },
        "id": "9S-4m87YWR-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt the hashing trick to implement cross features.\n",
        "# Maybe you just need to reimplement `get_features_hashes`\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "da65498b-985a-42ce-863d-acea33fa3f09"
        },
        "id": "X9kcSx1gWR-P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on small chunk"
      ],
      "metadata": {
        "id": "uGRj3SNTEcKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "2MGC6UYcLz8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at metrics"
      ],
      "metadata": {
        "id": "FG2geVMLMBXf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Local online learning - after 1st session",
      "dashboards": [],
      "language": "python",
      "widgets": {},
      "notebookOrigID": 1919988149413991
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}