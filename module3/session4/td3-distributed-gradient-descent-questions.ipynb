{"cells":[{"cell_type":"markdown","source":["# Distributed gradient descent\n\nIn this exercise, we will build from scratch a logistic regression model and train it with distributed gradient descent.\n\nAs for the other exercise with start with a few imports (fewer than before since we won't use MLlib) and create a local spark application."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7f6f4ba-6338-448f-b0af-d1a59d8afb43"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql import types as st\nfrom pyspark.sql import functions as sf\nfrom pyspark.sql import Row, DataFrame\nfrom pyspark import RDD\nfrom pyspark import StorageLevel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89bb87ca-ef2f-4985-9c69-06287017a929"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%matplotlib inline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56d8e295-93c2-4361-90e1-8d5ab48ecdac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import numpy as np\nimport urllib\nimport math\nimport matplotlib.pyplot as plot\nfrom typing import Tuple, Dict"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8f47b4c-9c48-4d83-a39f-1bd9b2e30d4f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ss = SparkSession \\\n    .builder \\\n    .appName(\"criteo-lr\") \\\n    .master(\"local[4]\") \\\n    .config(\"spark.submit.deployMode\", \"client\") \\\n    .config(\"spark.driver.memory\", \"4g\") \\\n    .config(\"spark.ui.port\", \"0\") \\\n    .getOrCreate()\nss"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1381a2da-d598-470b-b347-06178e211815"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["toy_dataset_url = 'https://www.dropbox.com/s/dle2t3szhljfevh/criteo_toy_dataset.txt?dl=1'\nurllib.request.urlretrieve(toy_dataset_url, \"criteo_toy_dataset.txt\")\ndbutils.fs.mv(\"file:/databricks/driver/criteo_toy_dataset.txt\", \"dbfs:/train.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24069555-3c03-4567-b6f9-7b1f2bddfecc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Q0: Load the data as a Spark DataFrame\n\nThis is exactly the same as Q1 for the other exercise.  \n\nWe will asumme in the rest of the code that your dataframe is called df, that categorical_features is the list of the categorical feature column names and that the label column is called 'label'."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24401cb6-705d-47f4-94b3-39f0e16532de"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a57b574c-2bdc-415a-9a85-a2f587ec89b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Convert the input to a vector using one hot encoding\n\nUnlike the previous exercise, we will use one hot encoding to transform the raw features to our input vector. We will restrict ourselves to a subset of the categorical features, the ones with a small number of distinct modalities. Using one hot encoding on this subset of features will give us a vector of dimension ~100. This will allow us to work with dense vectors. For feature hashing to work well, we have to use a much larger dimension (look at the 2^16 in the previous exercise) where sparse vectors are required."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"698a5807-e8b4-43db-9d31-5d9eb0a3e175"}}},{"cell_type":"markdown","source":["### Selecting a subset of features based on the number of modalities"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7efe2465-7a1c-4db0-b948-8c9303a24ac1"}}},{"cell_type":"code","source":["num_modalities = {} \nfor cat_feat in categorical_features:\n    num_modalities[cat_feat] = df \\\n        .filter(sf.col(cat_feat).isNotNull()) \\\n        .select(cat_feat) \\\n        .distinct() \\\n        .count()\nnum_modalities"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1da413d5-c8e4-4769-8490-f7ff85bb6e30"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We will use all categorical features with less than 50 distinct modalities."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2de68a37-7f7b-4240-861c-31788bfaa5ee"}}},{"cell_type":"code","source":["low_card_cat_feat = [cat_feat for cat_feat, num_modalities in num_modalities.items() if num_modalities < 50]\nlow_card_cat_feat"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f82e17dc-0462-4735-a925-65c744adbc0a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Building dict for one hot encoding\n\nFor one hot encoding, you need a dictionary mapping for each feature and each modality the index in the vector.  \nFirst let's collect the list of modalities for each feature."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b3739c4-66c1-42ac-ad2f-97fe5e03402a"}}},{"cell_type":"code","source":["modalities = {}\nfor cat_feat in low_card_cat_feat:\n    rows = df\\\n        .filter(sf.col(cat_feat).isNotNull())\\\n        .select(cat_feat)\\\n        .distinct()\\\n        .collect()\n    modalities[cat_feat] = [row[cat_feat] for row in rows]\n    # Previous line is to unpack the data from List[Row[str]] to a List[str]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b7dd4ac-7229-4d40-a7a0-d8913549856c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Then let's build the dictionary.  \nWe put in the dictionary all modalities collected in the previous step plus for account for the possibility of each feature being absent. Giving an index to the modality 'absent' for a feature will allow our model to give a weight to such an event and may increase model quality."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dceb7997-f757-49ed-aeba-4abc54a32dd7"}}},{"cell_type":"code","source":["one_hot_encoder = {cat_feat:{} for cat_feat in low_card_cat_feat}\nindex = 0\nfor cat_feat in low_card_cat_feat:\n    for value in modalities[cat_feat]:\n        one_hot_encoder[cat_feat][value] = index\n        index += 1\n    one_hot_encoder[cat_feat][None] = index\n    index += 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7303a4b4-60a2-4b4b-89c7-0043e3d85527"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Converting our input to a vector\n\nWe simply apply the previously generated dictionary and put a 1 in the vector at the index of each (feature,modality).  \nThe dimension of our vector will be the total number of distinct modalities + 1. We use one more dimension to compute the weight of the intercept. It will ease the code below to consider the intercept simply as a feature that all examples have."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df1ec861-ac6a-48bd-a0d5-6478dab92d52"}}},{"cell_type":"code","source":["dimension = 1 + np.sum([len(one_hot_encoder[feature]) for feature in one_hot_encoder.keys()])\ndimension"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c97b25f9-611c-4f64-bab0-97dc14c8f51c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def row_to_vector(\n    row: Row, dimension: int, encoder: Dict[str, Dict[str, int]]\n) -> np.ndarray:\n    x = np.zeros(dimension)\n    x[-1] = 1 # for intercept\n    for feat in encoder.keys():\n        value = row[feat]\n        index = encoder[feat].get(value, None)\n        # index == None mean this modality was not in our dictionary \n        # which is possible if we encouter a new modality in the test set\n        # that was not present in the training set used to build the dictionnary\n        # we don't have space for such features in our vector\n        if index != None:\n            x[index] = 1\n    return x"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f520fef5-6521-4355-89ce-f6d98c67b27f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Q1: Convert the dataframe to an RDD[vector, label]\n\nUsing the function row_to_vector, convert the dataframe to an RDD where each element of the RDD is the pair (vector, label).  \nPrint the first few elements of this RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ffc58a8-7048-4664-8a0b-d064138199f0"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfce9a8e-69ac-42cc-893c-34057601d5c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Computing the prediction and the loss\n\nThe prediction of the logistic regression model is defined as the dot product between the feature vector and the model weights followed by the sigmoid."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9880adba-a0c8-4957-b15e-e3f535b92f18"}}},{"cell_type":"code","source":["def sigmoid(x: float) -> float:\n    return 1 / (1 + math.exp(-x))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10ce3bd4-c804-404d-90ab-f925f7da9641"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["X = np.arange(-10, 10, 0.01)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89ad328d-ad25-4b40-9466-125494ad2ca3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["plot.plot(X, [sigmoid(x) for x in X])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e41f5d84-056e-4de8-86d0-9fa8413eabc0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def point_predict(x: np.ndarray, model: np.ndarray) -> float:\n    # implement me !"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"728dd325-ea46-4fce-b813-2e80f03869ff"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The logistic loss"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4379f76d-f8e2-4fff-8d00-5bfd2c04e82d"}}},{"cell_type":"code","source":["def point_loss(prediction: float, y: int) -> float:\n    return - y * math.log(prediction) - (1-y) * math.log(1-prediction)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6431763-d126-49bc-9f77-6b81e82d8d87"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The closest the prediction is to the label, the lower the loss."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e047b97d-833a-48e3-b788-3e6b534316c6"}}},{"cell_type":"code","source":["for pred, label in [(0.9, 1), (0.1, 0), (0.1, 1), (0.9, 0)]:\n    print(f'For a prediction of {pred} of positive, when the label is {\"positive\" if label ==1 else \"negative\"} the loss is {point_loss(pred, label)}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee30f55f-1656-4635-9884-c9ed5857cd3e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Q2: Compute the loss\n\nGiven an RDD of pair (vector,label), a model and the number of training examples, compute the average loss for this model on this RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92edced7-f6b0-490f-b0f0-54762bd59829"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76ed233b-9f43-4a5e-af6e-fe2b77eafb2b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Q3: Compute the gradient of the loss\n\nHere is the function to compute the gradient on one example"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5fdffae-4469-4634-a7a8-1c9260de8b59"}}},{"cell_type":"code","source":["def point_gradient(x: np.ndarray, y: int, model: np.ndarray) -> float:\n    # implement me !"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34837091-753b-429f-a7d8-7e45b01f8004"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Given an RDD of pair (vector, label), a model and the number of training example, use this function to compute the gradient of the loss."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c46bf16-b5c8-481e-9860-b9226bb923bb"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebffdfba-e1fe-47f5-b9b6-3a26e6daa6b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Q4: Smart initialization\n\nInitialize your model to zero except for the intercept which should be initialized with the logit of the average probability of the positive label. The logit is the inverse of the sigmoid and is given below.  \nCompare the loss of this smart model to the model which is always zero.  \nWhat is the prediction using this smart model?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a715147-e389-446e-a786-8b22d1e29c5d"}}},{"cell_type":"code","source":["def logit(x: float) -> float:\n    # implement me !"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96fa396a-08af-430b-bb8d-beaadd88d7fd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["np.sum(np.abs([x - logit(sigmoid(x)) for x in X]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4ca82b9-6fb7-4aa4-9859-5f281c168e88"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Q5: Distributed Gradient Descent\n\nWrite a train function taking as input the training dataframe, the dictionary for the encoder, a maximum number of iterations and a learning rate and that outputs a model. Print the initial and the final loss and the loss at every step to make sure it decreases."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4863d0fd-5224-4493-a16f-a5986ba1bc99"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fae3e642-f5d9-42fd-85ac-f520aae8e08b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Q6: Weight analysis\n\nPrint the intercept and compare it to the average probability of positive.  \nPrint the weight associated to every feature and modality."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4d9af3e-9dc3-47c5-8859-36b3dd8b497f"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55bd0a8b-cae9-488b-91b4-40e7bc2f99a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## [OPT] Q7: Sparse vectors and feature hashing\n\nReplace the one hot encoding scheme by feature hashing and use all categorical features.  \nReplace all usage of dense vectors by sparse vectors.  \nCompare the performance of your model to Spark MLlib model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d76f12c6-34ff-41e3-af88-30f6f50b0380"}}},{"cell_type":"code","source":["ss.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90098b5b-e7d6-4f77-8f2a-cab05fa5b13c"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.8","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"distributed-gradient-descent","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4298237357093658}},"nbformat":4,"nbformat_minor":0}
