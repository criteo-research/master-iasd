{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Distributed gradient descent\n",
        "\n",
        "In this exercise, we will build from scratch a logistic regression model and train it with distributed gradient descent.\n",
        "\n",
        "As for the other exercise with start with a few imports (fewer than before since we won't use MLlib) and create a local spark application."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f7f6f4ba-6338-448f-b0af-d1a59d8afb43"
        },
        "id": "4UavdX8faE_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -O https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "UvV5fbsuieLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "RvWiLCgaignb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "ss = spark"
      ],
      "metadata": {
        "id": "eRVlSvD2kC94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import types as st\n",
        "from pyspark.sql import functions as sf\n",
        "from pyspark.sql import Row, DataFrame\n",
        "from pyspark import RDD\n",
        "from pyspark import StorageLevel"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "89bb87ca-ef2f-4985-9c69-06287017a929"
        },
        "id": "9QlMr7vHaE_v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "56d8e295-93c2-4361-90e1-8d5ab48ecdac"
        },
        "id": "MTr2OlSVaE_z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import urllib\n",
        "import math\n",
        "import matplotlib.pyplot as plot\n",
        "from typing import Tuple, Dict"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e8f47b4c-9c48-4d83-a39f-1bd9b2e30d4f"
        },
        "id": "hlMs96GNaE_1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "toy_dataset_url = 'https://www.dropbox.com/s/hn2pt29gbne9c99/criteo_toy_dataset-part2.txt?dl=1'\n",
        "request.urlretrieve(toy_dataset_url, \"train.txt\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "24069555-3c03-4567-b6f9-7b1f2bddfecc"
        },
        "id": "7Z0KLM7AaE_4",
        "outputId": "8ebc7c48-534a-4e99-ba21-d94a465d5bd4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q0: Load the data as a Spark DataFrame\n",
        "\n",
        "This is exactly the same as Q1 for the other exercise.  \n",
        "\n",
        "We will asumme in the rest of the code that your dataframe is called df, that categorical_features is the list of the categorical feature column names and that the label column is called 'label'."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "24401cb6-705d-47f4-94b3-39f0e16532de"
        },
        "id": "-6AOojaMaE_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_df = # implement me !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a57b574c-2bdc-415a-9a85-a2f587ec89b3"
        },
        "id": "dfObYLP3aE_6",
        "outputId": "8539fccd-9af3-4bc9-dfd4-03edc280cefe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert the input to a vector using one hot encoding\n",
        "\n",
        "Unlike the previous exercise, we will use one hot encoding to transform the raw features to our input vector. We will restrict ourselves to a subset of the categorical features, the ones with a small number of distinct modalities. Using one hot encoding on this subset of features will give us a vector of dimension ~100. This will allow us to work with dense vectors. For feature hashing to work well, we have to use a much larger dimension (look at the 2^16 in the previous exercise) where sparse vectors are required."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "698a5807-e8b4-43db-9d31-5d9eb0a3e175"
        },
        "id": "PObKodfHaE_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting a subset of features based on the number of modalities"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7efe2465-7a1c-4db0-b948-8c9303a24ac1"
        },
        "id": "-DllyNFCaE_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_modalities = {} \n",
        "for cat_feat in categorical_features:\n",
        "    num_modalities[cat_feat] = df \\\n",
        "        .filter(sf.col(cat_feat).isNotNull()) \\\n",
        "        .select(cat_feat) \\\n",
        "        .distinct() \\\n",
        "        .count()\n",
        "num_modalities"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1da413d5-c8e4-4769-8490-f7ff85bb6e30"
        },
        "id": "Q1eGhcQ5aE_-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use all categorical features with less than 50 distinct modalities."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2de68a37-7f7b-4240-861c-31788bfaa5ee"
        },
        "id": "ZkL8z0mdaE__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low_card_cat_feat = [cat_feat for cat_feat, num_modalities in num_modalities.items() if num_modalities < 50]\n",
        "low_card_cat_feat"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f82e17dc-0462-4735-a925-65c744adbc0a"
        },
        "id": "AuHdigE1aFAB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building dict for one hot encoding\n",
        "\n",
        "For one hot encoding, you need a dictionary mapping for each feature and each modality the index in the vector.  \n",
        "First let's collect the list of modalities for each feature."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7b3739c4-66c1-42ac-ad2f-97fe5e03402a"
        },
        "id": "mmmpPDH5aFAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modalities = {}\n",
        "for cat_feat in low_card_cat_feat:\n",
        "    rows = df\\\n",
        "        .filter(sf.col(cat_feat).isNotNull())\\\n",
        "        .select(cat_feat)\\\n",
        "        .distinct()\\\n",
        "        .collect()\n",
        "    modalities[cat_feat] = [row[cat_feat] for row in rows]\n",
        "    # Previous line is to unpack the data from List[Row[str]] to a List[str]"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8b7dd4ac-7229-4d40-a7a0-d8913549856c"
        },
        "id": "5uYOEws4aFAC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then let's build the dictionary.  \n",
        "We put in the dictionary all modalities collected in the previous step plus for account for the possibility of each feature being absent. Giving an index to the modality 'absent' for a feature will allow our model to give a weight to such an event and may increase model quality."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dceb7997-f757-49ed-aeba-4abc54a32dd7"
        },
        "id": "WmygNHQLaFAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoder = {cat_feat:{} for cat_feat in low_card_cat_feat}\n",
        "index = 0\n",
        "for cat_feat in low_card_cat_feat:\n",
        "    for value in modalities[cat_feat]:\n",
        "        one_hot_encoder[cat_feat][value] = index\n",
        "        index += 1\n",
        "    one_hot_encoder[cat_feat][None] = index\n",
        "    index += 1"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7303a4b4-60a2-4b4b-89c7-0043e3d85527"
        },
        "id": "nUBbLE7RaFAE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting our input to a vector\n",
        "\n",
        "We simply apply the previously generated dictionary and put a 1 in the vector at the index of each (feature,modality).  \n",
        "The dimension of our vector will be the total number of distinct modalities + 1. We use one more dimension to compute the weight of the intercept. It will ease the code below to consider the intercept simply as a feature that all examples have."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "df1ec861-ac6a-48bd-a0d5-6478dab92d52"
        },
        "id": "sDeSUiE4aFAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dimension = 1 + np.sum([len(one_hot_encoder[feature]) for feature in one_hot_encoder.keys()])\n",
        "dimension"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c97b25f9-611c-4f64-bab0-97dc14c8f51c"
        },
        "id": "ECQpdQgxaFAG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def row_to_vector(\n",
        "    row: Row, dimension: int, encoder: Dict[str, Dict[str, int]]\n",
        ") -> np.ndarray:\n",
        "    x = np.zeros(dimension)\n",
        "    x[-1] = 1 # for intercept\n",
        "    for feat in encoder.keys():\n",
        "        value = row[feat]\n",
        "        index = encoder[feat].get(value, None)\n",
        "        # index == None mean this modality was not in our dictionary\n",
        "        # which is possible if we encouter a new modality in the test set\n",
        "        # that was not present in the training set used to build the dictionnary\n",
        "        # we don't have space for such features in our vector\n",
        "        if index != None:\n",
        "            x[index] = 1\n",
        "    return x"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f520fef5-6521-4355-89ce-f6d98c67b27f"
        },
        "id": "5vlYFZmhaFAH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: Convert the dataframe to an RDD[vector, label]\n",
        "\n",
        "Using the function row_to_vector, convert the dataframe to an RDD where each element of the RDD is the pair (vector, label).  \n",
        "Print the first few elements of this RDD."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3ffc58a8-7048-4664-8a0b-d064138199f0"
        },
        "id": "ayiy1VBFaFAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_vectors(df: DataFrame, dimension: int, encoder: Dict[str, Dict[str, int]]) -> RDD:\n",
        "  # implement me !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "bfce9a8e-69ac-42cc-893c-34057601d5c4"
        },
        "id": "s-zlu-q4aFAI",
        "outputId": "0ca2687d-1e59-4653-b44a-e2a284b26fa6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the prediction and the loss\n",
        "\n",
        "The prediction of the logistic regression model is defined as the dot product between the feature vector and the model weights followed by the sigmoid."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9880adba-a0c8-4957-b15e-e3f535b92f18"
        },
        "id": "p26teaIZaFAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x: float) -> float:\n",
        "    return 1 / (1 + math.exp(-x))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "10ce3bd4-c804-404d-90ab-f925f7da9641"
        },
        "id": "Q-V5SUtYaFAJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.arange(-10, 10, 0.01)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "89ad328d-ad25-4b40-9466-125494ad2ca3"
        },
        "id": "bsDhQd85aFAK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot.plot(X, [sigmoid(x) for x in X])"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e41f5d84-056e-4de8-86d0-9fa8413eabc0"
        },
        "id": "90zUetAXaFAK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def point_predict(x: np.ndarray, model: np.ndarray) -> float:\n",
        "    # implement me !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "728dd325-ea46-4fce-b813-2e80f03869ff"
        },
        "id": "iQUnP2sRaFAL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic loss"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4379f76d-f8e2-4fff-8d00-5bfd2c04e82d"
        },
        "id": "gct96QTFaFAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_loss(prediction: float, y: int) -> float:\n",
        "    return - y * math.log(prediction) - (1-y) * math.log(1-prediction)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f6431763-d126-49bc-9f77-6b81e82d8d87"
        },
        "id": "1Z5sS7Y5aFAM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The closest the prediction is to the label, the lower the loss."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e047b97d-833a-48e3-b788-3e6b534316c6"
        },
        "id": "Q2CV2zCXaFAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pred, label in [(0.9, 1), (0.1, 0), (0.1, 1), (0.9, 0)]:\n",
        "    print(f'For a prediction of {pred} of positive, when the label is {\"positive\" if label ==1 else \"negative\"} the loss is {point_loss(pred, label)}')"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ee30f55f-1656-4635-9884-c9ed5857cd3e"
        },
        "id": "35CKaB87aFAN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Compute the loss\n",
        "\n",
        "Given an RDD of pair (vector,label), a model and the number of training examples, compute the average loss for this model on this RDD."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "92edced7-f6b0-490f-b0f0-54762bd59829"
        },
        "id": "ukF-uahzaFAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(vec_label_rdd: RDD, model: np.ndarray, num_examples: int) -> float:\n",
        "  # implement me !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "76ed233b-9f43-4a5e-af6e-fe2b77eafb2b"
        },
        "id": "HlZ6vPRDaFAO",
        "outputId": "4c6fb6cb-beef-470e-f6c7-8296aded6bdd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3: Compute the gradient of the loss\n",
        "\n",
        "Here is the function to compute the gradient on one example"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d5fdffae-4469-4634-a7a8-1c9260de8b59"
        },
        "id": "vqCHg9fzaFAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_gradient(x: np.ndarray, y: int, model: np.ndarray) -> float:\n",
        "  # implement me !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "34837091-753b-429f-a7d8-7e45b01f8004"
        },
        "id": "eDpgYFPFaFAQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given an RDD of pair (vector, label), a model and the number of training example, use this function to compute the gradient of the loss."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2c46bf16-b5c8-481e-9860-b9226bb923bb"
        },
        "id": "FCRTAkEUaFAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(vec_label_rdd: RDD, model: np.ndarray, num_examples: int) -> np.ndarray:\n",
        "  # implement me !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ebffdfba-e1fe-47f5-b9b6-3a26e6daa6b2"
        },
        "id": "Gu6tPBuVaFAR",
        "outputId": "dd151614-5eab-4331-d59b-7ddadcb6e318"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4: Smart initialization\n",
        "\n",
        "Initialize your model to zero except for the intercept which should be initialized with the logit of the average probability of the positive label.\n",
        "\n",
        "The logit L is the inverse of the sigmoid S : L(S(x)) = x.  \n",
        "Compare the loss of this smart model to the model which is always zero.  \n",
        "What is the prediction using this smart model?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5a715147-e389-446e-a786-8b22d1e29c5d"
        },
        "id": "x1MXtbCPaFAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logit(x: float) -> float:\n",
        "    # implement me !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "96fa396a-08af-430b-bb8d-beaadd88d7fd"
        },
        "id": "QxYcV7j_aFAT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(np.abs([x - logit(sigmoid(x)) for x in X]))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e4ca82b9-6fb7-4aa4-9859-5f281c168e88"
        },
        "id": "rwj6dhFraFAT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def smart_init(dimension: int, avg_label: float) -> np.ndarray:\n",
        "  # returns initial weights for model\n",
        "  # implement me !"
      ],
      "metadata": {
        "id": "ZjSEa63AdEQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5: Distributed Gradient Descent\n",
        "\n",
        "Write a train function taking as input the training dataframe, the dictionary for the encoder, a maximum number of iterations and a learning rate and that outputs a model. Print the initial and the final loss and the loss at every step to make sure it decreases."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4863d0fd-5224-4493-a16f-a5986ba1bc99"
        },
        "id": "I_itnJQnaFAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    training_set: DataFrame,\n",
        "    dimension: int,\n",
        "    encoder: Dict[str, Dict[str, int]],\n",
        "    nb_iter: int,\n",
        "    lr: float # step size that we keep constant\n",
        ") -> Tuple[np.ndarray, float]:\n",
        "  # implement me !\n",
        "  # initialize weights with smart_init\n",
        "  # print the loss at each iteration\n",
        "  # return weights and final loss"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fae3e642-f5d9-42fd-85ac-f520aae8e08b"
        },
        "id": "sXMpGqeDaFAU",
        "outputId": "f1bca143-5234-48d8-d0d6-a05af46db712"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6: Weight analysis\n",
        "\n",
        "Print the intercept and compare it to the average probability of positive.  \n",
        "Print the weight associated to every feature and modality."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c4d9af3e-9dc3-47c5-8859-36b3dd8b497f"
        },
        "id": "VJPrCE8WaFAV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "55bd0a8b-cae9-488b-91b4-40e7bc2f99a5"
        },
        "id": "lgYyAL6ZaFAW",
        "outputId": "7fa3c7b1-4f29-45bd-f89f-3e0e663d00ce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "type": "ipynbError",
              "data": "",
              "errorSummary": "",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          }
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [OPT] Q7: Sparse vectors and feature hashing\n",
        "\n",
        "Replace the one hot encoding scheme by feature hashing and use all categorical features.  \n",
        "Replace all usage of dense vectors by sparse vectors.  \n",
        "Compare the performance of your model to Spark MLlib model."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d76f12c6-34ff-41e3-af88-30f6f50b0380"
        },
        "id": "9n97U2gwaFAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ss.stop()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "90098b5b-e7d6-4f77-8f2a-cab05fa5b13c"
        },
        "id": "E38c2b5ZaFAX"
      },
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.6.8",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "distributed-gradient-descent",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 4298237357093658
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}