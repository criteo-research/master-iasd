{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Factorization\n",
        "\n",
        "We will experiment with the recent MovieLens 25M Dataset and build a recommender system using two approaches:\n",
        "* Factorizing the user-item matrix using Spark ALS implementation\n",
        "* Factorizing the item-item PMI maatrix using randomized SVD\n",
        "\n",
        "In both settings we will index the item embeddings and inspect their quality using KNN queries."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ec58202f-3dde-4483-b569-f93f1ab4a8c7"
        },
        "id": "b1G9-EI4Tf8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -O https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "eUjrfPwtVSNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "hRap5s_jVS_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "ss = spark"
      ],
      "metadata": {
        "id": "h8JO-9oHVUY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "RoD64a80VZL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "63c24520-7d86-48bc-8abe-c38f26bf1bc6"
        },
        "id": "EfIGcSTITf8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the dataset"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "930121a6-31fe-4b95-8459-4fb22caf15e5"
        },
        "id": "j5PFG4obTf8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
        "!unzip ml-25m"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f2fa6086-66c5-4421-997c-3abc0bc79ab2"
        },
        "id": "4LtWoJfrTf8S"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the ratings dataset"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "83dc87e1-4575-41fb-b9b9-53515664b2ac"
        },
        "id": "EobcJpCDTf8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark.sql import SparkSession\n",
        "#from pyspark.ml.evaluation import RegressionEvaluator\n",
        "#from pyspark.ml.recommendation import ALS\n",
        "#from pyspark.sql import Row\n",
        "#import pyspark.sql.functions as F"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8e18eb64-a4bd-4e19-a0da-156831d0df8b"
        },
        "id": "OvjeeEb-Tf8V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df = spark.read.csv('ml-25m/movies.csv', header=True, inferSchema=True).cache()\n",
        "ratings_df = spark.read.csv('ml-25m/ratings.csv', header=True, inferSchema=True)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "30cde39c-8089-4c5c-9b95-347a8b3414b0"
        },
        "id": "mMtfp96LTf8V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the dataset\n",
        "We want to randomly split the dataset into train and test parts"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9638b534-2471-4f74-8c9c-df471a6dbc43"
        },
        "id": "GqwyLUPATf8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "099d08ee-00b8-496e-a91e-240c18f31a5d"
        },
        "id": "s5G60ukkTf8X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# you may want to try this :\n",
        "\n",
        "#training_percent = 80\n",
        "#training_df = user_movies_interactions = (\n",
        "#    ratings_df\n",
        "#    .filter(F.expr(f'PMOD(HASH(userId),100)')<training_percent)\n",
        "#    .repartition('userId', 'movieId')\n",
        "#).cache()\n",
        "#validation_df = user_movies_interactions = (\n",
        "#    ratings_df\n",
        "#    .filter(F.expr(f'PMOD(HASH(userId),100)')>=training_percent)\n",
        "#    .repartition('userId', 'movieId')\n",
        "#).cache()\n",
        "\n",
        "# it won't help much in the validation phase though !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e4fb1bf3-ef27-4585-adfb-56a543e1ca72"
        },
        "id": "ivo72M-kTf8Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "(training_df, validation_df) = ratings_df.randomSplit([0.8, 0.2])"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "003256b1-ec6f-4041-b447-cc702a2f0a08"
        },
        "id": "NFuhpTl7Tf8Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.count()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9dc8e22b-5a6b-4a23-ad94-4bf1ed8adfac"
        },
        "id": "m0mSY76fTf8a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build ALS model\n",
        "Using the Spark ALS implementation described here https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html\n",
        "Build a model using the ml-25m dataset.\n",
        "\n",
        "How long does the training take, change the rank (i.e. the dimension of the vectors) from 10 to 20. How does that affect training speed ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b7b13148-8e7e-46f1-b856-3727dd2c7021"
        },
        "id": "Ze64TjECTf8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "import time\n",
        "\n",
        "ranks=[10,15,20,30]\n",
        "models=[]\n",
        "training_time=[]\n",
        "\n",
        "for rank in ranks:\n",
        "  start_time = time.time()\n",
        "  als = ALS(rank=rank, maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
        "  model = als.fit(training_df)\n",
        "  models.append(model)\n",
        "  training_time.append(time.time() - start_time)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b3fd44e1-0529-4af3-919f-3118b6e30d13"
        },
        "id": "-Dtmbv37Tf8b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import plot\n",
        "%matplotlib inline\n",
        "plot(ranks, training_time)\n",
        "\n",
        "# processing time seems linear as long as we don't have memory issues to deal-with."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2d712aad-f474-441f-8236-8a94e6cda128"
        },
        "id": "aAwxxVfZTf8b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "Using the code described in the Spark documentation, evaluate how good your model is doing on the test set.\n",
        "The goal is to predict the held out ratings.\n",
        "A good metric could be RMSE or MAE."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3342dd06-1737-43ec-bef3-c185c601bb24"
        },
        "id": "BG8C6iGGTf8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "predictions = model.transform(validation_df)\n",
        "evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "mae = evaluator.evaluate(predictions)\n",
        "print(\"MAE = \" + str(mae))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "49bbd4e6-7c06-4929-9c97-cd56f7c7bcda"
        },
        "id": "IDBKMNaxTf8c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting the results\n",
        "\n",
        "Retrieve the movie vectors from the learned model object (the property is called itemFactors).\n",
        "and `collect` all these vectors in a list."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9de1257d-ea6d-4833-ac47-79311f0c0f8a"
        },
        "id": "AX7Bx3JuTf8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_vectors_df = model.itemFactors.join(movies_df.withColumnRenamed('movieId', 'id'), 'id').select('title', 'features')"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7d9e5482-54ff-4950-9b8d-4d889c64dd7b"
        },
        "id": "TCDvFsDJTf8d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to create a dictionary mapping the movieId to it's title to ease the inspection. \n",
        "Load the `movies.csv` file using pyspark or pandas and create a `dict` movieId -> title."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6f67edcb-fc63-47b8-ae6d-ec1c1c6c1e42"
        },
        "id": "ifIaUvuRTf8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_vect_dict = {r['title'] : r['features'] for r in movie_vectors_df.collect()}"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d018601f-236f-4777-8a93-6846f5ac9ab7"
        },
        "id": "0WfsiG4gTf8e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Nearest neighbours\n",
        "\n",
        "Pick a few movies, and for each of them, find-out the top 5 nearest neighbours. This is very similar to an optional question of the PLSA project..."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "89e7fa17-0f14-42ae-9bd7-7894dbbc6069"
        },
        "id": "sKxb3LDUTf8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title_vector_array = movie_vectors_df.collect()\n",
        "titles = [r['title'] for r in title_vector_array]\n",
        "vectors = [r['features'] for r in title_vector_array]"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "acefd735-d0a6-454e-811a-8ccf54092731"
        },
        "id": "j_CuBp21Tf8e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from numpy import linalg as LA\n",
        "import heapq\n",
        "# naive knn with queue, using numpy to batch vector operations\n",
        "def knn(query, k, titles, vectors):\n",
        "  start_time = time.time()\n",
        "  nb_movies = len(titles)\n",
        "  diff = numpy.array(vectors) - numpy.array(query)\n",
        "  distances = LA.norm(diff, axis=1)\n",
        "  indices = heapq.nlargest(k, range(0, nb_movies), key=lambda x: -distances[x])\n",
        "  ret = [(titles[i], distances[i]) for i in indices]\n",
        "  print(f\"{time.time() - start_time}\")\n",
        "  return ret"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c22a620d-a7d7-46d6-ab38-fb6556e0ce7c"
        },
        "id": "Z9oowqZPTf8f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze(i):\n",
        "  print(f\"Query title : {titles[i]}\")\n",
        "  query_vec = vectors[i]\n",
        "  ret = knn(query_vec, 10, titles, vectors)\n",
        "  for res in ret:\n",
        "    print(res)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6afe8993-2f1a-4bee-ad77-ccee37bda30a"
        },
        "id": "t3_2rnK2Tf8g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "analyze(4)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b66d2c26-8ab1-4039-b1bd-4db8119fe990"
        },
        "id": "uEOK199tTf8g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0ec9dd97-3a51-4663-9c7a-5c046daf94bf"
        },
        "id": "WuF9nDIRTf8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Another approach - RSVD\n",
        "\n",
        "We now are going to factorize the item-item PMI matrix using randomized SVD."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6a498b1f-c395-429d-9a32-7ce55ca8e470"
        },
        "id": "eciFO1erTf8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the PMI matrix"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "943f025b-c703-483e-a0b1-554964f69a09"
        },
        "id": "Apt5v9cwTf8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the movie pair counts by doing a self join on the ratings dataframe (filtered to keep only the relevant movies).\n",
        "\n",
        "Cautious ! This computation is expensive as we explicit all movie pairs from all users.\n",
        "\n",
        "You will need to filter / sample your data wisely to avoid big join."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b11143da-b1f3-443b-bb73-f345126ee5d4"
        },
        "id": "yX-9mt4oTf8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first things first we only keep movies liked by user.\n",
        "ratings_df = ratings_df.filter(F.col('rating')>=3.5).cache()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d1f8d326-4d2e-4210-8fa8-4a03f4b90e33"
        },
        "id": "VATnEZayTf8i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "29HT0Nof5Dol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at how much ratings are done user by user\n",
        "# When user has scored a lots of movies, amount of pairs will increase quadratically !\n",
        "ratings_count_by_user_df = ratings_df.groupby('userId').agg(F.count('*').alias('count')).sort(F.col('count').desc()).cache()\n",
        "display(ratings_count_by_user_df)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0ee3af67-67ca-430c-9875-9fb1486c9e08"
        },
        "id": "C01uFrgKTf8i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# We will sample user ratings to make sure they don't exceed a given threshold.\n",
        "threshold = 40\n",
        "ratings_sampled_df = (\n",
        "  ratings_df\n",
        "  .join(ratings_count_by_user_df, 'userId')\n",
        "  .filter(F.rand() < threshold / F.col('count'))\n",
        "  .select('userId', 'movieId')\n",
        "  .repartition('userId', 'movieId')\n",
        "  .cache()\n",
        ")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3945e65b-55ad-4018-af28-35aa9fedf6be"
        },
        "id": "3YK12iDRTf8j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Also, self join will rely on sort merge join. We want to avoid two sort so we store the dataset, sorted.\n",
        "ratings_sampled_df.write\\\n",
        "    .bucketBy(8, 'userId') \\\n",
        "    .sortBy('userId') \\\n",
        "    .saveAsTable('bucketed_ratings', format='parquet')\n",
        "sorted_ratings_df = spark.table('bucketed_ratings').cache()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2834dd9e-e28e-485d-b83c-983038345f89"
        },
        "id": "I1j2Z6RdTf8j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_df = (\n",
        "  sorted_ratings_df\n",
        "    .join(sorted_ratings_df.withColumnRenamed('movieId', 'movieId2'), 'userId')\n",
        "    .groupby(F.concat(F.greatest('movieId', 'movieId2'), F.lit(\"-\"), F.least('movieId','movieId2')).alias('pair'))\n",
        "    .agg(F.count(\"*\").alias('pair_count'))\n",
        "    .cache()\n",
        ")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a4fb4411-09e9-4f33-a9ce-a2cd801f57b9"
        },
        "id": "JmTbVHTVTf8j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_df.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6453a38e-a774-4c7f-9369-b4bc90e91b3e"
        },
        "id": "cYEz-H9hTf8j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the amount of ratings by movie. You will need it in order to compute the pmi formula."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7660b5ca-a3d9-4c16-a359-325eef4896b6"
        },
        "id": "J1wjexfdTf8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_counts = ratings_sampled_df.groupby('movieId').agg(F.count(\"*\").alias('count')).cache()\n",
        "print(f\"Nb Movies : {movie_counts.count()}\")\n",
        "display(movie_counts.sort(F.col('count').desc()))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d30053fe-cdd2-4b81-8ff1-4da04b63c693"
        },
        "id": "LJHRX2I3Tf8k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the movie counts and the pair counts, compute the PMI dataframe using the formula provided in the lecture.\n",
        "You will be doing a join between the pairs and counts twice."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6288f2f5-ef02-41e3-8541-24457ce97a95"
        },
        "id": "tKIEjpHqTf8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_ratings = ratings_sampled_df.count()\n",
        "\n",
        "pmi_df = (\n",
        "  pairs_df\n",
        "    .withColumn('split', F.split(F.col('pair'), '-').alias('split'))\n",
        "    .select(F.element_at('split', 1).alias('movieId1'), F.element_at('split', 2).alias('movieId2'), F.col('pair_count'))\n",
        "    .join(movie_counts.withColumnRenamed('movieId', 'movieId2'), 'movieId2')\n",
        "    .withColumnRenamed('count', 'count_movie2')\n",
        "    .join(movie_counts.withColumnRenamed('movieId', 'movieId1'), 'movieId1')\n",
        "    .withColumnRenamed('count', 'count_movie1')\n",
        "    .select(\n",
        "      F.col('movieId1'), \n",
        "      F.col('movieId2'), \n",
        "      ((F.col('pair_count') * n_ratings) / (F.col('count_movie1') * F.col('count_movie2'))).alias('pmi')\n",
        "    )\n",
        "    .cache()\n",
        ")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "90473310-1c36-4da2-b7d4-082166991e40"
        },
        "id": "N4HnokfdTf8l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pmi_df.show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "742f6313-ff54-49c8-9cc8-bc4e55039979"
        },
        "id": "ldVlSXIwTf8m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RSVD"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6ae3d2c4-8142-41b2-b44f-e4653e480ed7"
        },
        "id": "R4Zda5LNTf8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to build a scipy sparse matrix (lil_matrix) from the PMI dataframe. It is small enough to be collected into memory."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "632c1d5f-eae1-4c24-b710-a1243abb2822"
        },
        "id": "DnA2ZYcqTf8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = pmi_df.collect()\n",
        "vocabulary={}\n",
        "for row in rows:\n",
        "  vocabulary.setdefault(row['movieId1'], len(vocabulary))\n",
        "  vocabulary.setdefault(row['movieId2'], len(vocabulary))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9f7ca07d-9fdc-4608-97fd-bc001ce33b75"
        },
        "id": "zMIcjmAoTf8m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "from scipy.sparse import lil_matrix\n",
        "matrix = lil_matrix((len(vocabulary),len(vocabulary)))\n",
        "for row in rows:\n",
        "  i = vocabulary[row['movieId1']]\n",
        "  j = vocabulary[row['movieId2']]\n",
        "  matrix[i,j] = log(row['pmi'])\n",
        "  matrix[j,i] = log(row['pmi'])"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "873af638-2351-42f3-967f-dedac32ff907"
        },
        "id": "skJfIJ5DTf8m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the scikit-learn implementation of SVD https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html to factorize the PMI matrix. It uses the randomized SVD algorithm presented as a default."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7bb52455-55f5-4d0a-8256-56a44a67e598"
        },
        "id": "sakU6NBiTf8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd = TruncatedSVD(n_components=30, random_state=42)\n",
        "svd.fit(matrix)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "74999f03-2bb9-4702-b542-023f2829a00a"
        },
        "id": "_l-u5SXkTf8n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Faiss Index"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "34a5998d-9e80-46d8-9040-df5655ceecf7"
        },
        "id": "e5Sv9hguTf8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install faiss-cpu, and create an index from these vectors. Query the index like what we have done previously."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e8e57ed4-fb78-40c9-8db0-f25503f58d3a"
        },
        "id": "dk7UNtgATf8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8a3898b4-c15d-46bb-a526-8548b637251f"
        },
        "id": "acmhAdE6Tf8o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# create faiss index\n",
        "import faiss\n",
        "index = faiss.IndexFlatL2(components_.shape[0])\n",
        "faiss_matrix = svd.components_.transpose().astype('float32')\n",
        "index.add(faiss_matrix)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b604a1e3-a4cb-4752-9f25-56409f8d6fd4"
        },
        "id": "WzZPKCOLTf8o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# used to display movie names\n",
        "inverted_index = {vocabulary[k]:k for k in vocabulary.keys()}\n",
        "titles_by_id = {row['movieId']:row['title'] for row in movies_df.collect()}\n",
        "\n",
        "# utility function to display top k\n",
        "def analyze(movie_index, k):\n",
        "  nb_dims = faiss_matrix.shape[1]\n",
        "  (embeddings, indexes) = index.search(faiss_matrix[movie_index,:].reshape((1,nb_dims)), k)\n",
        "  for movie in [titles_by_id[int(inverted_index[i])] for i in indexes[0,:]]:\n",
        "    print(movie)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "255b883f-9808-4877-ad0c-b7035b808ace"
        },
        "id": "kE3tCzDUTf8p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "analyze(14, 10)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a169adc8-9c7b-4d63-b16e-2319f315faba"
        },
        "id": "sZ5H5DwsTf8q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BG3s6ht25P4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.7.4",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Matrix Factorization (Solution) (1)",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 3508639176841972
    },
    "colab": {
      "name": "td4-part1-matrix-factorization-solution.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eciFO1erTf8h"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}