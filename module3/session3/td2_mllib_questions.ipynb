{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Presentation\n",
        "In this workshop we will discover Mllib features, and apply them on the titanic dataset.\n",
        "\n",
        "We will try to predict passenger survival rate based on a few features, with a logistic regression model."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "51cf46e8-e114-407b-bf24-fef10f9aea04"
        },
        "id": "gLseculkdvsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Spark Environment\n",
        "Since we are not running on databricks, we will need to install Spark by ourselves, every time we run the session.  \n",
        "We need to install Spark, as well as a Java Runtime Environment.  \n",
        "Then we need to setup a few environment variables."
      ],
      "metadata": {
        "id": "uBbKFvQjdzIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -O https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "3bYRWp9Bd14H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "naZYybE3d53s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "OKGfiMwyd6xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional step : Enable SparkUI through secure tunnel\n",
        "This step is useful if you want to look at Spark UI.\n",
        "First, you need to create a free ngrok account : https://dashboard.ngrok.com/login.  \n",
        "Then connect on the website and copy your AuthToken."
      ],
      "metadata": {
        "id": "-OFIUJ1kd-Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this step downloads ngrok, configures your AuthToken, then starts the tunnel\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!./ngrok authtoken my_ngrok_auth_token_retrieved_from_website # <-------------- change this line !\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "metadata": {
        "id": "WAs7X-7ieBRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now** get the Spark UI url on https://dashboard.ngrok.com/endpoints/status. We're done !"
      ],
      "metadata": {
        "id": "foINfBWKeEDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset\n",
        "We need to download dataset and put it inside HDFS."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "56b3941f-b4d2-465e-b2a9-edd7eb069b66"
        },
        "id": "1HnFTY0JdvsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset, make sure it is available on your gateway\n",
        "from urllib import request\n",
        "url = \"https://www.dropbox.com/s/1tl236ptjuwvcib/titanic-passengers.csv?dl=1\"\n",
        "request.urlretrieve(url, \"titanic.csv\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "117f3fe7-056b-477c-8761-43c47a4680e6"
        },
        "id": "T-ylPJ9qdvsM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools of the trade\n",
        "We need a few imports to learn some model with MLLib."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2bc1d6d0-4ec8-44ef-bc8d-7b840e4a0d89"
        },
        "id": "G6ZGPDZbdvsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F # you already know this one ! need it whenever you want to transform columns\n",
        "from pyspark.ml.feature import *       # this package contains most of mllib feature engineering tools\n",
        "from pyspark.ml import Pipeline        # pipeline is used to combine features"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "55e21102-1244-482c-afcb-5aabc90c3dc3"
        },
        "id": "AgyzZY3fdvsO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 0\n",
        "Load the dataset.\n",
        "\n",
        "Make sure the remainder of the schema is correct.\n",
        "\n",
        "Split the dataset in train and test with 'randomSplit' method. 90%/10%"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ded2ba2e-2331-4d83-9046-cef307969cd3"
        },
        "id": "6E5FMo3UdvsP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b13970e3-3b8c-4f32-929d-fb11f0caf844"
        },
        "id": "nGZN7lotdvsP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9fbd51b0-33aa-4331-a26d-e7c92aaeb6de"
        },
        "id": "YbPA4tiqdvsQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1\n",
        "On training set, fit a model that predicts passenger survival probability, function of ticket price.\n",
        "\n",
        "You will need to convert survived column in 0/1 to pass it to the logistic regression. Transform it with StringIndexer.\n",
        "\n",
        "Use a pipeline ending with a logistic regression.\n",
        "\n",
        "Compute model AUC on validation set.\n",
        "\n",
        "Documentation:\n",
        "- https://spark.apache.org/docs/latest/ml-classification-regression.html#binomial-logistic-regression\n",
        "- https://spark.apache.org/docs/latest/ml-pipeline.html#example-pipeline\n",
        "- https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#binary-classification"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "463001f6-503d-4ab9-8654-f31637582886"
        },
        "id": "InX1nb1xdvsR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "06ce7c3d-3d2e-4bde-9560-ed9171bf58a0"
        },
        "id": "Riy8Hp18dvsT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2\n",
        "We will do a lots of feature engineering now and we don't want you to copy-paste code all-way long.\n",
        "\n",
        "Write the following function:\n",
        "\n",
        "Inputs:\n",
        "- pipeline\n",
        "- training set\n",
        "- validation set\n",
        "\n",
        "Outputs:\n",
        "- auc\n",
        "- transformed dataset (with prediction)\n",
        "\n",
        "Make sure it returns on previous pipeline."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4579a58c-9e9b-4f9a-bd32-5feecd3dd41d"
        },
        "id": "DLJcatGtdvsU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f11a5d25-2e33-4200-b447-e987b7e08878"
        },
        "id": "kfUEnHR7dvsV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "26583cd3-baea-4ec8-81ca-d4fca488d933"
        },
        "id": "tVp2DwFMdvsW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3\n",
        "Relying on raw continuous feature may be a bit rough.\n",
        "We can try to bucketize numeric feature in five buckets instead."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b03803ba-1430-4794-8f51-f4db5801fdbb"
        },
        "id": "PduCajlfdvsX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "87482d50-c46e-45e7-84e5-44bbbce657af"
        },
        "id": "bD6FhSJrdvsX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4\n",
        "Why don't you try to rely on other numerical features now ?\n",
        "\n",
        "You can try to leverage 'Age', and maybe 'PassengerId' while we're at it.\n",
        "\n",
        "Is it better ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a83bf019-a813-4107-8c4a-493ca86046a1"
        },
        "id": "Dtp72XxvdvsY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b390460e-373a-413e-afe1-7544bf4ef83c"
        },
        "id": "N-LvvuYPdvsY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5\n",
        "We should try to use categorial features.\n",
        "\n",
        "Remember, spark just understands vectors. So you need to convert categories in vectors with OneHotEncoder.\n",
        "\n",
        "Try several categories and identify what works."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f3cd8a2b-ab7b-4ab9-a1cf-29b7ecde5f79"
        },
        "id": "K02psIq0dvsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "27703893-d86a-4127-b6d9-afbf306bc1d4"
        },
        "id": "rWL5B0NgdvsZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gender is not numeric, we need to convert it before one-hot-encoding it !"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c8633f54-965a-463d-94c3-a7bdfee5081a"
        },
        "id": "43OcNfN3dvsa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "521f533a-9c49-4cee-ba30-d880c13306a7"
        },
        "id": "d-ce1E59dvsa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6\n",
        "\n",
        "These are open questions you can try to tackle in any order:\n",
        "- cross features. E.g., try to use features like : passenger is male and passenger is older than 30 years.\n",
        "- use feature hashing\n",
        "- rely on name feature"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1427b306-351d-42b7-afa2-7f54664141f8"
        },
        "id": "TX8RLYgvdvsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Hashing\n",
        "In this one, you will need to create a custom transformation that transforms a sparse vector into another sparse vector with lower dimension (MLLib does not have exactly what we want there).\n",
        "- you can rely on this post to see how to create transformer : https://csyhuang.github.io/2020/08/01/custom-transformer/\n",
        "- look at the following classes for your udf : VectorUDT ; SparseVector"
      ],
      "metadata": {
        "id": "xv0bLRcrOAmA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ls3TInn_uG4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDTdHc83Voxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4HmUT-KYXLUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rgxFawfCuxRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Titanic + MLLib (solution)",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 323843935193549
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}